<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://canva4.github.io/"/>
  <updated>2020-09-03T15:51:56.515Z</updated>
  <id>http://canva4.github.io/</id>
  
  <author>
    <name>阿翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CS231n Assignment3 遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-27T09:04:38.000Z</published>
    <updated>2020-09-03T15:51:56.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment3-遇到的问题"><a href="#CS231n-Assignment3-遇到的问题" class="headerlink" title="CS231n Assignment3 遇到的问题"></a>CS231n Assignment3 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Image-Captioning-with-RNNs"><a href="#Image-Captioning-with-RNNs" class="headerlink" title="Image Captioning with RNNs"></a>Image Captioning with RNNs</h2><p>本部分主要是实现RNN的基础版本。即如下的RNN，不过需要注意的是在代码中实现时要注意矩阵相乘的顺序。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/FQu3U9OsD5gtvYi.png" alt="image-20200903233248910"></p><p>首先是每次time step时的forward的backward，这里比较简单，按照上图公式implement一下就ok了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span>(<span class="params">x, prev_h, Wx, Wh, b</span>):</span></span><br><span class="line">    next_h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    next_h = np.tanh(np.dot(prev_h, Wh) + np.dot(x, Wx) + b)</span><br><span class="line">    cache = Wx, Wh, next_h, prev_h, x, b</span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span>(<span class="params">dnext_h, cache</span>):</span></span><br><span class="line">    dx, dprev_h, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    Wx, Wh, next_h, prev_h, x, b = cache</span><br><span class="line">    dmid = (<span class="number">1</span> - np.square(next_h)) * dnext_h</span><br><span class="line">    dprev_h = np.dot(dmid, Wh.T)</span><br><span class="line">    dx = np.dot(dmid, Wx.T)</span><br><span class="line">    dWh = np.dot(prev_h.T, dmid)</span><br><span class="line">    dWx = np.dot(x.T, dmid)</span><br><span class="line">    db = np.sum(dmid, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>然后是在一定time sequence上的forward和backward，forward就是多层step forward的叠加，backward计算梯度就是将每次对x的梯度持续回传，将对W权值矩阵的梯度叠加即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span>(<span class="params">x, h0, Wx, Wh, b</span>):</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    cache = []</span><br><span class="line">    h = np.zeros((N, T, h0.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, c = rnn_step_forward(x[:, i, :], h0, Wx, Wh, b)</span><br><span class="line">        h[:, i] += h0</span><br><span class="line">        cache.append(c)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span>(<span class="params">dh, cache</span>):</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    D = cache[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, H))</span><br><span class="line">    dWh = np.zeros((H, H))</span><br><span class="line">    db = np.zeros((H,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        dx[:, i], dh0, dWx_mid, dWh_mid, db_mid = rnn_step_backward(dh[:, i] + dh0, cache.pop())</span><br><span class="line">        dWx += dWx_mid</span><br><span class="line">        dWh += dWh_mid</span><br><span class="line">        db += db_mid</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>实现这些基本核心组件后，还需要实现的就是根据word 生成 embedding，这里使用的是类似于查询的方法，有一个对应的生成embedding的矩阵，这个也是可以学习的。forward很简单，就是类似的查询，backward的实现我遇到了实现上的问题，最后借鉴了一下别人的code。:)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_forward</span>(<span class="params">x, W</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    out = W[x]</span><br><span class="line">    cache = x, W</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dW = <span class="literal">None</span></span><br><span class="line">    x, W = cache</span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line">    N, T, D = dout.shape</span><br><span class="line">    np.add.at(dW, x.flatten(), dout.reshape(<span class="number">-1</span>, D))     <span class="comment"># 这里借鉴了一下别人的代码</span></span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure><p>最后要实现的就是class RNN了。这个部分只要认真看代码中的提升，注意下细节根据流程和之前实现好的模块实现即可了。</p><p><strong>forward 函数</strong>，位于<code>rnn.py</code> rnn类内。这里的处理是将caption分为两部分：captions_in除了最后一个单词外，所有内容都将被输入到RNN； 而captions_out只不包含第一个单词。这就是期望RNN生成的东西。 它们彼此相对偏移一个，因为RNN在接收到单词t之后会产生单词（t + 1）。 captions_in的第一个元素将是START caption，我们的期望是captions_out的第一个元素将是第一个单词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, features, captions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute training-time loss for the RNN. We input image features and</span></span><br><span class="line"><span class="string">    ground-truth captions for those images, and use an RNN (or LSTM) to compute</span></span><br><span class="line"><span class="string">    loss and gradients on all parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Input image features, of shape (N, D)</span></span><br><span class="line"><span class="string">    - captions: Ground-truth captions; an integer array of shape (N, T) where</span></span><br><span class="line"><span class="string">      each element is in the range 0 &lt;= y[i, t] &lt; V</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Scalar loss</span></span><br><span class="line"><span class="string">    - grads: Dictionary of gradients parallel to self.params</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Cut captions into two pieces: captions_in has everything but the last word</span></span><br><span class="line">    <span class="comment"># and will be input to the RNN; captions_out has everything but the first</span></span><br><span class="line">    <span class="comment"># word and this is what we will expect the RNN to generate. These are offset</span></span><br><span class="line">    <span class="comment"># by one relative to each other because the RNN should produce word (t+1)</span></span><br><span class="line">    <span class="comment"># after receiving word t. The first element of captions_in will be the START</span></span><br><span class="line">    <span class="comment"># token, and the first element of captions_out will be the first word.</span></span><br><span class="line">    captions_in = captions[:, :<span class="number">-1</span>]</span><br><span class="line">    captions_out = captions[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You&#x27;ll need this</span></span><br><span class="line">    mask = (captions_out != self._null)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the affine transform from image features to initial</span></span><br><span class="line">    <span class="comment"># hidden state</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Word embedding matrix</span></span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input-to-hidden, hidden-to-hidden, and biases for the RNN</span></span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the hidden-to-vocab transformation.</span></span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward and backward passes for the CaptioningRNN.   #</span></span><br><span class="line">    <span class="comment"># In the forward pass you will need to do the following:                   #</span></span><br><span class="line">    <span class="comment"># (1) Use an affine transformation to compute the initial hidden state     #</span></span><br><span class="line">    <span class="comment">#     from the image features. This should produce an array of shape (N, H)#</span></span><br><span class="line">    <span class="comment"># (2) Use a word embedding layer to transform the words in captions_in     #</span></span><br><span class="line">    <span class="comment">#     from indices to vectors, giving an array of shape (N, T, W).         #</span></span><br><span class="line">    <span class="comment"># (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #</span></span><br><span class="line">    <span class="comment">#     process the sequence of input word vectors and produce hidden state  #</span></span><br><span class="line">    <span class="comment">#     vectors for all timesteps, producing an array of shape (N, T, H).    #</span></span><br><span class="line">    <span class="comment"># (4) Use a (temporal) affine transformation to compute scores over the    #</span></span><br><span class="line">    <span class="comment">#     vocabulary at every timestep using the hidden states, giving an      #</span></span><br><span class="line">    <span class="comment">#     array of shape (N, T, V).                                            #</span></span><br><span class="line">    <span class="comment"># (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #</span></span><br><span class="line">    <span class="comment">#     the points where the output word is &lt;NULL&gt; using the mask above.     #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># In the backward pass you will need to compute the gradient of the loss   #</span></span><br><span class="line">    <span class="comment"># with respect to all model parameters. Use the loss and grads variables   #</span></span><br><span class="line">    <span class="comment"># defined above to store loss and gradients; grads[k] should give the      #</span></span><br><span class="line">    <span class="comment"># gradients for self.params[k].                                            #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># Note also that you are allowed to make use of functions from layers.py   #</span></span><br><span class="line">    <span class="comment"># in your implementation, if needed.                                       #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    caches = []</span><br><span class="line">    out, cache = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    word_in, cache = word_embedding_forward(captions_in, W_embed)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:  <span class="comment"># must rnn or lstm</span></span><br><span class="line">        out, cache = rnn_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out, cache = lstm_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    out, cache = temporal_affine_forward(out, W_vocab, b_vocab)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss, dx = temporal_softmax_loss(out, captions_out, mask)</span><br><span class="line"></span><br><span class="line">    dx, grads[<span class="string">&#x27;W_vocab&#x27;</span>], grads[<span class="string">&#x27;b_vocab&#x27;</span>] = temporal_affine_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = rnn_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = lstm_backward(dx, caches.pop())</span><br><span class="line">    grads[<span class="string">&#x27;W_embed&#x27;</span>] = word_embedding_backward(d_caption, caches.pop())</span><br><span class="line">    _, grads[<span class="string">&#x27;W_proj&#x27;</span>], grads[<span class="string">&#x27;b_proj&#x27;</span>] = affine_backward(dx, caches.pop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p><strong>sample 函数</strong>，位于<code>rnn.py</code> rnn类内。其要实现的效果如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/AtirSuRMczObNC9.png" alt="image-20200903234531509"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, features, max_length=<span class="number">30</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Run a test-time forward pass for the model, sampling captions for input</span></span><br><span class="line"><span class="string">    feature vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep, we embed the current word, pass it and the previous hidden</span></span><br><span class="line"><span class="string">    state to the RNN to get the next hidden state, use the hidden state to get</span></span><br><span class="line"><span class="string">    scores for all vocab words, and choose the word with the highest score as</span></span><br><span class="line"><span class="string">    the next word. The initial hidden state is computed by applying an affine</span></span><br><span class="line"><span class="string">    transform to the input image features, and the initial word is the &lt;START&gt;</span></span><br><span class="line"><span class="string">    token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For LSTMs you will also have to keep track of the cell state; in that case</span></span><br><span class="line"><span class="string">    the initial cell state should be zero.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Array of input image features of shape (N, D).</span></span><br><span class="line"><span class="string">    - max_length: Maximum length T of generated captions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - captions: Array of shape (N, max_length) giving sampled captions,</span></span><br><span class="line"><span class="string">      where each element is an integer in the range [0, V). The first element</span></span><br><span class="line"><span class="string">      of captions should be the first sampled word, not the &lt;START&gt; token.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = features.shape[<span class="number">0</span>]</span><br><span class="line">    captions = self._null * np.ones((N, max_length), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unpack parameters</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement test-time sampling for the model. You will need to      #</span></span><br><span class="line">    <span class="comment"># initialize the hidden state of the RNN by applying the learned affine   #</span></span><br><span class="line">    <span class="comment"># transform to the input image features. The first word that you feed to  #</span></span><br><span class="line">    <span class="comment"># the RNN should be the &lt;START&gt; token; its value is stored in the         #</span></span><br><span class="line">    <span class="comment"># variable self._start. At each timestep you will need to do to:          #</span></span><br><span class="line">    <span class="comment"># (1) Embed the previous word using the learned word embeddings           #</span></span><br><span class="line">    <span class="comment"># (2) Make an RNN step using the previous hidden state and the embedded   #</span></span><br><span class="line">    <span class="comment">#     current word to get the next hidden state.                          #</span></span><br><span class="line">    <span class="comment"># (3) Apply the learned affine transformation to the next hidden state to #</span></span><br><span class="line">    <span class="comment">#     get scores for all words in the vocabulary                          #</span></span><br><span class="line">    <span class="comment"># (4) Select the word with the highest score as the next word, writing it #</span></span><br><span class="line">    <span class="comment">#     (the word index) to the appropriate slot in the captions variable   #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># For simplicity, you do not need to stop generating after an &lt;END&gt; token #</span></span><br><span class="line">    <span class="comment"># is sampled, but you can if you want to.                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You will not be able to use the rnn_forward or lstm_forward       #</span></span><br><span class="line">    <span class="comment"># functions; you&#x27;ll need to call rnn_step_forward or lstm_step_forward in #</span></span><br><span class="line">    <span class="comment"># a loop.                                                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> we are still working over minibatches in this function. Also if   #</span></span><br><span class="line">    <span class="comment"># you are using an LSTM, initialize the first cell state to zeros.        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    next_h, _ = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    next_c = np.zeros((N, W_proj.shape[<span class="number">1</span>]))</span><br><span class="line">    word = self._start * np.ones((N,), dtype=np.int32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">        word, _ = word_embedding_forward(word, W_embed)</span><br><span class="line">        <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">            next_h, _ = rnn_step_forward(word, next_h, Wx, Wh, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_h, next_c, _ = lstm_step_forward(word, next_h, next_c, Wx, Wh, b)</span><br><span class="line">        out, _ = affine_forward(next_h, W_vocab, b_vocab)</span><br><span class="line">        word = out.argmax(axis=<span class="number">1</span>)</span><br><span class="line">        captions[:, i] = word</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> captions</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment3-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment3-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment3 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment2 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-12T09:16:42.000Z</published>
    <updated>2020-08-26T14:09:37.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment2-遇到的问题"><a href="#CS231n-Assignment2-遇到的问题" class="headerlink" title="CS231n Assignment2 遇到的问题"></a>CS231n Assignment2 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Fully-connected-Neural-Network"><a href="#Fully-connected-Neural-Network" class="headerlink" title="Fully-connected Neural Network"></a>Fully-connected Neural Network</h2><p>相比于Assignment1，对于整个网络的实现进行了进一步的封装，可以实现任意深度，大小的MLP。</p><div class="note info">            <p>值得一看的代码部分！solver.py中实现调用更新规则函数（在optim.py中实现），实现的很有趣！</p>          </div><p>基本思路为：使用getattr获取定义在optim.py中定义好的update rule函数！我是第一次见这种写法，感觉很巧妙，值得学习一波:)</p><p>当然，要先import optim。</p><p>Core Code(extract):</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.update_rule = kwargs.pop(<span class="string">&#x27;update_rule&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>)</span><br><span class="line">...</span><br><span class="line">   <span class="comment"># Make sure the update rule exists, then replace the string</span></span><br><span class="line">   <span class="comment"># name with the actual function</span></span><br><span class="line">   <span class="keyword">if</span> <span class="keyword">not</span> hasattr(optim, self.update_rule):</span><br><span class="line">       <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)</span><br><span class="line">self.update_rule = getattr(optim, self.update_rule)</span><br></pre></td></tr></table></figure><p>其余部分的实现（如：affine，ReLU的forward和backward；优化算法）注意好细节后都比较容易实现，因为比较复杂的代码框架已经提供好了。</p><p>值得注意的是，官方github仓库中的关于课程内容的markdown笔记值得一读。<a href="http://github.com/cs231n/cs231n.github.io" title="官方课程github仓库">课程github仓库</a>. </p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>本部分的实现我遇到了一些问题，在完成上也花费了很多时间，主要遇到的问题还是导数的求取，以及如何将其变化为numpy数组的形式。</p><h3 id="The-Gradient-of-Batch-Normalization"><a href="#The-Gradient-of-Batch-Normalization" class="headerlink" title="The Gradient of Batch Normalization"></a>The Gradient of Batch Normalization</h3><p>由于本部分想知道自己的代码是正确与否需要变为代码后，进行numerical check才能验证。所以，在第一次求导后，我花了很长时间debug，但最后发现是导数求错了。。。</p><p>求导中还是遇到了不少的问题，尤其是在使用链式法则的时候遇到了问题。看来是好久没有好好推公式了QuQ，于是重新复习了一下chain rule和矩阵求导之类的；并重新推到了一下公式。NOTE：以下推导可能并不非常严谨（部分可能不符合矩阵相乘维数），但作为示意和实现代码足够了。</p><p>首先是正向（forward pass）的公式。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/heQRF4Um6j7pOxk.png" alt="image-20200819013718586"></p><p>然后是backward计算梯度。这里在计算关于x的偏导时，我遇到了一些问题，很容易丢掉一个导数项；画出变量之间的关系图可以很好地解决这个问题。推导如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ovXSVBY5bPkcyA3.png" alt="image-20200819013948134.png"></p><p>有了这些部分，就可以实现第一个函数batchnorm_backward()和batchnorm_forward()了！</p><p>实际上，上式还可以继续化简，化简的结果更加简洁，省去很多中间变量。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/eudk5aMOFLJCApj.png" alt="image-20200819014219098"></p><p>至此，batch normalization的公式推导部分就OK了，下面就是代码实现了。</p><p>这里需要注意的是，由于在predict的时候，一般是没有batch数据的，所以此时没法直接使用batch normalization，所以一种方法就是利用训练时得到的均值和方差来作为predict时的均值和方差。其更新方法使用momentum更新为：</p><pre><code>running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var</code></pre><h3 id="Code-Implement-of-Batch-Normalization"><a href="#Code-Implement-of-Batch-Normalization" class="headerlink" title="Code Implement of Batch Normalization"></a>Code Implement of Batch Normalization</h3><p>forward没啥可说的，很easy，分开train与test即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span>(<span class="params">x, gamma, beta, bn_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: &#x27;train&#x27; or &#x27;test&#x27;; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mode = bn_param[<span class="string">&#x27;mode&#x27;</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        sample_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line"></span><br><span class="line">        x_norm = (x - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">        cache = x, x_norm, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        x_norm = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid forward batchnorm mode &quot;%s&quot;&#x27;</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward实现两个版本，我是一个根据未化简公式来计算，另一个是根据化简后公式来计算，对比后明显可以看到化简后大量减少了运算次数，可以达到原来速度的3倍。</p><p>未化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_norm.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_norm * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx_norm = dout * gamma</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - sample_mean), axis=<span class="number">0</span>) * np.power(sample_var + eps, <span class="number">-1.5</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(sample_var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">-1</span>/N) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=<span class="number">0</span>) + dL_dvar * np.sum(<span class="number">-2</span>*(x - sample_mean)/N, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / N) * (x - sample_mean) * dL_dvar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><p>化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dx = (<span class="number">1</span> / N) * mid * (N * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>LN按照如下公式来输出，实际上就是把BN倒过来。。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ugYP7bKtTAvhzOj.png" alt="image-20200819015437962"></p><p>LN的操作类似于将BN做了一个“<strong>转置</strong>”，对同一层网络的输出做一个标准化。注意，同一层的输出是单个图片的输出，比如对于一个batch为32的神经网络训练，会有32个均值和方差被得出，<strong>每个均值和方差都是由单个图片的所有channel之间做一个标准化</strong>。这么操作，就使得LN不受batch size的影响。</p><p>在代码的实现上只需将所有的相关矩阵装置一下就OK啦，即对于转置过来的输入x做BN即可！注意要保证输出的维数正确。</p><p>Layer Normalization Forward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_forward</span>(<span class="params">x, gamma, beta, ln_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During both training and test-time, the incoming data is normalized per data-point,</span></span><br><span class="line"><span class="string">    before being scaled by gamma and beta parameters identical to that of batch normalization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note that in contrast to batch normalization, the behavior during train and test-time for</span></span><br><span class="line"><span class="string">    layer normalization are identical, and we do not need to keep track of running averages</span></span><br><span class="line"><span class="string">    of any sort.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - ln_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">        - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    sample_var = np.var(x.T, axis=<span class="number">0</span>)</span><br><span class="line">    x_norm = (x.T - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">    out = gamma * x_norm.T + beta</span><br><span class="line">    cache = x, x_norm.T, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>Layer Normalization Backward：这里我实现了两个版本，分别是基于化简后的公式和未化简后的公式，均通过测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you can heavily rely on the work you&#x27;ve done already</span></span><br><span class="line"><span class="string">    for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from layernorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line"></span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dxhat = dxhat.T</span><br><span class="line">    x_hat = x_hat.T</span><br><span class="line">    dx = (<span class="number">1</span> / D) * mid * (D * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line">    dx = dx.T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment">#    Another vision of  LN backward (based on the origin vision of bn backward)     #</span></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment"># x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span></span><br><span class="line">    <span class="comment"># N, D = x_norm.shape</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dbeta = np.sum(dout, axis=0)</span></span><br><span class="line">    <span class="comment"># dgamma = np.sum(x_norm * dout, axis=0)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># x = x.T</span></span><br><span class="line">    <span class="comment"># dout = dout.T</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dx_norm = dout.T * gamma</span></span><br><span class="line">    <span class="comment"># dx_norm = dx_norm.T</span></span><br><span class="line">    <span class="comment"># dL_dvar = -0.5 * np.sum(dx_norm * (x - sample_mean), axis=0) * np.power(sample_var + eps, -1.5)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx = dx_norm / np.sqrt(sample_var + eps)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (-1/D) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=0) + dL_dvar * np.sum(-2*(x - sample_mean)/N, axis=0)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;var--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (2 / D) * (x - sample_mean) * dL_dvar</span></span><br><span class="line">    <span class="comment"># dx = dx.T</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="BN-vs-LN"><a href="#BN-vs-LN" class="headerlink" title="BN vs LN"></a>BN vs LN</h3><h4 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h4><p>首先是BN，BN是通过mini-batch来对相应的activation做规范化操作，使得输出的各个维度的均值为0，方差为1（标准化）。而最后的“scale and shift”，即加入一个放射变换，则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入，同时也缓解因为数据可能会因此丢失了一些信息，所以再加上beta和gama来恢复原始数据，这里beta和gama是可学习的。</p><p><strong>BN的好处：</strong></p><p>(1) 减轻了对参数、权重初始化的依赖。</p><p>(2) 训练更快，可以使用更高的学习率。</p><p>(3) BN一定程度上增加了泛化能力。</p><p><strong>BN的缺点：</strong></p><p>batch normalization依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。会引入很多噪声误差，若网络队伍差很敏感，则会难以训练和收敛。</p><p>这一个特性，导致batch normalization不适合以下的几种场景。</p><p>(1)batch非常小，比如训练资源有限无法应用较大的batch。</p><p>(2)RNN，因为它是一个动态的网络结构，即输入的size是不固定的，同一个batch中训练实例有长有短，无法根据BN的公式进行标准化。</p><p>关于Normalization的<strong>有效的原因</strong>：</p><p>Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，目前大部分资料都这样解释，比如BN的原始论文认为的缓解了Internal Covariate Shift(ICS)问题。加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。</p><h4 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h4><p>BN 的一个缺点是需要较大的 batchsize 才能合理估训练数据的均值和方差，这在计算资源比较有限的时候往往不能达到，同时它也很难应用在数据长度不同的 RNN 模型上。Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化，他是针对于per datapoint的更新。</p><p>整体而言，LN用于RNN效果比较明显，但是在CNN上，不如BN。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout的代码部分非常简单，material中已经给出了代码实现，只需要实现一下forward和backw以及更新一下计算loss的函数即可。需要注意的是增加loss的部分，这里我使用caches当做<strong>堆栈</strong>存储前向计算loss时产生的caches，这样反向传播时只需要依次pop并根据网络结构计算梯度即可。本部分代码位于：<code>cs231n/classifiers/fc_net.py</code>。 该loss位于为FullyConnectedNet类内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute loss and gradient for the fully-connected net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / output: Same as TwoLayerNet above.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X = X.astype(self.dtype)</span><br><span class="line">    mode = <span class="string">&#x27;test&#x27;</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">&#x27;train&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">    <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">    <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">        self.dropout_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    <span class="keyword">if</span> self.normalization==<span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">            bn_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    scores = X</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">        W = self.params[<span class="string">&#x27;W&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        b = self.params[<span class="string">&#x27;b&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            scores, cache = affine_forward(scores, W, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                scores, cache = affine_relu_forward(scores, W, b)</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;batchnorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_bn_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;layernorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_ln_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cache = <span class="literal">None</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout <span class="keyword">and</span> i != self.num_layers<span class="number">-1</span>:</span><br><span class="line">            scores, cache = dropout_forward(scores, self.dropout_param)</span><br><span class="line">            caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If test mode return early</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    reg = self.reg</span><br><span class="line">    loss, dx = softmax_loss(scores, y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(self.num_layers)):</span><br><span class="line">        w = <span class="string">&#x27;W&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        b = <span class="string">&#x27;b&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        gamma = <span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        beta = <span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        loss += <span class="number">0.5</span> * reg * np.sum(W * W)  <span class="comment"># add reg term</span></span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            dx, grads[w], grads[b] = affine_backward(dx, caches.pop())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">                dx = dropout_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                dx, grads[w], grads[b] = affine_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_bn_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;layernorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_ln_relu_backward(dx, caches.pop())</span><br><span class="line">        grads[w] += reg * self.params[w]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><h2 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h2><p>这部分就是实现CNN了！核心就是实现好卷积层和pooling层。同时也修改batch normalization以便适用于CNN，同时增加group normalization。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>由于在实现时并不需要考虑计算复杂度和时间复杂度，我使用了最简单直接的方法，在forward时，同官方给的note一样，每次更新一个激活神经元的值，即使用4层循环嵌套，直观的实现卷积的过程。TODO：向量化方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span>(<span class="params">x, w, b, conv_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - &#x27;pad&#x27;: The number of pixels that will be used to zero-pad the input. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, &#x27;pad&#x27; zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) / stride)</span><br><span class="line">    out = np.zeros((N, F, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.sum(</span><br><span class="line">                        x_pad[n, :, h * stride:h * stride + HH, w_mid * stride:w_mid * stride + WW] * w[f, :, :, :]) + b[f]</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>在实现backward时，我也写出了简单情况下更新的公式，并根据这个最简单的展开形式以此来反向求梯度。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/26/7R2qvGbEiOLPgZw.png" alt="image-20200826005100903"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, F, H_out, W_out = dout.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    dx_pad = np.zeros_like(x_pad)</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.sum(dout, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW]</span><br><span class="line">                    dx_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW] += \</span><br><span class="line">                        dout[n, f, h_mid, w_mid] * w[f, :, :, :]</span><br><span class="line">                    dw[f, :, :, :] += window * dout[n, f, h_mid, w_mid]</span><br><span class="line">    dx = dx_pad[:, :, pad:pad + H, pad:pad + W]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>forward很简单，只需取respect field中最大的即可；backward时，将取最大值的位置处的梯度直接回传，其余置一即可。比较简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span>(<span class="params">x, pool_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;pool_height&#x27;: The height of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;pool_width&#x27;: The width of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here. Output size is given by </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H - pool_height) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W - pool_width) / stride)</span><br><span class="line">    out = np.zeros((N, C, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.max(</span><br><span class="line">                        x[n, f, h * stride:h * stride + pool_height, w_mid * stride:w_mid * stride + pool_width])</span><br><span class="line"></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H_out, W_out = dout.shape</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                             stride * w_mid:stride * w_mid + pool_width]</span><br><span class="line">                    mask = window == np.max(window)</span><br><span class="line">                    dx[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                    stride * w_mid:stride * w_mid + pool_width] = mask * dout[n, f, h_mid, w_mid]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h3 id="Spatial-Batch-Normalization"><a href="#Spatial-Batch-Normalization" class="headerlink" title="Spatial Batch Normalization"></a>Spatial Batch Normalization</h3><p>实现起来非常简单，只需要重新reshape输入，之后使用之前实现过的正常的Batch Normalization就OK了，代码请看我的github仓库，这部分没有遇到问题。</p><h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><p>forward只需要稍微修改正常的Batch Normalization即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_forward</span>(<span class="params">x, gamma, beta, G, gn_param</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = gn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_new = x.reshape((N, G, C // G, H, W))</span><br><span class="line">    mean = np.mean(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var = np.var(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_norm = (x_new - mean) / np.sqrt(var + eps)</span><br><span class="line">    x_norm = x_norm.reshape((N, C, H, W))</span><br><span class="line">    gamma_new = gamma.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    beta_new = beta.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    out = gamma_new * x_norm + beta_new</span><br><span class="line">    cache = G, x, x_norm, mean, var, gamma, beta, eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward也并不复杂，本质上的求导与正常的batch normalization一致，不过在多个导数求和时，需要注意怎么进行sum。这里如果想要通过Gradient check也有一个小坑。。。 ps: 这里我调试了很久。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    G, x, x_norm, mean, var, gamma, beta, eps = cache</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout * x_norm, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.reshape(N, G, C // G, H, W)</span><br><span class="line">    <span class="comment"># 这里想通过Gradientcheck必须需要将其reshape为(1, C, 1, 1)</span></span><br><span class="line">    dbeta = np.sum(dout, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    dx_norm = (dout * gamma).reshape(N, G, C // G, H, W)</span><br><span class="line">    mean = mean.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    var = var.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - mean), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)) * np.power(var.squeeze() + eps, <span class="number">-1.5</span>)</span><br><span class="line">    dL_dvar = dL_dvar.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    mid = H * W * C // G</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += ((<span class="number">-1</span> / mid) * np.sum(dx_norm / np.sqrt(var + eps), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>) + dL_dvar * (</span><br><span class="line">        np.sum(<span class="number">-2</span> * (x - mean) / mid, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / mid) * (x - mean) * dL_dvar</span><br><span class="line">    dx = dx.reshape((N, C, H, W))</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h2 id="PyTorch-on-CIFAR-10"><a href="#PyTorch-on-CIFAR-10" class="headerlink" title="PyTorch on CIFAR-10"></a>PyTorch on CIFAR-10</h2><p>这部分比较简单，我在实现时没有遇到问题。偷了懒，没有实现最后的CIFAR-10 open-ended challenge。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment2-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment2-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment2 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>单片机解决方案调研</title>
    <link href="http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/"/>
    <id>http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/</id>
    <published>2020-08-12T02:35:36.000Z</published>
    <updated>2020-08-27T08:40:46.471Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单片机解决方法调研"><a href="#单片机解决方法调研" class="headerlink" title="单片机解决方法调研"></a>单片机解决方法调研</h1><p>目标：<strong>模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线通信；具有一定的算力</strong></p><p>本文主要讨论3种不同解决方案。Arduino，stm32和C51系列。列出的这三种都是我有过使用经历的。我经验较多的是stm32，没有用arduino做过比较大型的东西。</p><p>实际上三者并不能直接比较，Arduino算是一个硬件平台，他的早期，也是最广泛的核心是基于AVR单片机（这种芯片我没单独用过）。</p><p>后两者stm32与C51则是两种特定系列的单片机了。</p><table><thead><tr><th></th><th>Arduino</th><th>stm32</th><th>C51</th></tr></thead><tbody><tr><td>模块化</td><td>强（有很多各种各样现成的模块）</td><td>中（配合开发板使用，可以达到部分模块化的效果）</td><td>中（同stm32）</td></tr><tr><td>运算能力</td><td>中、高（一般使用AVR的算力差，现在有支持STM32系列的和esp32的了）</td><td>中、高</td><td>低</td></tr><tr><td>功耗</td><td>低、电池供电足够</td><td>低（极低）、电池供电足够</td><td>极低、电池供电足够</td></tr><tr><td>开发难度</td><td>低、开发速度快、代码易于迭代更新，不必考虑寄存器层面编程</td><td>较高（寄存器复杂，但ST提供封装的的库函数）</td><td>中（硬件资源少，编程难度高）</td></tr><tr><td>价格</td><td>较高</td><td>中</td><td>极低</td></tr><tr><td>支持无线通信</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>优点</td><td>开放周期较短，模块化强，代码移植性强，社区丰富</td><td>功能强大，增加功能灵活，社区丰富</td><td>极其便宜，功耗低</td></tr><tr><td>缺点</td><td>扩展模块可选有限；算力有限；</td><td>开发难度大；程序移植性差；</td><td>算力低下，框架老旧</td></tr><tr><td>总结&amp;建议</td><td><strong>可选方案</strong>。方便开发，算力比较OK；但价格较高。</td><td><strong>可选方案</strong>。芯片功能极其强大；但开发周期和难度可能较长。</td><td>不建议使用，如果要批量生成且算力要求不高，可以考虑</td></tr></tbody></table><p><strong>TODO：</strong></p><ul><li>初步分别设计一套Arduino方案和STM32方案</li></ul><h2 id="Arduino"><a href="#Arduino" class="headerlink" title="Arduino"></a>Arduino</h2><p>Arduino准确的说是一个单片机及其外设的集合，比较经典板子的主控是ATMEL出的AVR单片机，比51系列性能强一点。这个集合之所以出名在于其操作简单，不需要涉及很多底层、寄存器层面的编程。例如，stm32库函数的一大堆命令，在这里只需要一句即可完成功能，并且有相当丰富的外设模块。</p><p>总体而言做原型，快速开发的时候，硬件搭设方便，基本不用去设计电路板，画板子之类的，基本上导线连接模块就OK了。代码比较简单易懂的。基本不涉及到寄存器级的操作。总得来说就是开发快。小量定制化还是划算，做产品或者较多数量的成本很高；且由于其代码的高度封装会导致程序效率底下以及资源开销大。</p><p>我个人对于Arduino的使用不是很多，还需进一步调研。</p><p>该图为比较常见的Arduino型号的单片机的性能参数。<a href="https://www.arduino.cn/thread-42417-1-1.html">原文链接</a> 原文发布于2017年</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/aodKOxTjRWQztpb.jpg" alt="211543xjgg8attjataqkgk"></p><h2 id="STM32系列单片机"><a href="#STM32系列单片机" class="headerlink" title="STM32系列单片机"></a>STM32系列单片机</h2><p>stm32是st半导体公司向arm公司购买了核心(嵌入式)版权，加上自己的外设生产的一个系列的芯片。其特点是：功能强大、速度快、外设多。STM32比较常见的框架是ARM CORTEX-m3或m4。并且其：寄存器复杂，直接用汇编操作比较麻烦，但ST官方了提供封装的的库函数，现在还出了专门的代码生成软件cube来简化操作。</p><p>一个STM32常用型号之间对比：<a href="https://blog.csdn.net/ybhuangfugui/article/details/88266385">https://blog.csdn.net/ybhuangfugui/article/details/88266385</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/nkjBiR6fT2lSQb7.jpg" alt="en.microcontrollers_FM141.jpg"></p><p>上图为ST官网对于其系列芯片的简介与分类。</p><p>我对于STM32系列芯片的使用比较有经验，我使用的型号为主要为：STM32F1系列和F4系列。</p><p>STM32F407这款芯片，使我们机器人队使用的主控芯片，其最高主频可达<strong>168MHz</strong>（远远大于Arduino的常见型号的16MHz），可见其算力的强大。</p><p>我们队内并没有使用STM32系列的开发板，而是买了裸的芯片，之后自己设计了电路板（设计部分我不是很擅长），画板子、印板子、焊板子、改板子这样进行开发，导致开发一代新的主控板周期比较长。</p><p>不过市面上也有很多STM32现成的开发板，预留了很多IO接口，初步观察感觉基本满足需求，价格适中，如STM32F4系列的开发板不足100元。使用这种开发板一般也不需要自己设计电路，只需购买不同的模块即可交互使用。</p><center class="half">    <img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/7DGvqR42cTMiNVA.jpg width="400"/><img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/gTPjAvzL4KEnIFC.jpg width="400"/> </center><p>左图的为我近期购买的STM32F1系列的的开发板；右图为STM32F4系列的开发板。例如左边的F1开发板，可以看到这种开发板也像Arduino一样支持很多的扩展功能，而且只需要插接即可。</p><p>整体而言，使用STM32系列也绝对可以实现我们的预期目标，使用开发板也基本可以避免电路的设计等工作。由于STM32系列芯片本身功能强大，其上限应该是高于Arduino方案的。但其开发难度会比Arduino方案难上不少。这里指的STM32开发方案是指使用STM32 CUBEMX硬件配置和生成代码模板（HAL库），之后在代码模板上进行开发（一般使用IAR作为IDE）。</p><h2 id="Arduino-与-STM32"><a href="#Arduino-与-STM32" class="headerlink" title="Arduino 与 STM32"></a>Arduino 与 STM32</h2><p>通过进一步的了解，我发现了arduino支持了STM32的开发！即可以使用Arduino的IDE来编程。这样可能会降低部分开发难度。</p><p>github链接<a href="https://github.com/rogerclarkmelbourne/Arduino_STM32">https://github.com/rogerclarkmelbourne/Arduino_STM32</a>。目前支持STM32F4和F1系列，其可以将Stm32F103（主要）系列单片机刷入Arduino的Bootloader，并且使用Arduino的编译器和IDE来完成代码的编写，省去了一大部分配置寄存器和学习的时间，完整的性能和灵活性还有待探究。</p><p>同时也有一个类似于arduino+STM32的project，其链接如下。<a href="https://www.leaflabs.com/maple">https://www.leaflabs.com/maple</a>。该板子在淘宝有售，其芯片使stm32f103 arm cortex-M3 32位处理器，主频最高可达72MHz，远远大于常见的Arduino的8位（AVR）MCU。</p><p>Leaf Maple 是一个类似Arduino的开发平台，使用的Cotrex M3内核的32位MCU，所以要比Arduino的8位（AVR）MCU强悍很多，有更高的主频，更丰富的资源。 Leaf Maple也提供了一个类似Arduino IDE的IDE， 并且很多简单上层函数兼容Arduino的函数库，让移植库和代码变得相当简单。比起使用CubeMX+IAR来开发一个STM32项目，使用leaf maple会节省很多时间，更适合初学者和需要快速原型开发的用户。</p><h2 id="Arduino新品M5Stack（使用esp32）"><a href="#Arduino新品M5Stack（使用esp32）" class="headerlink" title="Arduino新品M5Stack（使用esp32）"></a>Arduino新品M5Stack（使用esp32）</h2><p>Arduino的生态总体来讲还是很好用的，目前了解到一款<strong>模块化极强</strong>，性能出色，上市时间不长的支持Arduino开发平台的开发板。M5系列。</p><p>M5Stack是一种模块化、可堆叠扩展的开发板，每个模块一般为5cmX5cm的尺寸，这也是M5Stack名字的由来。与常规的开发板不同，M5Stack更注重产品形态的完整性，更注重用户的应用场景和研发的简易性，M5没有密密麻麻的飞线，没有错乱无章的接口插头，不需要繁琐的开发流程，简简单单、轻轻松松地完成高质量的电子原型创作。（官方介绍）</p><p>M5Stack主要采用ESP32芯片体系，CORE主机内已集成了240M双核主频CPU（esp32）、 WiFi、蓝牙、2.0寸彩色屏幕LCD、扬声器、按键、TF卡、陀螺仪以及内置电池（部分有）。CORE基本满足一般的功能需求，功能模块Function Module则根据应用的情况选择，比如电机驱动、信号采集、通信等功能。另外，也会配备不同的应用底座及配件，方便用户做出高质量的研发。</p><p><a href="https://github.com/m5stack">官方github</a> 与 <a href="https://m5stack.taobao.com/index.htm?spm=2013.1.w5002-22404213498.2.2149622fvBd0zs">官方淘宝链接</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/oXGRIYD8r7OMFq6.jpg" alt="img"></p><p>比如其Basic款。其内含2.4G Wi-Fi和蓝牙4.0。外设接口有Type-c，I2C，GPIO和UART接口（数量较少）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/gRsKaQhdYloI5Ut.png" alt="image-20200827162008345"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;单片机解决方法调研&quot;&gt;&lt;a href=&quot;#单片机解决方法调研&quot; class=&quot;headerlink&quot; title=&quot;单片机解决方法调研&quot;&gt;&lt;/a&gt;单片机解决方法调研&lt;/h1&gt;&lt;p&gt;目标：&lt;strong&gt;模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="单片机" scheme="http://canVa4.github.io/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="arduino" scheme="http://canVa4.github.io/tags/arduino/"/>
    
      <category term="STM32" scheme="http://canVa4.github.io/tags/STM32/"/>
    
  </entry>
  
  <entry>
    <title>Python学习杂记(1)</title>
    <link href="http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/"/>
    <id>http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/</id>
    <published>2020-08-08T13:09:52.000Z</published>
    <updated>2020-08-15T13:03:30.917Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python学习杂记-1"><a href="#Python学习杂记-1" class="headerlink" title="Python学习杂记(1)"></a>Python学习杂记(1)</h1><p>本篇文章是为了记录进一步学习python时遇到的一些问题和相对陌生的知识点。</p><ul><li><p>range()函数返回的是一个独特的“范围类”对象！</p></li><li><p>for 变量 in 字符串|集合|范围|任何可迭代对象:</p><p> <strong>可迭代对象：指该对象中包含一个–iter–方法</strong></p></li><li><p><strong>isinstance()函数</strong>，判断某个变量是否为指定类型的实例，前一个参数是要判断的变量，后一个参数是类型。如：isinstance(2, int)</p></li><li><p>zip()函数，将多个列表压缩为一个zip对象（可迭代对象），这样就可以使用一个循环遍历两个列表</p></li><li><p>reversed()函数：接受序列（元组、列表、区间等），然后返回一个“反序排列”的迭代器，sorted()类似</p></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python学习杂记-1&quot;&gt;&lt;a href=&quot;#Python学习杂记-1&quot; class=&quot;headerlink&quot; title=&quot;Python学习杂记(1)&quot;&gt;&lt;/a&gt;Python学习杂记(1)&lt;/h1&gt;&lt;p&gt;本篇文章是为了记录进一步学习python时遇到的一些问题
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment1 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-07T08:21:14.000Z</published>
    <updated>2020-08-08T13:01:59.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment1-遇到的问题"><a href="#CS231n-Assignment1-遇到的问题" class="headerlink" title="CS231n Assignment1 遇到的问题"></a>CS231n Assignment1 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><h2 id="Softmax-implement"><a href="#Softmax-implement" class="headerlink" title="Softmax implement"></a>Softmax implement</h2><p>不论是实现softmax，SVM损失函数，二者遇到的问题都比较相似，主要为<strong>导数的推导</strong>和<strong>numpy的使用</strong>。由于softmax的实现稍微复杂一些，这里只记录softmax实现时的问题。</p><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>使用SGD核心的工作就是计算softmax关于权值W的梯度。课程中没有给出推导过程，这里推导一下。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/mg9rJC2LzVaAxO3.png" alt="image-20200807173340447"></p><h3 id="Numeric-Stability-Trick"><a href="#Numeric-Stability-Trick" class="headerlink" title="Numeric Stability Trick"></a>Numeric Stability Trick</h3><p>为了防止出现数值计算不稳定，要在计算损失函数式加入修正项（对Gradient无影响）。</p><p>原始为：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/w469bMc7skBYd52.png" alt="image-20200807174209667" style="zoom:50%;" /></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/t7zyhReGVp6QEi2.png" alt="image-20200807174255380" style="zoom: 80%;" /><h3 id="Implement-with-numpy"><a href="#Implement-with-numpy" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><h4 id="Navie-Version"><a href="#Navie-Version" class="headerlink" title="Navie Version"></a>Navie Version</h4><p>给出naive版本的代码。如何计算的示意图已在推导过程中给出。naive版本的代码基本按照推导的公式梳理下来即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)    <span class="comment"># 一个数值修正的技巧，防止出现数值不稳定的问题</span></span><br><span class="line">        scores = np.exp(scores)</span><br><span class="line"></span><br><span class="line">        sum_scores = np.sum(scores)        <span class="comment"># 可以简化写法，节省空间，懒得修改了</span></span><br><span class="line">        P = scores / sum_scores</span><br><span class="line">        L = -np.log(P)</span><br><span class="line"></span><br><span class="line">        loss += L[y[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):    <span class="comment"># 计算梯度，分类讨论</span></span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:, j] += (<span class="number">-1</span> + P[y[i]])*X[i].T</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dW[:, j] += P[j]*X[i].T</span><br><span class="line"></span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += reg * W</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h4 id="Vectorized-Version"><a href="#Vectorized-Version" class="headerlink" title="Vectorized Version"></a>Vectorized Version</h4><p>向量化版本。这里就有非常多的细节需要注意了。首先还是给出完整代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    scores = X @ W  <span class="comment"># ( N*C )</span></span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss += np.sum(L[np.arange(num_train), y])</span><br><span class="line"></span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)   <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算gradient</span></span><br><span class="line">    mid = np.zeros_like(P)  <span class="comment"># 生成一个和P一样的0矩阵</span></span><br><span class="line">    mid[np.arange(num_train), y] = <span class="number">1</span>  <span class="comment"># 对矩阵中Y所对应的部分加一个1，因为一会要构造出需要的梯度计算</span></span><br><span class="line">    dW = X.T @ (-mid + P)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>首先应该画图明白计算中各个量的关系，以及他们是怎么来的，这个很重要。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/Knh24oeJ53Nydqj.png" alt="image-20200807175314544"></p><p>第一处就是在计算Numeric Stability Trick时，要找到每一个输入向量的最大元素，这里注意需要保证keepdims=True。</p><p>其控制了返回数组的shape，这样返回的shape为(N,1)。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>同理在sum时，也要进行类似的处理，这样在归一化时才能work。</p><h4 id="An-Important-Trick"><a href="#An-Important-Trick" class="headerlink" title="An Important Trick!!!"></a>An Important Trick!!!</h4><p>在这里我遇到了不少的问题，主要是numpy使用的不熟练。。。:( 所以特此记录下来。</p><p><strong>L[np.arange(num_train), y] **与 **L[:,y]</strong> 的区别！</p><p>一开始的代码使用的是后者，因为目标就是获得所有行i中，列位置为y[i]的元素。所以想当然的使用了后者。但实际上，后者返回的是所有行x[i]中，x[i,y[j]]的元素！！！</p><p>示例如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/dKFBsGk3qzxbV5c.png" alt="image-20200807180256834"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/QbpEhW9DK63ex1F.png" alt="image-20200807180318182"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/2meKSxGVvcsb3zA.png" alt="image-20200807180331281"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/jJGhMKFWD7iTULy.png" alt="image-20200807180431683"></p><p>而**L[np.arange(num_train), y] **则为：</p><p>如果将np.arange(num_train)看为list，则其长度必须与y相同！！！其效果就是二者分别迭代，每次返回二者迭代结果下标位置处的元素。如图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/d7nQCOvX8Goue9z.png" alt="image-20200807180731248"></p><p>所以可见，基于我们的需要，后者才能满足要求。</p><h2 id="Two-Layer-Neural-Network"><a href="#Two-Layer-Neural-Network" class="headerlink" title="Two-Layer Neural Network"></a>Two-Layer Neural Network</h2><p>本部分的工作也与之前的部分比较相似，这里遇到主要问题还是如何处理求导的问题。</p><p>由于在这里我也遇到了一些问题，所以再次给出部分求导流程。</p><p>首先先给出网络的结构。</p><h3 id="Gradient-1"><a href="#Gradient-1" class="headerlink" title="Gradient"></a>Gradient</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/8yBxLVldSgqsrmZ.png" alt="image-20200807221946519"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/pb8PvAu7RjhNJHT.png" alt="image-20200807223402245"></p><h3 id="Implement-with-numpy-1"><a href="#Implement-with-numpy-1" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><p>下面给出代码实现。由于主要难点就是loss的实现了，之后的SGD和predict函数都非常简单，我没有遇到什么问题，这里只给出遇到了部分问题的loss与grad计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None, reg=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">    W1, b1 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2, b2 = self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">    N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    h = np.maximum(X @ W1 + b1, <span class="number">0</span>)</span><br><span class="line">    scores = h @ W2 + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we&#x27;re done</span></span><br><span class="line">    <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss = np.sum(L[np.arange(N), y])</span><br><span class="line"></span><br><span class="line">    loss /= N</span><br><span class="line">    loss += <span class="number">1</span> * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算W2，b2</span></span><br><span class="line">    dscore = P</span><br><span class="line">    dscore[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">    dscore /= N        <span class="comment"># 这里需要注意！！！</span></span><br><span class="line">    <span class="comment"># 计算梯度时只需要除一次N，这里debug花了很久。。</span></span><br><span class="line">    grads[<span class="string">&#x27;W2&#x27;</span>] = h.T @ dscore + <span class="number">2</span> * reg * W2</span><br><span class="line">    grads[<span class="string">&#x27;b2&#x27;</span>] = np.sum(dscore, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 计算W1，b1</span></span><br><span class="line">    dh = dscore @ W2.T      <span class="comment"># 目标函数对于h的偏导</span></span><br><span class="line">    dh[h &lt;= <span class="number">0</span>] = <span class="number">0</span>          <span class="comment"># 此时dh变为关于w1@x+b1的导数</span></span><br><span class="line">    grads[<span class="string">&#x27;W1&#x27;</span>] = X.T @ dh + <span class="number">2</span> * reg * W1</span><br><span class="line">    grads[<span class="string">&#x27;b1&#x27;</span>] = np.sum(dh, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p>基本上按照公式并注意矩阵维数和细节就OK了，遇到不太会的画个图就解决了。</p><div class="note warning">            <p>需要注意的是，在除以输入个数的时候，只需要除一次</p>          </div><p>这里一开始没有注意到，我一开始在每次计算梯度的时候都除了N，导致出现了误差，这里居然debug了很久。。</p><h3 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h3><p>有点懒，这部分工作没有完成。</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>其余的部分（k-Nearest Neighbor classifier, SVM, Higher Level Representations: Image Features）并未遇到很多的问题。具体详情代码见我的github仓库。<a href="https://github.com/canVa4/CS231n-Assignments">https://github.com/canVa4/CS231n-Assignments</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment1-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment1-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment1 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR论文复现</title>
    <link href="http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    <id>http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-08-05T16:31:51.000Z</published>
    <updated>2020-08-08T03:30:00.768Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p>整体的代码使用pytorch实现，基于<a href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）</p><a id="more"></a><p>训练为：<em>run.py</em>；修改训练参数、Loss、数据集等需要修改：<em>config.yaml</em> ；评估使用<em>evluation.py</em>（测试训练分开的原因是因为我租了GPU，用GPU训练，用我的PC测试，这样可以更快一些）</p><p>个人运行环境：win10 + pytorch 1.5 + cuda 10.2（租的GPU 1080ti）</p><table><thead><tr><th>日期</th><th>进度</th></tr></thead><tbody><tr><td>5-19 Tue（基本满课+实验）</td><td>论文阅读，选定使用pytorch实现和决定基于上文链接实现代码</td></tr><tr><td>5-20 Wed</td><td>熟悉基础知识、了解代码整体框架，理解loss function，并进行初步尝试编写loss，未调试</td></tr><tr><td>5-21 Thu（满课+实验）</td><td>写完了evaluation部分</td></tr><tr><td>5-22 Fri（基本满课）</td><td>跑代码，发现只用CPU究极龟速；于是装cuda，结果装了一白天的cuda T.T，晚上测试代码并初步验证loss function是否书写正确；初步移植LARS</td></tr><tr><td>5-23 Sat</td><td>测试三个Loss并尝试调参，尝试使用resnet18作为backbone网络，旁晚开始租了个GPU来跑模型，实现triplet loss(sh)</td></tr><tr><td>5-24 Sun</td><td>调参、修复bug、跑代码、微调loss（Logistic loss增加欠采样和改变权重）</td></tr><tr><td>5-25 Mon</td><td>调参、跑代码</td></tr></tbody></table><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Linear evaluation均使用Logistic Regression，均train from scratch（no pretrain）</p><p>GPU: 1080ti    resnet50训练+测试一次需5.5h；resnet18训练+测试一次需2.6h；总代码运行时间：约75h（包括未列出测试）</p><table><thead><tr><th>batch</th><th>epoch</th><th>out dim</th><th>optimizer</th><th>Loss</th><th>BackBone</th><th>t/m</th><th>CIARF10 Top-1</th></tr></thead><tbody><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>0.1</td><td>78.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>79.3%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>1</td><td>77.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.4</td><td>65.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.8</td><td>70.7%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss(sh)</td><td>resnet50</td><td>0.8</td><td>73.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(none)</td><td>resnet50</td><td>0.2</td><td>37.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.2</td><td>62.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.5</td><td>69.9%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>1</td><td>65.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>LARS</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>TODO</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet18</td><td>0.5</td><td>71.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(weight)</td><td>resnet18</td><td>0.2</td><td>66.5%</td></tr></tbody></table><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于每一个输入图片，模型会生成两个representation，最终优化的目标可以理解为：同一个batch内来自同一张图片的两个representation的距离近，让来自不同输入图片的representation的距离远。注意，论文中给出的是negative loss function</p><h3 id="Logistic-Loss"><a href="#Logistic-Loss" class="headerlink" title="Logistic Loss"></a>Logistic Loss</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ log \sigma(u^Tv^+/\tau) + log\sigma(-u^T v^-/ \tau) $$</li></ul><p>这里对于此公式，我一开始是没有理解的，于是自己尝试推理了一下。</p><p>对于每一个输入样本，模型会生成两个representation，对于一个有N个输入的batch的，就会产生2*N个representation，对于每一对representation计算一个cosine similarity。而每一对representation（下文用 $(i,j)$ 序偶来表示他们）可以根据他们的来源来确定他们label（即：来自同一输入的为正类，来自不同输入的为反类），这样就构成了一个监督任务。</p><p>将这个任务看为监督后，因为论文中提到的这个损失函数的名字是logistic loss，我自然地想到了logistics regression。于是从这个角度入手，来推理这个loss function。</p><p>用$ P(i,j) $表示一对representation为正类的概率。设正类y=1，反类y=0</p><p>那么写出整个数据集的对数似然函数$$ LL(\theta;X)=\sum_{each(i,j)} (y_{(i,j)} logP(i,j)+(1-y_{(i,j)})log(1-P(i,j)) )$$</p><p>对上式化简可以得到：$$ LL(\theta;X)=\sum_{正类} logP(i,j)+\sum_{反类}log(1-P(i,j)) $$</p><p>而cosine similarity并不是一个[0,1]之间的数（或者说没有概率的意义），参照logistics regression，将cosine similarity经过一个sigmoid函数$$ \sigma( \cdot) $$ 之后就变为了一个[0,1之间的数]，而且对于sigmoid有$$ \sigma(-x)=1-\sigma(x) $$,所以有：$$ LL(\theta;X)=\sum_{正类} log[\sigma(sim(i,j))]+\sum_{反类}log[\sigma(-sim(i,j))] , sim(i,j)为(i,j)的相似度指标$$</p><p>只需引入temperature就可将上式变为与论文中公式相同的形式。</p><p>在使用原版loss时，发现最终结果效果很差（见result中的NT-Logistics none）。个人猜测原因如下：</p><ul><li>样本非常不均衡，正例对远远少于反例。</li></ul><p>解决办法：</p><ul><li>对反例样本对使用简单的<em>under-sampling</em>（欠采样）</li><li>对于loss计算时，正反例样本<em>设置不同的权重</em>（效果更好，因为欠采样会丢失部分信息）</li></ul><p>（注：由于训练时间太久，没有来得多次跑weight测试效果）</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line">    representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line">    l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line">    r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line">    positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line">    negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>) * <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    logits_pos = self.sigmoid(positives / self.temperature).log_()</span><br><span class="line">    logits_neg = self.sigmoid(negatives / self.temperature).log_()</span><br><span class="line">    <span class="keyword">if</span> self.method == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># under-sampling</span></span><br><span class="line">        all_one_vec = np.ones((<span class="number">1</span>, <span class="number">2</span> * self.batch_size,))</span><br><span class="line">        all_zero_vec = np.zeros((<span class="number">1</span>, <span class="number">2</span> * self.batch_size * (<span class="number">2</span> * self.batch_size - <span class="number">3</span>)))</span><br><span class="line">        under_sampling_matrix = np.column_stack((all_one_vec, all_zero_vec)).flatten()</span><br><span class="line">        np.random.shuffle(under_sampling_matrix)</span><br><span class="line">        under_sampling_matrix = torch.tensor(under_sampling_matrix).view(</span><br><span class="line">            (<span class="number">2</span> * self.batch_size, <span class="number">2</span> * self.batch_size - <span class="number">2</span>)).type(torch.bool).to(self.device)</span><br><span class="line"></span><br><span class="line">        logits_neg = logits_neg[under_sampling_matrix]</span><br><span class="line">        loss = torch.sum(logits_pos) + torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    <span class="keyword">elif</span> self.method == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># change weight</span></span><br><span class="line">        neg_count = <span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>)</span><br><span class="line">        pos_count = <span class="number">2</span>*self.batch_size</span><br><span class="line">        loss = neg_count * torch.sum(logits_pos) + pos_count*torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss/(pos_count+neg_count)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># none</span></span><br><span class="line">        total_logits = torch.cat((logits_pos, logits_neg), dim=<span class="number">1</span>)</span><br><span class="line">        loss = torch.sum(total_logits)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Margin-Triplet"><a href="#Margin-Triplet" class="headerlink" title="Margin Triplet"></a>Margin Triplet</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ -max(u^Tv^–u^Tv^+m,0)$$</li></ul><p>此公式理解起来相对直观，即对于一个输入样本，计算其和一个负样本相似度减去和正样本的相似度在加上m，并与0取max。该m可以理解：m越大为希望正反样本分开的距离越大。其目标是希望输入样本和正样本的相似度减去和负样本的相似度可以大于阈值m值。下图很形象的描述了这些关系。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/08/NwCTG5rc3J4D7R9.png" alt="triplet1"></p><p>所以，对于每一个输入样本k，该样本的<strong>margin tripl loss</strong>为$$ \sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^+m,0) $$</p><p>所以总的loss就是将所有输入样本的loss加起来</p><ul><li><p>$$ \frac{1}{2N*(2N-2)}\sum_{k}^{所有样本}\sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^++m,0) $$</p></li><li><p>同时也实现了semi-hard negative mining. 即计算loss（梯度）时，只考虑上图中semi-hard negatives的loss。即选择满足：$$ u^Tv^++m&gt;u^Tv^-$$</p></li></ul><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line"> representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"> similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"> <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line"> l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line"> r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line"> positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"> mid = similarity_matrix[self.mask_samples_from_same_repr]</span><br><span class="line"> negatives = mid.view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</span><br><span class="line"> zero = torch.zeros(<span class="number">1</span>).to(self.device)</span><br><span class="line"> triplet_matrix = torch.max(zero, negatives - positives + self.m_param)</span><br><span class="line"> <span class="comment"># max( sim(neg) - sim(pos) + m, 0)</span></span><br><span class="line"> <span class="comment"># 2N,2N-2 每一行代表了对于一个z关于其正类（z+batch）和其他反类的triplet loss</span></span><br><span class="line"> <span class="keyword">if</span> self.semi_hard == <span class="literal">True</span>:</span><br><span class="line">     <span class="comment"># semi-hard</span></span><br><span class="line">     semi_hard = - negatives + positives + self.m_param</span><br><span class="line">     <span class="comment"># print(semi_hard)</span></span><br><span class="line">     semi_hard_mask = torch.max(semi_hard, zero).type(torch.bool)</span><br><span class="line">     <span class="comment"># print(semi_hard_mask)</span></span><br><span class="line">     triplet_matrix_sh = triplet_matrix[semi_hard_mask]</span><br><span class="line">     <span class="comment"># print(triplet_matrix)</span></span><br><span class="line">     <span class="comment"># print(triplet_matrix_sh)</span></span><br><span class="line">     loss = torch.sum(triplet_matrix_sh)</span><br><span class="line">     <span class="keyword">return</span> loss</span><br><span class="line"> <span class="keyword">else</span>:    <span class="comment"># normal</span></span><br><span class="line">     loss = torch.sum(triplet_matrix)     </span><br><span class="line">     <span class="keyword">return</span> loss / (<span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="NT-Xent"><a href="#NT-Xent" class="headerlink" title="NT-Xent"></a>NT-Xent</h3><p>论文中的形式：</p><ul><li>$$l(i,j)=-log \frac{exp(s_{i,j}/\tau)}{\sum^{2N}<em>{k=1}1</em>{k\not=i}exp(s_{i,j}/\tau)}$$  </li><li>$$ L = \frac{1}{2N} \sum^{N}_{k=1}[l(2k-1,2k)+l(2k,2k-1)]$$</li></ul><p>代码实现未进行修改。</p><h2 id="simCLR模型"><a href="#simCLR模型" class="headerlink" title="simCLR模型"></a>simCLR模型</h2><p>主要使用ResNet-50来实现，参照论文B.9中所写：将Resnet第一个卷积层改为了3*3的Conv，stride=1，并去除第一个max pooling层；在augmentation中去除了Guassian Blur。</p><p>projection head同论文中一样，使用两层的MLP。</p><h2 id="遇到的问题与解决方法"><a href="#遇到的问题与解决方法" class="headerlink" title="遇到的问题与解决方法"></a>遇到的问题与解决方法</h2><p>Q1：使用个人笔记本训练，显存不足，使用cpu训练耗时过久。</p><p>A1：尝试使用过resnet18，仍时间仍很长，最终决定租GPU（1080ti）来训练。</p><p>Q2：训练时发现最终测试结果不好。</p><p>A2：最终控制变量，与未修改的代码对比测试，发现个人版本在sampler的时候不小心去掉了很多的训练样例，已修复为同原版。修复后，基本同原版效果</p><p>Q3：使用LARS效果不好，loss不能稳定下降，震荡严重。（unsolve）</p><p>A3：尝试修改debug，修改学习率，由于时间不足，暂未解决。</p><h2 id="关于Loss的个人想法"><a href="#关于Loss的个人想法" class="headerlink" title="关于Loss的个人想法"></a>关于Loss的个人想法</h2><p>从测试结果和论文结果可以看出，NT-xent的效果更佳。个人认为其主要的优势在于：</p><ul><li>NT-xent（cross entropy）利用的是相对相似度，而其余二者不是。这样可以缓解个别样本差异过大导致的不均衡（感觉类似于input的normalization）。</li><li>NT-xent计算了所有positive pair的loss。而NT-logistic和Margin Triplet则使用全部的pair来计算，不使用semi-hard mining的话，可能会造成坍塌。对于此模型生成的样本，可以看到其样本类别并不均衡，对于NT-logistic，这可能会导致训练效果下降。（使用semi-hard negative mining、采样、改变权重可以缓解这个问题）</li></ul><p>经过自己的implement之后，实在是羡慕google的TPU集群了！</p><p>这是我第一次真正接触self-supervised learning，之前只是有所耳闻，感觉这种contrastive learning的想法真的很有趣。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在开头&quot;&gt;&lt;a href=&quot;#写在开头&quot; class=&quot;headerlink&quot; title=&quot;写在开头&quot;&gt;&lt;/a&gt;写在开头&lt;/h2&gt;&lt;p&gt;整体的代码使用pytorch实现，基于&lt;a href=&quot;https://github.com/sthalles/SimCLR&quot;&gt;https://github.com/sthalles/SimCLR&lt;/a&gt; （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Papers/"/>
    
    
      <category term="论文复现" scheme="http://canVa4.github.io/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>First test Blog</title>
    <link href="http://canva4.github.io/2020/08/05/First-test-Blog/"/>
    <id>http://canva4.github.io/2020/08/05/First-test-Blog/</id>
    <published>2020-08-05T15:54:29.000Z</published>
    <updated>2020-08-05T15:57:36.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一个测试博客！！！"><a href="#第一个测试博客！！！" class="headerlink" title="第一个测试博客！！！"></a>第一个测试博客！！！</h1><p>语无伦次语无伦次语无伦次</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一个测试博客！！！&quot;&gt;&lt;a href=&quot;#第一个测试博客！！！&quot; class=&quot;headerlink&quot; title=&quot;第一个测试博客！！！&quot;&gt;&lt;/a&gt;第一个测试博客！！！&lt;/h1&gt;&lt;p&gt;语无伦次语无伦次语无伦次&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://canva4.github.io/2020/08/05/hello-world/"/>
    <id>http://canva4.github.io/2020/08/05/hello-world/</id>
    <published>2020-08-05T15:41:38.862Z</published>
    <updated>2020-08-05T15:41:38.862Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
