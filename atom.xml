<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://canva4.github.io/"/>
  <updated>2020-08-08T13:01:59.970Z</updated>
  <id>http://canva4.github.io/</id>
  
  <author>
    <name>阿翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CS231n Assignment1 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-07T08:21:14.000Z</published>
    <updated>2020-08-08T13:01:59.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment1-遇到的问题"><a href="#CS231n-Assignment1-遇到的问题" class="headerlink" title="CS231n Assignment1 遇到的问题"></a>CS231n Assignment1 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><h2 id="Softmax-implement"><a href="#Softmax-implement" class="headerlink" title="Softmax implement"></a>Softmax implement</h2><p>不论是实现softmax，SVM损失函数，二者遇到的问题都比较相似，主要为<strong>导数的推导</strong>和<strong>numpy的使用</strong>。由于softmax的实现稍微复杂一些，这里只记录softmax实现时的问题。</p><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>使用SGD核心的工作就是计算softmax关于权值W的梯度。课程中没有给出推导过程，这里推导一下。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/mg9rJC2LzVaAxO3.png" alt="image-20200807173340447"></p><h3 id="Numeric-Stability-Trick"><a href="#Numeric-Stability-Trick" class="headerlink" title="Numeric Stability Trick"></a>Numeric Stability Trick</h3><p>为了防止出现数值计算不稳定，要在计算损失函数式加入修正项（对Gradient无影响）。</p><p>原始为：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/w469bMc7skBYd52.png" alt="image-20200807174209667" style="zoom:50%;" /></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/t7zyhReGVp6QEi2.png" alt="image-20200807174255380" style="zoom: 80%;" /><h3 id="Implement-with-numpy"><a href="#Implement-with-numpy" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><h4 id="Navie-Version"><a href="#Navie-Version" class="headerlink" title="Navie Version"></a>Navie Version</h4><p>给出naive版本的代码。如何计算的示意图已在推导过程中给出。naive版本的代码基本按照推导的公式梳理下来即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)    <span class="comment"># 一个数值修正的技巧，防止出现数值不稳定的问题</span></span><br><span class="line">        scores = np.exp(scores)</span><br><span class="line"></span><br><span class="line">        sum_scores = np.sum(scores)        <span class="comment"># 可以简化写法，节省空间，懒得修改了</span></span><br><span class="line">        P = scores / sum_scores</span><br><span class="line">        L = -np.log(P)</span><br><span class="line"></span><br><span class="line">        loss += L[y[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):    <span class="comment"># 计算梯度，分类讨论</span></span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:, j] += (<span class="number">-1</span> + P[y[i]])*X[i].T</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dW[:, j] += P[j]*X[i].T</span><br><span class="line"></span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += reg * W</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h4 id="Vectorized-Version"><a href="#Vectorized-Version" class="headerlink" title="Vectorized Version"></a>Vectorized Version</h4><p>向量化版本。这里就有非常多的细节需要注意了。首先还是给出完整代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    scores = X @ W  <span class="comment"># ( N*C )</span></span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss += np.sum(L[np.arange(num_train), y])</span><br><span class="line"></span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)   <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算gradient</span></span><br><span class="line">    mid = np.zeros_like(P)  <span class="comment"># 生成一个和P一样的0矩阵</span></span><br><span class="line">    mid[np.arange(num_train), y] = <span class="number">1</span>  <span class="comment"># 对矩阵中Y所对应的部分加一个1，因为一会要构造出需要的梯度计算</span></span><br><span class="line">    dW = X.T @ (-mid + P)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>首先应该画图明白计算中各个量的关系，以及他们是怎么来的，这个很重要。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/Knh24oeJ53Nydqj.png" alt="image-20200807175314544"></p><p>第一处就是在计算Numeric Stability Trick时，要找到每一个输入向量的最大元素，这里注意需要保证keepdims=True。</p><p>其控制了返回数组的shape，这样返回的shape为(N,1)。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>同理在sum时，也要进行类似的处理，这样在归一化时才能work。</p><h4 id="An-Important-Trick"><a href="#An-Important-Trick" class="headerlink" title="An Important Trick!!!"></a>An Important Trick!!!</h4><p>在这里我遇到了不少的问题，主要是numpy使用的不熟练。。。:( 所以特此记录下来。</p><p><strong>L[np.arange(num_train), y] **与 **L[:,y]</strong> 的区别！</p><p>一开始的代码使用的是后者，因为目标就是获得所有行i中，列位置为y[i]的元素。所以想当然的使用了后者。但实际上，后者返回的是所有行x[i]中，x[i,y[j]]的元素！！！</p><p>示例如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/dKFBsGk3qzxbV5c.png" alt="image-20200807180256834"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/QbpEhW9DK63ex1F.png" alt="image-20200807180318182"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/2meKSxGVvcsb3zA.png" alt="image-20200807180331281"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/jJGhMKFWD7iTULy.png" alt="image-20200807180431683"></p><p>而**L[np.arange(num_train), y] **则为：</p><p>如果将np.arange(num_train)看为list，则其长度必须与y相同！！！其效果就是二者分别迭代，每次返回二者迭代结果下标位置处的元素。如图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/d7nQCOvX8Goue9z.png" alt="image-20200807180731248"></p><p>所以可见，基于我们的需要，后者才能满足要求。</p><h2 id="Two-Layer-Neural-Network"><a href="#Two-Layer-Neural-Network" class="headerlink" title="Two-Layer Neural Network"></a>Two-Layer Neural Network</h2><p>本部分的工作也与之前的部分比较相似，这里遇到主要问题还是如何处理求导的问题。</p><p>由于在这里我也遇到了一些问题，所以再次给出部分求导流程。</p><p>首先先给出网络的结构。</p><h3 id="Gradient-1"><a href="#Gradient-1" class="headerlink" title="Gradient"></a>Gradient</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/8yBxLVldSgqsrmZ.png" alt="image-20200807221946519"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/pb8PvAu7RjhNJHT.png" alt="image-20200807223402245"></p><h3 id="Implement-with-numpy-1"><a href="#Implement-with-numpy-1" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><p>下面给出代码实现。由于主要难点就是loss的实现了，之后的SGD和predict函数都非常简单，我没有遇到什么问题，这里只给出遇到了部分问题的loss与grad计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None, reg=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">    W1, b1 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2, b2 = self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">    N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    h = np.maximum(X @ W1 + b1, <span class="number">0</span>)</span><br><span class="line">    scores = h @ W2 + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we&#x27;re done</span></span><br><span class="line">    <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss = np.sum(L[np.arange(N), y])</span><br><span class="line"></span><br><span class="line">    loss /= N</span><br><span class="line">    loss += <span class="number">1</span> * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算W2，b2</span></span><br><span class="line">    dscore = P</span><br><span class="line">    dscore[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">    dscore /= N        <span class="comment"># 这里需要注意！！！</span></span><br><span class="line">    <span class="comment"># 计算梯度时只需要除一次N，这里debug花了很久。。</span></span><br><span class="line">    grads[<span class="string">&#x27;W2&#x27;</span>] = h.T @ dscore + <span class="number">2</span> * reg * W2</span><br><span class="line">    grads[<span class="string">&#x27;b2&#x27;</span>] = np.sum(dscore, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 计算W1，b1</span></span><br><span class="line">    dh = dscore @ W2.T      <span class="comment"># 目标函数对于h的偏导</span></span><br><span class="line">    dh[h &lt;= <span class="number">0</span>] = <span class="number">0</span>          <span class="comment"># 此时dh变为关于w1@x+b1的导数</span></span><br><span class="line">    grads[<span class="string">&#x27;W1&#x27;</span>] = X.T @ dh + <span class="number">2</span> * reg * W1</span><br><span class="line">    grads[<span class="string">&#x27;b1&#x27;</span>] = np.sum(dh, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p>基本上按照公式并注意矩阵维数和细节就OK了，遇到不太会的画个图就解决了。</p><div class="note warning">            <p>需要注意的是，在除以输入个数的时候，只需要除一次</p>          </div><p>这里一开始没有注意到，我一开始在每次计算梯度的时候都除了N，导致出现了误差，这里居然debug了很久。。</p><h3 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h3><p>有点懒，这部分工作没有完成。</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>其余的部分（k-Nearest Neighbor classifier, SVM, Higher Level Representations: Image Features）并未遇到很多的问题。具体详情代码见我的github仓库。<a href="https://github.com/canVa4/CS231n-Assignments">https://github.com/canVa4/CS231n-Assignments</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment1-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment1-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment1 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR论文复现</title>
    <link href="http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    <id>http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-08-05T16:31:51.000Z</published>
    <updated>2020-08-08T03:30:00.768Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p>整体的代码使用pytorch实现，基于<a href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）</p><a id="more"></a><p>训练为：<em>run.py</em>；修改训练参数、Loss、数据集等需要修改：<em>config.yaml</em> ；评估使用<em>evluation.py</em>（测试训练分开的原因是因为我租了GPU，用GPU训练，用我的PC测试，这样可以更快一些）</p><p>个人运行环境：win10 + pytorch 1.5 + cuda 10.2（租的GPU 1080ti）</p><table><thead><tr><th>日期</th><th>进度</th></tr></thead><tbody><tr><td>5-19 Tue（基本满课+实验）</td><td>论文阅读，选定使用pytorch实现和决定基于上文链接实现代码</td></tr><tr><td>5-20 Wed</td><td>熟悉基础知识、了解代码整体框架，理解loss function，并进行初步尝试编写loss，未调试</td></tr><tr><td>5-21 Thu（满课+实验）</td><td>写完了evaluation部分</td></tr><tr><td>5-22 Fri（基本满课）</td><td>跑代码，发现只用CPU究极龟速；于是装cuda，结果装了一白天的cuda T.T，晚上测试代码并初步验证loss function是否书写正确；初步移植LARS</td></tr><tr><td>5-23 Sat</td><td>测试三个Loss并尝试调参，尝试使用resnet18作为backbone网络，旁晚开始租了个GPU来跑模型，实现triplet loss(sh)</td></tr><tr><td>5-24 Sun</td><td>调参、修复bug、跑代码、微调loss（Logistic loss增加欠采样和改变权重）</td></tr><tr><td>5-25 Mon</td><td>调参、跑代码</td></tr></tbody></table><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Linear evaluation均使用Logistic Regression，均train from scratch（no pretrain）</p><p>GPU: 1080ti    resnet50训练+测试一次需5.5h；resnet18训练+测试一次需2.6h；总代码运行时间：约75h（包括未列出测试）</p><table><thead><tr><th>batch</th><th>epoch</th><th>out dim</th><th>optimizer</th><th>Loss</th><th>BackBone</th><th>t/m</th><th>CIARF10 Top-1</th></tr></thead><tbody><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>0.1</td><td>78.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>79.3%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>1</td><td>77.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.4</td><td>65.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.8</td><td>70.7%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss(sh)</td><td>resnet50</td><td>0.8</td><td>73.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(none)</td><td>resnet50</td><td>0.2</td><td>37.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.2</td><td>62.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.5</td><td>69.9%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>1</td><td>65.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>LARS</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>TODO</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet18</td><td>0.5</td><td>71.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(weight)</td><td>resnet18</td><td>0.2</td><td>66.5%</td></tr></tbody></table><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于每一个输入图片，模型会生成两个representation，最终优化的目标可以理解为：同一个batch内来自同一张图片的两个representation的距离近，让来自不同输入图片的representation的距离远。注意，论文中给出的是negative loss function</p><h3 id="Logistic-Loss"><a href="#Logistic-Loss" class="headerlink" title="Logistic Loss"></a>Logistic Loss</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ log \sigma(u^Tv^+/\tau) + log\sigma(-u^T v^-/ \tau) $$</li></ul><p>这里对于此公式，我一开始是没有理解的，于是自己尝试推理了一下。</p><p>对于每一个输入样本，模型会生成两个representation，对于一个有N个输入的batch的，就会产生2*N个representation，对于每一对representation计算一个cosine similarity。而每一对representation（下文用 $(i,j)$ 序偶来表示他们）可以根据他们的来源来确定他们label（即：来自同一输入的为正类，来自不同输入的为反类），这样就构成了一个监督任务。</p><p>将这个任务看为监督后，因为论文中提到的这个损失函数的名字是logistic loss，我自然地想到了logistics regression。于是从这个角度入手，来推理这个loss function。</p><p>用$ P(i,j) $表示一对representation为正类的概率。设正类y=1，反类y=0</p><p>那么写出整个数据集的对数似然函数$$ LL(\theta;X)=\sum_{each(i,j)} (y_{(i,j)} logP(i,j)+(1-y_{(i,j)})log(1-P(i,j)) )$$</p><p>对上式化简可以得到：$$ LL(\theta;X)=\sum_{正类} logP(i,j)+\sum_{反类}log(1-P(i,j)) $$</p><p>而cosine similarity并不是一个[0,1]之间的数（或者说没有概率的意义），参照logistics regression，将cosine similarity经过一个sigmoid函数$$ \sigma( \cdot) $$ 之后就变为了一个[0,1之间的数]，而且对于sigmoid有$$ \sigma(-x)=1-\sigma(x) $$,所以有：$$ LL(\theta;X)=\sum_{正类} log[\sigma(sim(i,j))]+\sum_{反类}log[\sigma(-sim(i,j))] , sim(i,j)为(i,j)的相似度指标$$</p><p>只需引入temperature就可将上式变为与论文中公式相同的形式。</p><p>在使用原版loss时，发现最终结果效果很差（见result中的NT-Logistics none）。个人猜测原因如下：</p><ul><li>样本非常不均衡，正例对远远少于反例。</li></ul><p>解决办法：</p><ul><li>对反例样本对使用简单的<em>under-sampling</em>（欠采样）</li><li>对于loss计算时，正反例样本<em>设置不同的权重</em>（效果更好，因为欠采样会丢失部分信息）</li></ul><p>（注：由于训练时间太久，没有来得多次跑weight测试效果）</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line">    representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line">    l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line">    r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line">    positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line">    negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>) * <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    logits_pos = self.sigmoid(positives / self.temperature).log_()</span><br><span class="line">    logits_neg = self.sigmoid(negatives / self.temperature).log_()</span><br><span class="line">    <span class="keyword">if</span> self.method == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># under-sampling</span></span><br><span class="line">        all_one_vec = np.ones((<span class="number">1</span>, <span class="number">2</span> * self.batch_size,))</span><br><span class="line">        all_zero_vec = np.zeros((<span class="number">1</span>, <span class="number">2</span> * self.batch_size * (<span class="number">2</span> * self.batch_size - <span class="number">3</span>)))</span><br><span class="line">        under_sampling_matrix = np.column_stack((all_one_vec, all_zero_vec)).flatten()</span><br><span class="line">        np.random.shuffle(under_sampling_matrix)</span><br><span class="line">        under_sampling_matrix = torch.tensor(under_sampling_matrix).view(</span><br><span class="line">            (<span class="number">2</span> * self.batch_size, <span class="number">2</span> * self.batch_size - <span class="number">2</span>)).type(torch.bool).to(self.device)</span><br><span class="line"></span><br><span class="line">        logits_neg = logits_neg[under_sampling_matrix]</span><br><span class="line">        loss = torch.sum(logits_pos) + torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    <span class="keyword">elif</span> self.method == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># change weight</span></span><br><span class="line">        neg_count = <span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>)</span><br><span class="line">        pos_count = <span class="number">2</span>*self.batch_size</span><br><span class="line">        loss = neg_count * torch.sum(logits_pos) + pos_count*torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss/(pos_count+neg_count)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># none</span></span><br><span class="line">        total_logits = torch.cat((logits_pos, logits_neg), dim=<span class="number">1</span>)</span><br><span class="line">        loss = torch.sum(total_logits)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Margin-Triplet"><a href="#Margin-Triplet" class="headerlink" title="Margin Triplet"></a>Margin Triplet</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ -max(u^Tv^–u^Tv^+m,0)$$</li></ul><p>此公式理解起来相对直观，即对于一个输入样本，计算其和一个负样本相似度减去和正样本的相似度在加上m，并与0取max。该m可以理解：m越大为希望正反样本分开的距离越大。其目标是希望输入样本和正样本的相似度减去和负样本的相似度可以大于阈值m值。下图很形象的描述了这些关系。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/08/NwCTG5rc3J4D7R9.png" alt="triplet1"></p><p>所以，对于每一个输入样本k，该样本的<strong>margin tripl loss</strong>为$$ \sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^+m,0) $$</p><p>所以总的loss就是将所有输入样本的loss加起来</p><ul><li><p>$$ \frac{1}{2N*(2N-2)}\sum_{k}^{所有样本}\sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^++m,0) $$</p></li><li><p>同时也实现了semi-hard negative mining. 即计算loss（梯度）时，只考虑上图中semi-hard negatives的loss。即选择满足：$$ u^Tv^++m&gt;u^Tv^-$$</p></li></ul><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line"> representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"> similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"> <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line"> l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line"> r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line"> positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"> mid = similarity_matrix[self.mask_samples_from_same_repr]</span><br><span class="line"> negatives = mid.view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</span><br><span class="line"> zero = torch.zeros(<span class="number">1</span>).to(self.device)</span><br><span class="line"> triplet_matrix = torch.max(zero, negatives - positives + self.m_param)</span><br><span class="line"> <span class="comment"># max( sim(neg) - sim(pos) + m, 0)</span></span><br><span class="line"> <span class="comment"># 2N,2N-2 每一行代表了对于一个z关于其正类（z+batch）和其他反类的triplet loss</span></span><br><span class="line"> <span class="keyword">if</span> self.semi_hard == <span class="literal">True</span>:</span><br><span class="line">     <span class="comment"># semi-hard</span></span><br><span class="line">     semi_hard = - negatives + positives + self.m_param</span><br><span class="line">     <span class="comment"># print(semi_hard)</span></span><br><span class="line">     semi_hard_mask = torch.max(semi_hard, zero).type(torch.bool)</span><br><span class="line">     <span class="comment"># print(semi_hard_mask)</span></span><br><span class="line">     triplet_matrix_sh = triplet_matrix[semi_hard_mask]</span><br><span class="line">     <span class="comment"># print(triplet_matrix)</span></span><br><span class="line">     <span class="comment"># print(triplet_matrix_sh)</span></span><br><span class="line">     loss = torch.sum(triplet_matrix_sh)</span><br><span class="line">     <span class="keyword">return</span> loss</span><br><span class="line"> <span class="keyword">else</span>:    <span class="comment"># normal</span></span><br><span class="line">     loss = torch.sum(triplet_matrix)     </span><br><span class="line">     <span class="keyword">return</span> loss / (<span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="NT-Xent"><a href="#NT-Xent" class="headerlink" title="NT-Xent"></a>NT-Xent</h3><p>论文中的形式：</p><ul><li>$$l(i,j)=-log \frac{exp(s_{i,j}/\tau)}{\sum^{2N}<em>{k=1}1</em>{k\not=i}exp(s_{i,j}/\tau)}$$  </li><li>$$ L = \frac{1}{2N} \sum^{N}_{k=1}[l(2k-1,2k)+l(2k,2k-1)]$$</li></ul><p>代码实现未进行修改。</p><h2 id="simCLR模型"><a href="#simCLR模型" class="headerlink" title="simCLR模型"></a>simCLR模型</h2><p>主要使用ResNet-50来实现，参照论文B.9中所写：将Resnet第一个卷积层改为了3*3的Conv，stride=1，并去除第一个max pooling层；在augmentation中去除了Guassian Blur。</p><p>projection head同论文中一样，使用两层的MLP。</p><h2 id="遇到的问题与解决方法"><a href="#遇到的问题与解决方法" class="headerlink" title="遇到的问题与解决方法"></a>遇到的问题与解决方法</h2><p>Q1：使用个人笔记本训练，显存不足，使用cpu训练耗时过久。</p><p>A1：尝试使用过resnet18，仍时间仍很长，最终决定租GPU（1080ti）来训练。</p><p>Q2：训练时发现最终测试结果不好。</p><p>A2：最终控制变量，与未修改的代码对比测试，发现个人版本在sampler的时候不小心去掉了很多的训练样例，已修复为同原版。修复后，基本同原版效果</p><p>Q3：使用LARS效果不好，loss不能稳定下降，震荡严重。（unsolve）</p><p>A3：尝试修改debug，修改学习率，由于时间不足，暂未解决。</p><h2 id="关于Loss的个人想法"><a href="#关于Loss的个人想法" class="headerlink" title="关于Loss的个人想法"></a>关于Loss的个人想法</h2><p>从测试结果和论文结果可以看出，NT-xent的效果更佳。个人认为其主要的优势在于：</p><ul><li>NT-xent（cross entropy）利用的是相对相似度，而其余二者不是。这样可以缓解个别样本差异过大导致的不均衡（感觉类似于input的normalization）。</li><li>NT-xent计算了所有positive pair的loss。而NT-logistic和Margin Triplet则使用全部的pair来计算，不使用semi-hard mining的话，可能会造成坍塌。对于此模型生成的样本，可以看到其样本类别并不均衡，对于NT-logistic，这可能会导致训练效果下降。（使用semi-hard negative mining、采样、改变权重可以缓解这个问题）</li></ul><p>经过自己的implement之后，实在是羡慕google的TPU集群了！</p><p>这是我第一次真正接触self-supervised learning，之前只是有所耳闻，感觉这种contrastive learning的想法真的很有趣。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在开头&quot;&gt;&lt;a href=&quot;#写在开头&quot; class=&quot;headerlink&quot; title=&quot;写在开头&quot;&gt;&lt;/a&gt;写在开头&lt;/h2&gt;&lt;p&gt;整体的代码使用pytorch实现，基于&lt;a href=&quot;https://github.com/sthalles/SimCLR&quot;&gt;https://github.com/sthalles/SimCLR&lt;/a&gt; （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Papers/"/>
    
    
      <category term="论文复现" scheme="http://canVa4.github.io/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>First test Blog</title>
    <link href="http://canva4.github.io/2020/08/05/First-test-Blog/"/>
    <id>http://canva4.github.io/2020/08/05/First-test-Blog/</id>
    <published>2020-08-05T15:54:29.000Z</published>
    <updated>2020-08-05T15:57:36.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一个测试博客！！！"><a href="#第一个测试博客！！！" class="headerlink" title="第一个测试博客！！！"></a>第一个测试博客！！！</h1><p>语无伦次语无伦次语无伦次</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一个测试博客！！！&quot;&gt;&lt;a href=&quot;#第一个测试博客！！！&quot; class=&quot;headerlink&quot; title=&quot;第一个测试博客！！！&quot;&gt;&lt;/a&gt;第一个测试博客！！！&lt;/h1&gt;&lt;p&gt;语无伦次语无伦次语无伦次&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://canva4.github.io/2020/08/05/hello-world/"/>
    <id>http://canva4.github.io/2020/08/05/hello-world/</id>
    <published>2020-08-05T15:41:38.862Z</published>
    <updated>2020-08-05T15:41:38.862Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
