<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://canva4.github.io/"/>
  <updated>2020-11-28T16:53:28.525Z</updated>
  <id>http://canva4.github.io/</id>
  
  <author>
    <name>阿翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Model Compression Paper Reading</title>
    <link href="http://canva4.github.io/2020/11/28/Model-Compression-Paper-Reading/"/>
    <id>http://canva4.github.io/2020/11/28/Model-Compression-Paper-Reading/</id>
    <published>2020-11-28T03:16:38.000Z</published>
    <updated>2020-11-28T16:53:28.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Model-Compression-Paper-Reading"><a href="#Model-Compression-Paper-Reading" class="headerlink" title="Model Compression Paper Reading"></a>Model Compression Paper Reading</h1><p>Last note is a brief overview of model compression and a summary of learning resources.</p><p>This note is focus on some specific papers and methods.</p><h2 id="Total-LIST"><a href="#Total-LIST" class="headerlink" title="Total LIST:"></a>Total LIST:</h2><ol><li>Distilling the Knowledge in a Neural Network 2015 知识蒸馏开山之作</li></ol><h1 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h1><p>本部分主要关注Knowledge Distillation(简记为KD).</p><p><strong>LIST:</strong></p><ol><li>Distilling the Knowledge in a Neural Network 2015</li></ol><h2 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a>Distilling the Knowledge in a Neural Network</h2><p>Author: Geoffrey Hinton er al  <a href="https://arxiv.org/pdf/1503.02531.pdf">LINK</a></p><p>Conference: CVPR 2015</p><p>KEY WORDS: <strong>知识蒸馏(Knowledge Distillation )开山之作</strong></p><p>知识蒸馏的目的就是将大模型原有的知识，让一个小模型也学会。</p><p>正常我们在训练一个图片分类的任务中最后会使用cross entropy这个损失函数，最后学的模型可以的输出一个概率分布（softmax的结果），真实label出现的概率最大，其他的概率相对很小。这个结果，我们成为soft target，他其中包含了比原始label更多的信息，比如在MNIST上可能不大合适。因此，一个网络训练好之后，对于正确的答案会有一个很高的置信度。例如，在MNIST数据中，对于某个2的输入，对于2的预测概率会很高，而对于2类似的数字，例如3和7的预测概率为1e-6 和 1e-9。显然soft target包含了更多的信息。</p><p>但是如果直接学习这个概率分布，其小类别的概率太小了，作者引入了<strong>temperature</strong>来解决这个问题，直观的来讲，就是可以将小类别的概率输出的结果变大，但仍然小于真实label的概率。</p><p>下图就是引入temperature后的softmax. T越大会产生更softer的分布。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/28/QDOZEz7my8S1Jjo.png" alt="image-20201128233002251"></p><p>如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。</p><p>整体的过程就是先训练好一个teacher，之后将teacher的softmax换用带temperature的softmax，并将训练数据输入，记录下来训练数据的结果。再使用训练数据训练student网络，student网络也使用相同的temperature。训练好后，在去除temperature，即将temperature设为1。</p><p>当soft target具有较高的熵时，与硬目标（真实label）相比，它们在每个训练样本中提供的信息更多，并且训练样本之间的梯度差异较小，因此，相比于原始的繁琐模型，小型模型通常可以在少得多的数据上进行训练，并且使用更高的学习率。</p><p><strong>Loss</strong></p><p>如果我们也知道训练数据的真实label，只用这种加权和形式的损失函数会有更好的效果。训练小网络时的loss为：</p><p>$$L = CE(y,p) + \alpha CE(q,p)$$</p><p>在这篇论文中，作者认为可以将模型看成是黑盒子，知识可以看成是输入到输出的映射关系。因此，我们可以先训练好一个teacher网络，然后将teacher的网络的输出结果 <strong>q</strong> 作为student网络的目标，训练student网络，使得student网络的结果 <strong>p</strong>  接近 <strong>q</strong> 。这里CE是交叉熵（Cross Entropy），<strong>y</strong>是真实标签，对于分类任务就是one-hot编码，q是teacher网络的输出结果，p是student网络的输出结果。</p><p><strong>Summary：</strong></p><p>知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近ensemble的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Model-Compression-Paper-Reading&quot;&gt;&lt;a href=&quot;#Model-Compression-Paper-Reading&quot; class=&quot;headerlink&quot; title=&quot;Model Compression Paper Readin
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Model Compression" scheme="http://canVa4.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>Model Compression overview and resources</title>
    <link href="http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/"/>
    <id>http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/</id>
    <published>2020-11-19T12:47:47.000Z</published>
    <updated>2020-11-30T08:05:33.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Model-Compression-Overview-and-Resources"><a href="#Model-Compression-Overview-and-Resources" class="headerlink" title="Model Compression Overview and Resources"></a>Model Compression Overview and Resources</h1><p>此NOTE主要记录一些关于model compression 方面的overview和一些不错的入门资源、survey and papers.</p><ul><li><strong>Model pruning(模型剪枝)</strong>: removes less important parameters</li><li><strong>Weight Quantization</strong>: uses fewer bits to represent the parameters</li><li><strong>Parameter sharing</strong></li><li><strong>Knowledge distillation(知识蒸馏)</strong>: trains a smaller student model that learns from intermediate outputs from the original model.</li><li><strong>Module replacing / Dynamic Computation</strong>: Can network adjust the computation power it need?</li></ul><h2 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h2><ol><li><a href="https://www.bilibili.com/video/BV1tE411F7aC?from=search&seid=16988409402694511485">知识蒸馏简述–起源、改进与研究现状【截至2020年3月22日】</a></li><li><a href="https://www.bilibili.com/video/BV17z411B7Hk?from=search&seid=4245228983927115130">模型压缩Network Compression方法补充</a>    NOTE: 1的延续</li><li><a href="https://www.bilibili.com/video/BV1SC4y1h7HB?from=search&seid=16988409402694511485">网络压缩和知识蒸馏-李宏毅</a></li><li><a href="https://www.bilibili.com/video/BV1Rt4y1m7Fm?from=search&seid=4245228983927115130">【深度学习的模型压缩与加速】台湾交通大学 張添烜教授</a></li></ol><h2 id="Slides"><a href="#Slides" class="headerlink" title="Slides"></a>Slides</h2><p>Overview: <a href="https://slides.com/arvinliu/model-compression">https://slides.com/arvinliu/model-compression</a></p><p>Deep Mutual Learning <a href="https://slides.com/arvinliu/kd_mutual">https://slides.com/arvinliu/kd_mutual</a></p><h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><ol><li>深度学习模型压缩与加速综述 <a href="https://zhuanlan.zhihu.com/p/67871864">LINK</a></li><li>知识蒸馏是什么？一份入门随笔 <a href="https://zhuanlan.zhihu.com/p/90049906">LINK</a></li><li>知识蒸馏（Knowledge Distillation）简述（一）<a href="https://zhuanlan.zhihu.com/p/81467832">LINK</a></li><li>Mutual Mean-Teaching：为无监督学习提供更鲁棒的伪标签 <a href="https://zhuanlan.zhihu.com/p/116074945">LINK</a></li></ol><h2 id="Papers-and-Surveys"><a href="#Papers-and-Surveys" class="headerlink" title="Papers and Surveys"></a>Papers and Surveys</h2><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h3><ol><li>Knowledge Distillation 2015 <a href="https://arxiv.org/pdf/1503.02531.pdf">LINK</a> 知识蒸馏开山之作</li><li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks 2019 <a href="https://arxiv.org/abs/1803.03635">LINK</a></li><li>Rethinking the Value of Network Pruning 2019 <a href="https://arxiv.org/abs/1810.05270">LINK</a></li><li>BinaryConnect: Training Deep Neural Networks with binary weights during propagations 2015 <a href="https://arxiv.org/abs/1511.00363">LINK</a></li><li>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 2016 <a href="https://arxiv.org/abs/1602.02830">LINK</a></li><li>XNOR-NET 2016 <a href="https://arxiv.org/abs/1603.05279">LINK</a></li><li>MobileNets 2017 <a href="https://arxiv.org/abs/1704.04861">LINK</a></li><li>SqueezeNet 2016 <a href="https://arxiv.org/abs/1602.07360">LINK</a></li><li>Multi-Scale Dense Networks for Resource Efficient Image Classification 2018 <a href="https://arxiv.org/abs/1703.09844">LINK</a></li><li>Label Refinery: Improving ImageNet Classification through Label Progression 2018 <a href="https://arxiv.org/abs/1805.02641">LINK</a></li><li>Deep Mutual Learning 2018 <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf">LINK</a></li><li>Born Again Neural Networks 2018 <a href="https://arxiv.org/abs/1805.04770">LINK</a></li><li>Improved Knowledge Distillation via Teacher Assistant 2019 <a href="https://arxiv.org/abs/1902.03393">LINK</a></li><li>FITNETS: HINTS FOR THIN DEEP NETS 2015 <a href="https://arxiv.org/pdf/1412.6550.pdf">LINK</a></li><li>Relational Knowledge Distillation 2019 <a href="https://arxiv.org/pdf/1904.05068.pdf">LINK</a></li><li>Similarity-Preserving Knowledge Distillation 2019 <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf">LINK</a></li><li>Pruning Filters for Efficient ConvNets 2017 <a href="https://arxiv.org/abs/1608.08710">LINK</a></li><li>Learning Efficient Convolutional Networks Through Network Slimming 2017 <a href="https://arxiv.org/abs/1708.06519">LINK</a></li><li>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration 2019 <a href="https://arxiv.org/pdf/1811.00250.pdf">LINK</a></li><li>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures 2016 <a href="https://arxiv.org/abs/1607.03250">LINK</a></li><li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask 2019 <a href="https://arxiv.org/abs/1905.01067">LINK</a></li></ol><h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys:"></a>Surveys:</h3><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>model compression最主要包含以下四类大方法，<strong>每一类并不是独立的，是可以交替和交叉使用的。</strong></p><p>如下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/8bfmXQc5OCFEaj1.png" alt="image-20201126165839580" style="zoom:67%;" /><h2 id="Network-Pruning"><a href="#Network-Pruning" class="headerlink" title="Network Pruning"></a>Network Pruning</h2><p>主要思想为：将Network<strong>不重要</strong>的weight或neuron删除后，再重新训练一次。</p><p>General Reason：尤其在深度学习领域，网路很深，模型参数极多，存在很多冗杂的参数。</p><p>应用：所有有神经网络的地方基本都可以使用。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/y97ufNeRSBvkjDY.png" alt="image-20201126160856008" style="zoom:50%;" /><p>其整体流程大概可以如上图所示。</p><p>NOTE：这里如何计算weight或neuron的重要程度有各种不同的方法，e.g. L2, L1 Norm，the number of times it wasn’t zero on a given data set ……</p><p>可以看到，这是一个iteration的过程。</p><h3 id="Why-Pruning"><a href="#Why-Pruning" class="headerlink" title="Why Pruning"></a>Why Pruning</h3><p>为什么要做network Pruning? 而不是直接在一个小的model上学习?</p><p>经验上来讲：<strong>It is widely known that smaller network is more difficult to learn successfully.</strong> 即：大模型的训练往往比小模型更加简单，即更容易跳过一些local minimum。</p><p>当然也有一些关于这方面的理论，如：</p><ul><li>Lottery Ticket Hypothesis <a href="https://arxiv.org/abs/1803.03635">LINK</a></li><li>Larger network is easier to optimize? [LINK](<a href="https://www.youtube.com/watch?v=_VuWvQU">https://www.youtube.com/watch?v=_VuWvQU</a><br>MQVk)</li><li>Rethinking the Value of Network Pruning <a href="https://arxiv.org/abs/1810.05270">LINK</a>   NOTE: Lottery Ticket Hypothesis的反例</li></ul><p>基本上来讲network Pruning可以分为：</p><ul><li><p>对于weight 做pruning</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/zHNxSCdIkmYylfU.png" alt="image-20201126162158912" style="zoom:67%;" /><p>这种方法的最大问题是导致<strong>不方便implement！！！</strong>因为现在GPU加速都是使用矩阵运算，这样容易使网络结构变的不规则。导致无法Speed Up甚至会出现pruning后速度反而下降的情况。</p><p>在Practice中这类weight pruning常常就简单的将要pruning的weight设置为0，这样显然并没有办法对于模型的体积进行压缩。</p></li><li><p>对于Neuron 做pruning</p></li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/n4LjKY7qrQobh9N.png" alt="image-20201126162424440" style="zoom:67%;" /><p>可以看到，在pruning后，整个网络仍然是regular的，可以继续使用GPU进行加速。实践中比较常用。</p><h3 id="What-is-important-How-to-evaluate-importance"><a href="#What-is-important-How-to-evaluate-importance" class="headerlink" title="What is important? How to evaluate importance?"></a>What is important? How to evaluate importance?</h3><p>如何来衡量一个weight or neuron的importance</p><ul><li>evaluate by weight（看大小）</li><li>evaluate by activation</li><li>evaluate by gradient</li></ul><p>After that?</p><ul><li>Sort by importance and prune by rank</li><li>prune by handcrafted threshold</li><li>prune by generated threshold</li></ul><p>**Threshold or Rank? **</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/4Eci1AqsB67kwVF.png" alt="image-20201126200440010" style="zoom: 67%;" /><h4 id="Evaluate-Importance"><a href="#Evaluate-Importance" class="headerlink" title="Evaluate Importance"></a>Evaluate Importance</h4><p>这部分主要关注evaluate weight.</p><ul><li><p>sum of L1 norm（也可以是其他范数）</p><p>这种直接使用norm的方法一般如下（对于CNN而言）：</p><p>将卷积的filter排列为矩阵，每个filter的channel拼成一行，之后对每一行算norm，根据此norm来选择去掉哪些filter.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SAIQMnsiZF3DPez.png" alt="image-20201126201736434" style="zoom: 80%;" /><p>理想的norm分布应该如下图所示，即有：</p><ul><li>norm非常接近0的部分</li><li>整体是一个比价均匀，而且有较大方差的分布</li></ul><p>我们就想要prune掉norm接近0的部分。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/QYv96ng5r4OtIMH.png" alt="image-20201126201949988" style="zoom:80%;" /><p>而真实的分布往往并不尽如人意。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/qb2RPiuFAlSgY85.png" alt="image-20201126202145833" style="zoom:80%;" /><ol><li>方差很小，此时很难选取一个合适的threshold。</li><li>没有接近0的部分，不接近0，很难从norm的角度说明一个filter他trivial。</li></ol></li><li><p>FPGM(Filiter Pruning via Gemetirc Media 2019): 大的norm一定important？小的norm的一定trivial？用Gemetirc Media来解决这个问题。（解决hazard of pruning by norm）如下图所示。</p></li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/hly5eWXK4TaPvq9.png" alt="image-20201126202506572" style="zoom:80%;" /><ul><li><p>Evaluate By BN(Batch Norm) Network Slimming</p><ul><li>根据BN的γ来判断是否pruning</li><li><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SOvbfiQ73FJ84BM.png" alt="image-20201126203026861"></li><li>而往往γ的分布并不好，我们需要做一些penalty。让这个分布更容易被筛选。</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/KabxdY4LPWyjSn7.png" alt="image-20201126203224808" style="zoom:67%;" /></li><li><p>Eval by 0s after ReLU - APoZ(Average Percentage of Zeros)</p></li></ul><h3 id="Some-theory-about-Network-Pruning"><a href="#Some-theory-about-Network-Pruning" class="headerlink" title="Some theory about Network Pruning"></a>Some theory about Network Pruning</h3><h4 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/MeY3PogsmfuARxt.png" alt="image-20201126203825313" style="zoom:67%;" /><h4 id="Rethinking-the-Value-of-Network-Pruning"><a href="#Rethinking-the-Value-of-Network-Pruning" class="headerlink" title="Rethinking the Value of Network Pruning"></a>Rethinking the Value of Network Pruning</h4><h4 id="Rethinking-VS-Lottery-Ticket"><a href="#Rethinking-VS-Lottery-Ticket" class="headerlink" title="Rethinking VS Lottery Ticket"></a>Rethinking VS Lottery Ticket</h4><p>Rethinking: 一种neuron pruning or structural pruning</p><p>Lottery Ticket: 一种 weight pruning，且要求learning rate要小。</p><h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p>主要思想为：利用一个已经训练好的大model做teacher，来训练小model(student).</p><p>最核心的思路为下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/RHYLdCQWbqlySxc.png" alt="image-20201126163001439" style="zoom:67%;" /><p>对于Knowledge Distillation(下文简称为KD)的分类，基本上可以按照Distill What来分类。</p><ol><li><strong>Logits</strong> 即：网络的输出值，一个label的概率分布<ul><li>直接一对一匹配logits</li><li>以batch为学习单位来学习其中的logits distillation</li><li>……</li></ul></li><li><strong>Feature</strong> 即：网络每层中的中间值<ul><li>直接一对一匹配每层的中间值</li><li>学习teacher网络中feature是如何转换的</li></ul></li></ol><h3 id="The-Power-of-Soften-Label"><a href="#The-Power-of-Soften-Label" class="headerlink" title="The Power of Soften Label"></a>The Power of Soften Label</h3><p>对于分类任务，我们模型的输出并不是想真实的label中一样，是一个one-hot encoding，而是一组在许多label上都用几率的一组概率分布。可以直观的看到，这个模型的输出，相比于真实的label包含了更多的信息，甚至包含了类别间的relationship. 现在有一类研究方向就是在训练时不适用这种one-hot encoding，而是研究如何产生包含更多信息的Soften Label。</p><p>例如这篇Label Refinery: Improving ImageNet Classification through Label Progression <a href="https://arxiv.org/abs/1805.02641">LINK</a> 之后写这篇文章的总结和介绍。</p><h3 id="Logits-Distillation"><a href="#Logits-Distillation" class="headerlink" title="Logits Distillation"></a>Logits Distillation</h3><p>本质就是：通过soft target让小model可以学到class之间的关系。</p><p>一些比较有趣的Work:</p><ul><li>Deep Mutual Learning </li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkrAz1DXclxyK8P.png" alt="image-20201126171644149" style="zoom:67%;" /><ul><li>Born Again Neural Networks</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ERveOL2rFQAo6H5.png" alt="image-20201126171821469" style="zoom:67%;" /><p>显然，这其中存在着一些问题，可能因为teacher模型的模型能力过强，而小模型的能力不足，导致无法很好地直接学习。</p><p>其中的一种很有趣的解决方法就是，向我们上课一样，引入TA。TA模型的能力介于teacher和student之间。这样可以避免缩小模型间的差距。下图即为：Improved Knowledge Distillation via Teacher Assistant 2020这篇paper的想法。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/Lc9MU1257Gw4jXh.png" alt="image-20201126172800289" style="zoom:67%;" /><h3 id="Feature-Distillation"><a href="#Feature-Distillation" class="headerlink" title="Feature Distillation"></a>Feature Distillation</h3><p>不再是直接根据logits来学习，而是学习网络中的中间features。</p><p>其代表方法有：</p><ul><li>FitNet: 先让学生学习如何产生teacher的中间feature，以后再使用标准的KD。NOTE：框架越相似，效果越好。</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/EWcQDowM2CZNlzv.png" alt="image-20201126173316849" style="zoom:67%;" /><p>该方法存在两大问题：</p><ul><li>model capacity is different. 显然如果大模型很复杂，可能小模型的中间部分无法学习到大模型的复杂映射。</li><li>redundance in teacher feature. 这个是很直观的，对于复杂的模型，这个feature中并不是所有的部分都是起作用的，这些对于小模型来讲是一个学习的负担。</li></ul><p>解决上述问题的方法可以是对于大模型的每一个feature map做一个知识蒸馏，目的就是在压缩feature的同时也降低了redundance. </p><p>另外一种的解决方法就是使用<strong>Attention</strong>，告诉student model中的feature map(主要指CNN)中那些part是最重要的。</p><p>下图是一种简单计算attention的方法，就是将filter对应产生的out dim做一个压缩。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/JnTAcqSV2r1UE75.png" alt="image-20201126192625424" style="zoom:67%;" /><h3 id="Relational-Distillation-Learn-from-Batch"><a href="#Relational-Distillation-Learn-from-Batch" class="headerlink" title="Relational Distillation: Learn from Batch"></a>Relational Distillation: Learn from Batch</h3><p>前面的不论是logit KD还是feature KD，都是对于每一个sample来学习的（即：individual KD），这类Relational Distillation关注的则通过一个batch来distillation batch之间sample的关系。</p><p>下图是这individual KD与Relational KD的paradigm的对比。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkfRwnCVBd4WLaT.png" alt="image-20201126193016466" style="zoom:67%;" /><p>而衡量sample之间的relation 可以用以下的两种描述角度：</p><ul><li>Distance-wise KD: 使用L2 distance来描述。</li><li>Angle-wise KD: 使用cosin similarity来描述。</li></ul><p>该方法是使用logits来做KD的，自然也可以使用features来做KD。这就有了：Similarity-Preserving Knowledge Distillation这篇文章。</p><h2 id="Parameter-Quantization"><a href="#Parameter-Quantization" class="headerlink" title="Parameter Quantization"></a>Parameter Quantization</h2><p>将原本神经网络中数据的存储单位float32/64压缩为更小的单位，例如8位。</p><p>应用：对于所欲已经train好的模型使用，或者边train边让模型去做quantize。</p><p>基本来讲大致分为以下方法：</p><ol><li>Using less bits to represent a value</li><li>Weight clustering</li></ol><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ECuVJeA4ZiOKNsR.png" alt="image-20201126163333485" style="zoom:67%;" /><ol start="3"><li>Represent frequent clusters by less bits, represent rare clusters by more bits. e.g. Huffman encoding</li></ol><p>其中一类是使用Binary Weight。(从某种角度上，也是一种正则化的方法)</p><ul><li>Binary Connect <a href="https://arxiv.org/abs/1511.00363">LINK</a></li><li>Binary Network <a href="https://arxiv.org/abs/1602.02830">LINK</a></li><li>XNOR-Net <a href="https://arxiv.org/abs/1603.05279">LINK</a></li></ul><h2 id="Architecture-Design"><a href="#Architecture-Design" class="headerlink" title="Architecture Design"></a>Architecture Design</h2><p>方法：利用更少的参数来实现原本某些layer效果。</p><p>例如对于全连接层：</p><p>我们可以将原本的 N到M维的映射 变为 N-&gt;K-&gt;M的映射。从矩阵乘法的角度来看，这可以看作一种Low rank approximate. 这种方法也可以极大的减少全连接层的参数数量。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/U8oKGqR9xNWDlV3.png" alt="image-20201126164212464" style="zoom:67%;" /><p>对于卷积层：可以使用Depthwise Separable Convolution（这也是大名鼎鼎的mobile net使用的方法）</p><p>即将原本一步的卷积运算变为两个卷积运算，如下图所示。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/mqGlihdKrTAOX9I.png" alt="image-20201126164522446" style="zoom:67%;" /><p>其他的一些经典的这类design有：</p><ul><li>MobileNet</li><li>SqueezeNet</li><li>Xception</li><li>ShuffleNet</li></ul><h2 id="Dynamic-Computation"><a href="#Dynamic-Computation" class="headerlink" title="Dynamic Computation"></a>Dynamic Computation</h2><p>核心思想：Can network adjust the computation power it need?</p><p>即：若此时计算资源充分，使用大模型；若不足，使用较小的模型。</p><p>可能的解决方法：</p><ol><li><p>Train multiple classifiers</p></li><li><p>Classifiers at the intermedia layer 例如 Multi-Scale Dense Networks <a href="https://arxiv.org/abs/1703.09844">LINK</a></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/zJ5US8EW79Ftkv6.png" alt="image-20201130154939206" style="zoom:80%;" /></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Model-Compression-Overview-and-Resources&quot;&gt;&lt;a href=&quot;#Model-Compression-Overview-and-Resources&quot; class=&quot;headerlink&quot; title=&quot;Model Compre
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Model Compression" scheme="http://canVa4.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>CS224w HomeWork 2</title>
    <link href="http://canva4.github.io/2020/11/10/CS224w-HomeWork-2/"/>
    <id>http://canva4.github.io/2020/11/10/CS224w-HomeWork-2/</id>
    <published>2020-11-10T12:52:03.000Z</published>
    <updated>2020-11-19T03:26:27.254Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224w-HomeWork-2"><a href="#CS224w-HomeWork-2" class="headerlink" title="CS224w HomeWork 2"></a>CS224w HomeWork 2</h1><p>本文旨在记录CS224w Machine Learning With Graphs 2019完成作业中遇到的问题和作业的结果。</p><p>我的github仓库 <a href="https://github.com/canVa4/CS224w">LINK</a>。</p><p>本部分共包含：</p><ul><li>Node Classication: correlation with <strong>Collective Classification and message passing</strong></li><li>Node Embeddings with TransE: correlation with <strong>Graph Representation Learning</strong></li></ul><h2 id="Part-1-Node-Classification"><a href="#Part-1-Node-Classification" class="headerlink" title="Part 1 Node Classification"></a>Part 1 Node Classification</h2><p>Node Classication主要是使用Collective Classification的方法，其主要流程和组成部分如下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/hboyclIR93dNCar.png" alt="image-20201110211233087" style="zoom:80%;" /><p>其中approximate inference的方法主要为（这三者sort by how advanced these methods are）：</p><ul><li>Relational Classification</li><li>Iterative Classification</li><li>Belief Classification</li></ul><p>这三种方法都是基于Markov Assumption，即：</p><p> the labely $Y_i$ of one node depends on the labels of its neighbors, which can be mathematically written as:</p><p>$$P(Y_i|i)=P(Y_i|N_i)$$</p><p>其中$N_i$为节点i的neighbors。</p><h3 id="Relational-Classification"><a href="#Relational-Classification" class="headerlink" title="Relational Classification"></a>Relational Classification</h3><p>非常简单的方法，只使用了label和网络拓扑结构的信息，没有使用每个节点的features。对于每个无label节点的预测只是简单的取一个邻居的label的平均。</p><p>使用下式的公式来预测无label的节点。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/t4QmbGOg5zWsr7I.png" alt="image-20201114201117560" style="zoom:67%;" /><p>其缺点也很明显，即</p><ul><li>The convergence not guaranteed.</li><li>Cannot use node feature information, only use the graph information.</li></ul><p>这部分对应的作业也比较简单，即在给定的简单的图上实现这个算法。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/3NqRoHriQpXLITj.png" alt="image-20201114205829364" style="zoom: 67%;" /><p>简易的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> snap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span>():</span></span><br><span class="line">    g = snap.TUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">        g.AddNode(i)</span><br><span class="line">    g.AddEdge(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    g.AddEdge(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    g.AddEdge(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    g.AddEdge(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">    g.AddEdge(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    g.AddEdge(<span class="number">4</span>, <span class="number">7</span>)</span><br><span class="line">    g.AddEdge(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">6</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">6</span>, <span class="number">10</span>)</span><br><span class="line">    g.AddEdge(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G = create_graph()</span><br><span class="line">node_dict = &#123;&#125;</span><br><span class="line">positive = [<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">negative = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">node_dict[<span class="number">3</span>] = positive</span><br><span class="line">node_dict[<span class="number">5</span>] = positive</span><br><span class="line">node_dict[<span class="number">8</span>] = negative</span><br><span class="line">node_dict[<span class="number">10</span>] = negative</span><br><span class="line">label_id = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">node_num = G.GetNodes()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, node_num + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_id:</span><br><span class="line">        node_dict[i] = [<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line">loop_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> flag <span class="keyword">is</span> <span class="keyword">not</span> <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 当不在变化时，停止迭代</span></span><br><span class="line">    flag = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, node_num + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_id:</span><br><span class="line">            neighbors = []</span><br><span class="line">            cur_node = G.GetNI(i)</span><br><span class="line">            degree = cur_node.GetDeg()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> nbr <span class="keyword">in</span> range(degree):</span><br><span class="line">                neighbors.append(cur_node.GetNbrNId(nbr))</span><br><span class="line">            origin = node_dict[i]</span><br><span class="line">            sum_p = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> mid <span class="keyword">in</span> neighbors:</span><br><span class="line">                sum_p[<span class="number">0</span>] += node_dict[mid][<span class="number">0</span>] / degree</span><br><span class="line">                sum_p[<span class="number">1</span>] += node_dict[mid][<span class="number">1</span>] / degree</span><br><span class="line">            node_dict[i] = sum_p</span><br><span class="line">            <span class="keyword">if</span> abs(origin[<span class="number">0</span>] - sum_p[<span class="number">0</span>]) &gt; <span class="number">0.001</span>:</span><br><span class="line">                <span class="comment"># 当每次变化小于0.001时，认为仍在变化</span></span><br><span class="line">                flag += <span class="number">1</span></span><br><span class="line">            print(<span class="string">&#x27;id:&#123;&#125;\t pro:&#123;&#125;&#x27;</span>.format(i, sum_p))</span><br><span class="line">    loop_cnt += <span class="number">1</span></span><br><span class="line">    print(<span class="string">&#x27;Loop &#123;&#125; finish!!!&#x27;</span>.format(loop_cnt))</span><br></pre></td></tr></table></figure><p>运行代码即可得到Q1.1的答案。</p><h3 id="Belief-Propagation"><a href="#Belief-Propagation" class="headerlink" title="Belief Propagation"></a>Belief Propagation</h3><p>Belief Propagation is a dynamic programming approach to answering conditional probability queries in a graphical model. 本质上是模拟网络上信息的传递，根据相邻节点传递的信息来确定自己的状态。在迭代过程中，每个节点都会跟相邻节点通信，即传递信息。详细的公式和细节见slide。</p><p>答案：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/tHWLRPhnfJr5d6S.png" alt="image-20201114215247475"></p><h2 id="Part-2-Node-Embeddings-with-TransE"><a href="#Part-2-Node-Embeddings-with-TransE" class="headerlink" title="Part 2 Node Embeddings with TransE"></a>Part 2 Node Embeddings with TransE</h2><p>这里主要讨论一种经典的应用于Multi-relational graphs的embedding方法—-TransE。Multi-relational graphs是指：graphs with multiple types of edges. 这种图的一个典型例子就是知识图谱（Knowledge Graph）。而TransE也是知识图谱上面embedding的经典方法。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/16/gU8xO4V9tQdSMLD.png" alt="image-20201116215835875"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/16/K2DA7Ec8xw9BWNX.png" alt="image-20201116215808214"></p><h2 id="Part-3-GNN-Expressiveness"><a href="#Part-3-GNN-Expressiveness" class="headerlink" title="Part 3 GNN Expressiveness"></a>Part 3 GNN Expressiveness</h2><p>NOTE：在进行训练的时候，如果节点没有features，其features vector设为全1向量或者该节点的度。</p><p>本部分主要在研究，GNN的层数与其表示能力（Expressiveness）的关系。expressiveness refers to the set of functions (usually the loss function for classication or regression tasks) a neural network is able to compute, which depends on the structural properties of a neural network architecture.</p><h3 id="3-1-Effect-of-Depth-on-Expressiveness"><a href="#3-1-Effect-of-Depth-on-Expressiveness" class="headerlink" title="3.1 Effect of Depth on Expressiveness"></a>3.1 Effect of Depth on Expressiveness</h3><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/QesCVAjP6HDgxw5.png" alt="image-20201119103926420" style="zoom:50%;" /><h3 id="3-2-Relation-to-Random-Walk"><a href="#3-2-Relation-to-Random-Walk" class="headerlink" title="3.2 Relation to Random Walk"></a>3.2 Relation to Random Walk</h3><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/P2Dyvsf7AUlN1C3.png" alt="image-20201119105125162" style="zoom: 50%;" /><h3 id="3-3-Over-Smoothing-Effect"><a href="#3-3-Over-Smoothing-Effect" class="headerlink" title="3.3 Over-Smoothing Effect"></a>3.3 Over-Smoothing Effect</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/KEFlauwm1Io5W7i.png" alt="image-20201119111833303"></p><h3 id="3-4-Learning-BFS-with-GNN"><a href="#3-4-Learning-BFS-with-GNN" class="headerlink" title="3.4 Learning BFS with GNN"></a>3.4 Learning BFS with GNN</h3><p>很简单的average aggregation，这里就不再写出。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS224w-HomeWork-2&quot;&gt;&lt;a href=&quot;#CS224w-HomeWork-2&quot; class=&quot;headerlink&quot; title=&quot;CS224w HomeWork 2&quot;&gt;&lt;/a&gt;CS224w HomeWork 2&lt;/h1&gt;&lt;p&gt;本文旨在记录CS22
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS224w" scheme="http://canVa4.github.io/tags/CS224w/"/>
    
      <category term="GNN" scheme="http://canVa4.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Embedding Literature Review</title>
    <link href="http://canva4.github.io/2020/10/30/Embedding-Literature-Review/"/>
    <id>http://canva4.github.io/2020/10/30/Embedding-Literature-Review/</id>
    <published>2020-10-30T03:14:49.000Z</published>
    <updated>2020-11-19T03:00:21.832Z</updated>
    
    <content type="html"><![CDATA[<p>Embeddings主要思路分类：</p><ul><li>NLP类方法 使用LSTM等对时序数据做表示</li><li>Graph Embedding 对图做嵌入（引入图的原因之一是：用图可以表示复杂关系的长时间时间序列）</li><li>类似CNN的方法也可以看为Embedding</li><li>对比学习，学到更好的表示 or Embedding</li></ul><p>知乎LINKS</p><ol><li>Embedding的原因 <a href="https://zhuanlan.zhihu.com/p/164502624">https://zhuanlan.zhihu.com/p/164502624</a> </li><li>Embedding的简单发展史（主要为Word2vec -&gt; Item2Vec）<a href="https://zhuanlan.zhihu.com/p/164502624">https://zhuanlan.zhihu.com/p/164502624</a></li><li>万物皆可Embedding <a href="https://zhuanlan.zhihu.com/p/109935332">https://zhuanlan.zhihu.com/p/109935332</a></li><li>Embedding在深度学习中的3大方向 <a href="https://zhuanlan.zhihu.com/p/67218758">https://zhuanlan.zhihu.com/p/67218758</a></li></ol><p>Video：</p><ol><li>Pre-trained Models for Natural Language Processing: A Survey <a href="https://www.bilibili.com/video/BV16K4y1475Z?from=search&seid=10138468149493332259">LINK</a></li><li>自然语言处理中的表示学习 <a href="https://www.bilibili.com/video/BV1va4y1Y7BE/?spm_id_from=333.788.videocard.2">LINK</a></li></ol><h1 id="Resource-List："><a href="#Resource-List：" class="headerlink" title="Resource List："></a>Resource List：</h1><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h2><ol><li>A Tutorial on Network Embeddings 2018 <strong>Finished</strong></li><li>Tutorial on NLP-Inspired Network Embedding 2019 <strong>Finished</strong></li><li>DeepWalk   <strong>INCLUDE</strong></li><li>LINE   <strong>INCLUDE</strong></li><li>Node2Vec   <strong>INCLUDE</strong></li><li>GraphAttention   <strong>INCLUDE</strong></li><li>SDNE</li><li>HOPE</li><li>Learning edge representations via low-rank asymmetric projections  2017</li><li>Deep graph kernels 2015</li></ol><h2 id="Videos"><a href="#Videos" class="headerlink" title="Videos:"></a>Videos:</h2><ol><li>Pre-trained Models for Natural Language Processing: A Survey <a href="https://www.bilibili.com/video/BV16K4y1475Z?from=search&seid=10138468149493332259">LINK</a> </li><li>自然语言处理中的表示学习 <a href="https://www.bilibili.com/video/BV1va4y1Y7BE/?spm_id_from=333.788.videocard.2">LINK</a> <strong>Finished</strong></li></ol><h2 id="Specturm-Methods"><a href="#Specturm-Methods" class="headerlink" title="Specturm Methods:"></a>Specturm Methods:</h2><p><a href="https://www.cnblogs.com/pinard/p/6221564.html">https://www.cnblogs.com/pinard/p/6221564.html</a></p><h1 id="A-Tutorial-on-Network-Embeddings"><a href="#A-Tutorial-on-Network-Embeddings" class="headerlink" title="A Tutorial on Network Embeddings"></a>A Tutorial on Network Embeddings</h1><p>Author: Haochen Chen er al <a href="https://arxiv.org/abs/1808.02590">LINK</a></p><p>Journal: Social and Information Networks 2018</p><p><strong>Goal of Network Embeddings：</strong></p><p>Network embedding methods aim at learning low-dimensional latent representation of nodes in a network.</p><p>而获得的embeddings应该有如下的性质：</p><ul><li><strong>适应性（Adaptability）</strong>- 现实的网络在不断发展；新的应用算法不应该要求不断地重复学习过程。</li><li><strong>可扩展性（Scalability）</strong>- 真实网络本质上通常很大，因此网络嵌入算法应该能够在短时间内处理大规模网络。</li><li><strong>社区感知（Community aware）</strong>- 潜在表示之间的距离应表示用于评估网络的相应成员之间的相似性的度量。这就要求同质网络能够泛化。</li><li><strong>低维（Low dimensional）</strong>- 当标记数据稀缺时，低维模型更好地推广并加速收敛和推理。</li><li><strong>连续的（Continuous）</strong></li></ul><h2 id="从传统ML降维的角度看Graph-Embedding"><a href="#从传统ML降维的角度看Graph-Embedding" class="headerlink" title="从传统ML降维的角度看Graph Embedding"></a>从传统ML降维的角度看Graph Embedding</h2><p>这其中使用了例如：PCA和MDS等方法，这类方法都可以看为使用一个n by k矩阵来代表原始数据的n by m矩阵，其中k&lt;m。之后又提出了一些新的降维方法比如：IsoMap，LLE等方法。总体来讲这类方法在小的网络上显示了不错的性能，但由于其复杂度往往随矩阵规模而指数增长，导致这种方法无法应用于大型的图。</p><p>另外一类就是图的谱方法了（如LE: Laplacian eigenmaps），基本上来讲使用拉普拉斯矩阵or标准化的拉普拉斯矩阵的特征值和特征向量的信息来对于每个节点进行聚类和划分以实现嵌入的效果。这类方法的主要问题是：对矩阵的特征值分解是与矩阵规模呈指数形式的，所以在large network中也较难应用，且这类基于spectrum的方法的性能往往逊色于基于neural network的方法。</p><h2 id="The-Age-of-Deep-Learning"><a href="#The-Age-of-Deep-Learning" class="headerlink" title="The Age of Deep Learning"></a>The Age of Deep Learning</h2><p><strong>DeepWalk在图上使用表示学习（或深度学习）的方法，是一个非常经典的方法</strong>。DeepWalk 通过将节点视为单词并生成短随机游走（random walk）作为句子来弥补网络嵌入和单词嵌入之间的差距。然后，可以使用 Skip-gram 等，应用于这些随机游走以获得网络嵌入。</p><p>优点：</p><ul><li>Online algorithm</li><li>Scalable</li></ul><p>也引出了一种在图上使用Deep Learning的paradigm</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/ySGZqvIswTxdeLD.png" alt="image-20201105105601325" style="zoom:80%;" /><h2 id="Unsupervised-Network-Embeddings"><a href="#Unsupervised-Network-Embeddings" class="headerlink" title="Unsupervised Network Embeddings"></a>Unsupervised Network Embeddings</h2><p>最经典的还是DeepWalk，其有如下两个地方可以改进：</p><ul><li><p>Source of Context Node（如何产生序列）</p></li><li><p>Embedding Learning Methods(deepwalk中使用skip-gram，当然也可以使用其他学习序列表示的方法)</p></li></ul><p>针对于Deep Walk的可改良点，有这些改进的方法。<img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201105113538975.png" alt="image-20201105113538975"></p><p>其中比较出名的有的：LINE，Node2Vec，GraphAttention等。后面的SDNE和DNGR则引入了deep learning中较深的encoder类方法。</p><p>值得注意的是，这些方法主要是用于undirected graph中的。</p><h3 id="Directed-Graph-Embedding"><a href="#Directed-Graph-Embedding" class="headerlink" title="Directed Graph Embedding"></a>Directed Graph Embedding</h3><p>基本上全部的基于无向图的方法都可以很自然的推广到无向图中。</p><p>其中常见和经典的方法为：HOPE</p><h3 id="Edge-Embedding"><a href="#Edge-Embedding" class="headerlink" title="Edge Embedding"></a>Edge Embedding</h3><p>这类embedding可以用于link prediction等task中。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/W1Hxt5GgIqs6TDU.png" alt="image-20201105150551272"></p><h3 id="Signed-Graph-Embeddings"><a href="#Signed-Graph-Embeddings" class="headerlink" title="Signed Graph Embeddings"></a>Signed Graph Embeddings</h3><p>Signed Graph指：边的权值只为-1or1，主要方法报过：SiNE和SNE。</p><h3 id="Subgraph-Embeddings"><a href="#Subgraph-Embeddings" class="headerlink" title="Subgraph Embeddings"></a>Subgraph Embeddings</h3><p>经典方法为Deep Kernel也是这类方法的常见范式。</p><h3 id="Meta-strategies-for-Improving-Network-Embeddings"><a href="#Meta-strategies-for-Improving-Network-Embeddings" class="headerlink" title="Meta-strategies for Improving Network Embeddings"></a>Meta-strategies for Improving Network Embeddings</h3><p>主要方法HARP。HARP is a general meta-strategy to improve all of the state-of-the-art neural algorithms for embedding graphs, including DeepWalk, LINE, and Node2vec.</p><h2 id="Attributed-Network-Embeddings"><a href="#Attributed-Network-Embeddings" class="headerlink" title="Attributed Network Embeddings"></a>Attributed Network Embeddings</h2><p>前面的讨论针对的主要是没有attribute的图，而对于有attribute的图，其attribute一般是附加在节点上的，一般主要研究两类attribute</p><ul><li>high-level features such as text or images</li><li>node labels</li></ul><p>针对第一点（high-level features）：常见方法包括：TADW，CENE</p><p>针对第二点（node labels 常见于 引用网络和社交网络）：GENE</p><p>当然在许多真正的网络中，并不是所有节点都是有label的，所以也有一些Semi-supervised的方法设计出来。例如：Planetoid，Max-margin Deep Walk, </p><h2 id="Heterogeneous-Network-Embeddings"><a href="#Heterogeneous-Network-Embeddings" class="headerlink" title="Heterogeneous Network Embeddings"></a>Heterogeneous Network Embeddings</h2><p>Heterogeneous Network/Graph 异质图，即have multiple classes of nodes or edges。大部分异构网络嵌入方法通过联合最小化每种modality的损失来学习节点嵌入。这些方法要么直接在相同的潜在空间中学习所有节点嵌入，要么事先为每个模态构建嵌入，然后将它们映射到相同的潜在空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此paper主要是总结了一下目前常见的和经典的一些图嵌入的方法（连介绍都算不上），也对于在不同应用场景上的方法做了一个分类，很适合在之后想在某个场景使用图嵌入方法时来进行查阅。这部分note也主要是这个目的。作者也简单总结了未来图嵌入可能的发展方向，不过本文是2018年的，之后会跟进一些2018年之后更新的方法。</p><h1 id="Tutorial-on-NLP-Inspired-Network-Embedding"><a href="#Tutorial-on-NLP-Inspired-Network-Embedding" class="headerlink" title="Tutorial on NLP-Inspired Network Embedding"></a>Tutorial on NLP-Inspired Network Embedding</h1><p>主要介绍使用word embedding的方法到graph embedding中。主要包括以下几个经典的方法：</p><ul><li>DeepWalk: Online Learning of Social Representations 2014</li><li>LINE: Large-scale Information Network Embedding 2015</li><li>node2vec: Scalable Feature Learning for Networks 2016</li><li>struct2vec: Learning Node Representations from Structural Identity 2017</li><li>metapath2vec: Scalable representation learning for heterogeneous networks 2017</li></ul><p>LINE和node2vec是对于deepwalk的改进，公认后者更加稳定一些。</p><h2 id="word2vec（skip-gram）"><a href="#word2vec（skip-gram）" class="headerlink" title="word2vec（skip-gram）"></a>word2vec（skip-gram）</h2><p>word2vec基于的假设是：the principle of co-occurrence: the assumption that words with related semantic meanings appear close to each other in texts.</p><p>word2vec模型示意图，一种无监督方法。需要一个大小为2*k的slide window来生成输入序列。给定一个词w，其上下文词$c\in C(w)$由该slide window获得，我们优化的目标是：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/PUfhDNZBjCFczie.png" alt="image-20201112150224584"></p><p>即给定当前词，令上下文词出现的概率最大，找到合适的参数θ。</p><p>对于整个预料库，我们有：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ogSdlFwfcLmAtNG.png" alt="image-20201112150338856"></p><p>计算这个概率一般使用softmax，即：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/dGkvFhIPXKC6ySW.png" alt="image-20201112150434138"></p><p>$V_c, V_w$为生成的embedding。由于在计算分母部分时，也非常的耗费计算时间，所以经常使用negative sampling（本质是预测总体类别的一个子集）或hierarchy softmax（本质是把 N 分类问题变成 log(N)次二分类）来取一个approximate。整体模型如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/qRknro8QuvaA5Et.png" alt="image-20201112145802004"></p><h2 id="Deep-Walk"><a href="#Deep-Walk" class="headerlink" title="Deep  Walk"></a>Deep  Walk</h2><p>一个比较简答扩展，将word2vec可以应用于网络之中！对应关系如下图所示，具体细节可以见cs224w  slides or 原始paper</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/oCdrmGn3ayKiwJs.png" alt="image-20201112152105745"></p><p>The assumption is that sampling from multiple random walks captures the structure of the graph.</p><p><strong>缺点：</strong></p><p>生成序列使用的random walk是unbiased的，其与DFS比较相似。</p><h2 id="LINE"><a href="#LINE" class="headerlink" title="LINE"></a>LINE</h2><p>LINE的目标就是优化random walk的unbiased特性，solve this issue by preserving first-order and second-order node proximities（保留一阶和二阶节点的亲近性）。其没有使用random walk，而是使用1-hop和2-hop的节点来构造供给NLP word embedding的sequence序列。</p><ul><li>first-order proximity：节点的一阶proximity被定义为：其链接边的权值。</li></ul><p>其数学定义为：v是其生产的embedding</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/lOGSHkYgbuyvIaq.png" alt="image-20201112163126930" style="zoom: 67%;" /><p>其理想情况应该与基于节点权值定义的一阶相似度比较接近。即与下式接近。其中W为正则化因子。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/dAUixgQrvF6yIke.png" alt="image-20201112163256655" style="zoom:67%;" /><p>embedding的优化目标就是尽可能缩小二者分布的“距离”，即最小化二者的KL散度。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ibcMVprNhLkJs14.png" alt="image-20201112163428368" style="zoom:67%;" /><ul><li>second-order proximity：节点的二阶proximity被定义为：the common neighborhood of two nodes</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/k6nZMCovr2PUG7w.png" alt="image-20201112162920623" style="zoom:67%;" /><p>其二阶相似度的优化目标为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ajqCp6lYOUMvxiB.png" alt="image-20201112163646201" style="zoom:67%;" /><p>其中P2为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/w1uDkBVZvI475Ws.png" alt="image-20201112163703959" style="zoom:67%;" /><p>下面是对于一阶和二阶相似度的一个直观解释。如上图所示，可见节点6与7有很高的一阶相似度（相连的边的权值大），所以他们在embedding space应该比较接近。</p><p>节点5与节点6虽然没有直接连接，但二者有高度相似的邻居节点，所以二者的二阶相似度很高，他们在embedding space中也应该比较接近。</p><p>在LINE中，the embedding for the first-order and second-order proximities (i.e. maximizing for O1 and O2) is done separately。最终从这两个模型得到的embedding在concatenate成为最终的embedding。</p><p><strong>总结</strong>：</p><p>不同于DeepWalk的DFS，LINE更倾向于BFS；而且其最大的特点是可以利用在带权图中，利用了权重的信息。</p><h2 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h2><p>使用biasing the random walks。本质上就是改变了random walk生成序列的方式。作者因为了两个因子，来控制random walk是倾向于DFS还是BFS。在实现时要记录是从哪一个节点来的。</p><p>文中还提出了基于node2vec如何表示边的信息。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/khASl6HZfpFVroX.png" alt="image-20201112170716300" style="zoom:67%;" /><h2 id="struct2vec"><a href="#struct2vec" class="headerlink" title="struct2vec"></a>struct2vec</h2><p>Focuses on the <strong>role</strong> of nodes in a network. Struc2vec is a framework for representations based on structural similarity. </p><p>GOAL: The goal of struc2vec is to preserve the identity of the nodes’ structure when projecting them into Euclidean space.</p><p>这与node2vec 和 deepwalk 很不一样，这俩是捕获neighbor的信息，而许多节点在struct很相似，但在图上的距离却很远。</p><p>为了计算这个表示，struct2vec构建了一个特殊的graph：<strong>the context graph</strong> 其代表了structural similarities between nodes.有了context graph之后，只需在该图上使用deep walk or node2vec即可。</p><p>设原始的图的diameter（直径为K），则the context graph M is a multi-layer graph with K + 1 layers. Each layer includes all the nodes in G.在每一个layer中，带权的权值表示了两个节点的结构相似度。</p><h3 id="Step-1-Computing-Structural-Similarity"><a href="#Step-1-Computing-Structural-Similarity" class="headerlink" title="Step 1: Computing Structural Similarity"></a>Step 1: Computing Structural Similarity</h3><p>第一步主要是寻找节点之间的structural Similarity。For each node v, we look at the $N_k(u)$, which is the set of nodes which are k-distant from node u. 对于每一组$N_k(u)$看成一个ring，我们找他们的ordered degree squence，即把每个节点的度组成序列（按照降序）。k最大为K（图的diameter(直径)）。measure两个节点的structural Similarity就是使用每个节点计算的ordered degree squence。用于这个ordered degree squence显然未必是等长度的，所以作者使用了Dynamic Time Warping (DTW)，一种match element between 2 squence with different length.</p><p>下图即为整体的Structural Similarity的计算公式，可以看到时递归计算的。DS()即为ordered degree squence，g()即为前面提到的DTW，用于计算两个长度不一样序列的距离。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/iWlMAjBEmdy3cwx.png" alt="image-20201112191817212" style="zoom:67%;" /><h3 id="Step-2-Constructing-the-Context-Graph"><a href="#Step-2-Constructing-the-Context-Graph" class="headerlink" title="Step 2: Constructing the Context Graph"></a>Step 2: Constructing the Context Graph</h3><p>The context graph M is a multilayer graph with K layers. Each layer is a complete graph consisting of all the nodes u ∈ V in the original graph G. 所以图G中的每个节点可以用M中K+1层(k=0, … , K)个对应的节点来表示。</p><p>下图就是一个根据左边G建立的M。一共包含3层。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ovb5lOJXpqUQMED.png" alt="image-20201112201859657" style="zoom:67%;" /><p>在每一层都是一个全连接的无向带权图，其权重的计算公式为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/vI1WDsHVRcwUYSf.png" alt="image-20201112202040048"></p><p>式中的f就是上文计算不同长度序列的函数。直观的来讲，两个节点的的structural similarity越相似，即该计算的距离越小，这两个节点的间的权值就越大。</p><p>从图中也可以看出，不同layer间的同一个节点间也存在着双向的带权边，权值的计算方式如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/cRdreDCqtKy47Tw.png" alt="image-20201112202329435" style="zoom:67%;" /><p>其中$\Gamma_k(u)$为连接u的边中权重大于该层平均权重的数量。直观的来讲：$\Gamma_k(u)$就是有多少节点是与节点u相似的。</p><h3 id="Step-3-Generate-Context"><a href="#Step-3-Generate-Context" class="headerlink" title="Step 3: Generate Context"></a>Step 3: Generate Context</h3><p>现在我们已经建立了Context Graph，下一步就是与Deep Walk非常相似了。不过在这里显然一个节点可以往上面的层走，也可以向下面的层走，也可以在同层间游动。其中：留在本层继续游走的概率为q，自然跳层的概率就是1-q，该q为一个超参数。</p><p>切换layer的概率为1-q，向上和向下移动的概率为：</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201112203231908.png" alt="image-20201112203231908" style="zoom: 80%;" /><p>停留在当前layer的概率为q，移动的概率也根据边的权值决定。其中$Z_k(u)$为一个归一化因子。直观的来讲，结构越相似的节点越容易出现在一次random walk之中。</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201112203320875.png" alt="image-20201112203320875" style="zoom:80%;" /><h3 id="Step-4-Learn-Representations"><a href="#Step-4-Learn-Representations" class="headerlink" title="Step 4: Learn Representations"></a>Step 4: Learn Representations</h3><p>最后就是使用生成的序列来进行word2vec即可（skip-gram + negative sampling）</p><h2 id="metapath2vec"><a href="#metapath2vec" class="headerlink" title="metapath2vec"></a>metapath2vec</h2><p>主要针对heterogeneous networks（异质图）。在异质图中，节点可以属于不同的类别。这个与之前random walk类方法使用的同质图非常不同，作者主要使用random walks will be biased by using meta-paths。一个mate-path是一个predefined composite relation between nodes. 本质就是random walk只能发生在定义好的meta-path上，the random walks must follow the semantics dictated by the various prescribed meta-paths。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Embeddings主要思路分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP类方法 使用LSTM等对时序数据做表示&lt;/li&gt;
&lt;li&gt;Graph Embedding 对图做嵌入（引入图的原因之一是：用图可以表示复杂关系的长时间时间序列）&lt;/li&gt;
&lt;li&gt;类似CNN的方法也可以看为
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Embedding" scheme="http://canVa4.github.io/tags/Embedding/"/>
    
      <category term="Literature Review" scheme="http://canVa4.github.io/tags/Literature-Review/"/>
    
  </entry>
  
  <entry>
    <title>CS224w HomeWork 1</title>
    <link href="http://canva4.github.io/2020/10/28/CS224w-HomeWork-1/"/>
    <id>http://canva4.github.io/2020/10/28/CS224w-HomeWork-1/</id>
    <published>2020-10-28T08:21:31.000Z</published>
    <updated>2020-11-09T08:13:47.834Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224w-HomeWork-1"><a href="#CS224w-HomeWork-1" class="headerlink" title="CS224w HomeWork 1"></a>CS224w HomeWork 1</h1><p>本文旨在记录CS224w Machine Learning With Graphs 2019完成作业中遇到的问题和作业的结果。</p><p>我的github仓库 <a href="https://github.com/canVa4/CS224w">LINK</a>。</p><h2 id="Part-1-Network-Characteristics"><a href="#Part-1-Network-Characteristics" class="headerlink" title="Part 1 Network Characteristics"></a>Part 1 Network Characteristics</h2><p>课程中反复强调的一个非常重要的观点就是：想要比较一个网络的属性，我们需要一个criterion或者一个null network。</p><p>所以这部分的核心就是null network or criterion的生成。我们需要生成Erdös-Renyi Random Graphs和Small-World Random Network。</p><h3 id="Erdos-Renyi-Random-Graphs"><a href="#Erdos-Renyi-Random-Graphs" class="headerlink" title="Erdös-Renyi Random Graphs"></a>Erdös-Renyi Random Graphs</h3><p>非常的简单，即：undirected graph with n nodes, and m edges picked uniformly at random. 需要的参数就是节点数和边数了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genErdosRenyi</span>(<span class="params">N=<span class="number">5242</span>, E=<span class="number">14484</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string">    :param - E: number of edges</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Erdos-Renyi graph with N nodes and E edges</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    Graph = snap.PUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        Graph.AddNode(i)</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> cnt &lt; E:</span><br><span class="line">        src = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        dst = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Graph.IsEdge(src, dst):</span><br><span class="line">            Graph.AddEdge(src, dst)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br></pre></td></tr></table></figure><h3 id="Small-World-Random-Network"><a href="#Small-World-Random-Network" class="headerlink" title="Small-World Random Network"></a>Small-World Random Network</h3><p>生成Small-World Random Network需要三步。分别为：</p><ol><li>Begin with N nodes arranged as a ring, i.e. imagine the nodes form a circle and each node is connected to its two direct neighbors.</li><li>Connect each node to the neighbors of its neighbors.</li><li>Randomly select some given number pairs of nodes not yet connected and add an edge between them.</li></ol><p>实现起来并不困难，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCircle</span>(<span class="params">N=<span class="number">5242</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Circle graph with N nodes and N edges. Imagine the nodes form a</span></span><br><span class="line"><span class="string">        circle and each node is connected to its two direct neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    Graph = snap.PUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        Graph.AddNode(i)</span><br><span class="line">    Graph.AddEdge(N - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N - <span class="number">1</span>):</span><br><span class="line">        Graph.AddEdge(i, i + <span class="number">1</span>)</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connectNbrOfNbr</span>(<span class="params">Graph, N=<span class="number">5242</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing a circle graph on N nodes</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Graph object with additional N edges added by connecting each node</span></span><br><span class="line"><span class="string">        to the neighbors of its neighbors</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        n1 = (i + <span class="number">1</span> + N) % N</span><br><span class="line">        n2 = (i - <span class="number">1</span> + N) % N</span><br><span class="line">        Graph.AddEdge(n1, n2)</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connectRandomNodes</span>(<span class="params">Graph, M=<span class="number">4000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string">    :param - M: number of edges to be added</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Graph object with additional M edges added by connecting M randomly</span></span><br><span class="line"><span class="string">        selected pairs of nodes not already connected.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    N = Graph.GetNodes()</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> cnt &lt; M:</span><br><span class="line">        src = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        dst = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Graph.IsEdge(src, dst):</span><br><span class="line">            Graph.AddEdge(src, dst)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genSmallWorld</span>(<span class="params">N=<span class="number">5242</span>, E=<span class="number">14484</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string">    :param - E: number of edges</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Small-World graph with N nodes and E edges</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Graph = genCircle(N)</span><br><span class="line">    Graph = connectNbrOfNbr(Graph, N)</span><br><span class="line">    Graph = connectRandomNodes(Graph, <span class="number">4000</span>)</span><br><span class="line">    <span class="keyword">return</span> Graph</span><br></pre></td></tr></table></figure><h3 id="Question-1-1-Degree-Distribution"><a href="#Question-1-1-Degree-Distribution" class="headerlink" title="Question 1.1 Degree Distribution"></a>Question 1.1 Degree Distribution</h3><p>计算Degree Distribution。Degree Distribution就是：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/XTKUEY1jAedIkOM.png" alt="image-20201028163717245" style="zoom:50%;" /><p>这里并不需要计算出分数，而是直接将度为k的节点数以log-log图画出来。</p><p>这里画图使用matplotlib，首先要生成x,y坐标上对应的数。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDataPointsToPlot</span>(<span class="params">Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return values:</span></span><br><span class="line"><span class="string">    X: list of degrees</span></span><br><span class="line"><span class="string">    Y: list of frequencies: Y[i] = fraction of nodes with degree X[i]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    X, Y = [], []</span><br><span class="line">    DegToCntV = snap.TIntPrV()</span><br><span class="line">    snap.GetDegCnt(Graph, DegToCntV)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> DegToCntV:</span><br><span class="line">        X.append(item.GetVal1())</span><br><span class="line">        Y.append(item.GetVal2())</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>画图部分代码已给出，这里不再列出。值得注意的是画log-log图使用<code>plt.loglog</code>即可。</p><h4 id="Answers"><a href="#Answers" class="headerlink" title="Answers"></a>Answers</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/OLJ8VwY4PeaIBxM.png" alt="image-20201028164113889" style="zoom:67%;" /><h3 id="Question-1-2-Clustering-Coefficient"><a href="#Question-1-2-Clustering-Coefficient" class="headerlink" title="Question 1.2 Clustering Coefficient"></a>Question 1.2 Clustering Coefficient</h3><p>计算Clustering Coefficient。公式如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/h2bqeKRVTHPc3uA.png" alt="image-20201028164303055" style="zoom:50%;" /><p>代码如下，可能稍微负责的就是找该节点邻居间边的数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcClusteringCoefficientSingleNode</span>(<span class="params">Node, Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Node: node from snap.PUNGraph object. Graph.Nodes() will give an</span></span><br><span class="line"><span class="string">                   iterable of nodes in a graph</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: float</span></span><br><span class="line"><span class="string">    returns: local clustering coeffient of Node</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    C = <span class="number">0.0</span></span><br><span class="line">    neigbors = []</span><br><span class="line">    deg = Node.GetDeg()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(deg):</span><br><span class="line">        neigbors.append(Graph.GetNI(Node.GetNbrNId(i)))</span><br><span class="line">    cnt_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(deg):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            cnt_nbr += neigbors[i].IsInNId(neigbors[j].GetId())</span><br><span class="line">    <span class="keyword">if</span> deg &gt;= <span class="number">2</span>:</span><br><span class="line">        C = <span class="number">2</span> * cnt_nbr / (deg * (deg - <span class="number">1.0</span>))</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcClusteringCoefficient</span>(<span class="params">Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: float</span></span><br><span class="line"><span class="string">    returns: clustering coeffient of Graph</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here! If you filled out calcClusteringCoefficientSingleNode,</span></span><br><span class="line">    <span class="comment">#       you&#x27;ll probably want to call it in a loop here</span></span><br><span class="line">    C = <span class="number">0.0</span></span><br><span class="line">    V = Graph.GetNodes()</span><br><span class="line">    <span class="keyword">for</span> NI <span class="keyword">in</span> Graph.Nodes():</span><br><span class="line">        Ci = calcClusteringCoefficientSingleNode(NI, Graph)</span><br><span class="line">        C = C + Ci</span><br><span class="line">    C = C / V</span><br><span class="line"></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> C</span><br></pre></td></tr></table></figure><h4 id="Answers-1"><a href="#Answers-1" class="headerlink" title="Answers"></a>Answers</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/CitYa4IGqexDrbW.png" alt="image-20201028165111378"  /><h2 id="Part-2-Structural-Roles-Rolx-and-ReFex"><a href="#Part-2-Structural-Roles-Rolx-and-ReFex" class="headerlink" title="Part 2 Structural Roles: Rolx and ReFex"></a>Part 2 Structural Roles: Rolx and ReFex</h2><p>本部分主要是implement计算每个节点Structural Role的方法（实现Rolx 和 ReFex方法），可以看做一种对于每个节点拓扑信息的提取方法，将每个节点的拓扑信息变为一个向量，即实现一个feature extraction的方法。这里的feature extraction主要包含两步：</p><ol><li>Basic Features: First extract basic local features from every node.</li><li>Recursive Features: Then aggregate the basic features along graph edges so that global features are also obtained.</li></ol><h3 id="Basic-Features"><a href="#Basic-Features" class="headerlink" title="Basic Features"></a>Basic Features</h3><p>首先是计算basic feature，主要指的是节点本身的local features。这里计算的basic feature为一个三维向量。</p><p>其三个维度的意义是：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/EdNXguTZCts9Bnj.png" alt="image-20201029171924469"></p><p>这里简单的记录一下计算2和3的方法。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/24UTaJ1bmz5vQWE.png" alt="image-20201029195853008"></p><p>有了上图，这几个关系就很明确了，将代码完善即可。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_basic_features</span>(<span class="params">node, graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    提取basic features</span></span><br><span class="line"><span class="string">    :param node: 目标节点，SNAP中的node类，而非ID，可以用GetNI()获得</span></span><br><span class="line"><span class="string">    :param graph: 目标图（无向图）</span></span><br><span class="line"><span class="string">    :return: 长度为3的array 分别为：该节点deg，egonet内部deg，进出egonet的边数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    degree = node.GetDeg()</span><br><span class="line"></span><br><span class="line">    neighbors = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算全部邻居的deg的和</span></span><br><span class="line">    total_deg_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(degree):</span><br><span class="line">        <span class="comment"># 获取全部的邻居，目标构建egonet</span></span><br><span class="line">        neighbor = graph.GetNI(node.GetNbrNId(i))</span><br><span class="line">        neighbors.append(neighbor)</span><br><span class="line">        total_deg_nbr += neighbor.GetDeg()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算邻居之间边的数量</span></span><br><span class="line">    edge_between_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(neighbors)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            edge_between_nbr += neighbors[i].IsNbrNId(neighbors[j].GetId())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array((degree, edge_between_nbr + degree, total_deg_nbr - <span class="number">2</span> * edge_between_nbr - degree))</span><br></pre></td></tr></table></figure><p>我的结果为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/CRQPthH7lSfYaWc.png" alt="image-20201029200007798" style="zoom:50%;" /><h3 id="Recursive-Features"><a href="#Recursive-Features" class="headerlink" title="Recursive Features"></a>Recursive Features</h3><p>也相对简单，使用下图的公式即可。具体细节，请参照官方作业的pdf。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/4ND6q9Hz8xIWJap.png" alt="image-20201029200049787" style="zoom: 67%;" /><p>核心函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recursive_features</span>(<span class="params">v_mat, graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    迭代一次</span></span><br><span class="line"><span class="string">    :param v_mat: np array</span></span><br><span class="line"><span class="string">    :param graph: 目标图</span></span><br><span class="line"><span class="string">    :return: 新的v_mat，向量长度变为原来的3倍</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cnt_node, ori_len = v_mat.shape</span><br><span class="line">    new_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    mean_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    sum_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph.Nodes():</span><br><span class="line">        nodeId = node.GetId()</span><br><span class="line">        cnt_nbrs = node.GetDeg()</span><br><span class="line">        new_mat[nodeId] = v_mat[nodeId]</span><br><span class="line">        <span class="keyword">if</span> cnt_nbrs == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(cnt_nbrs):</span><br><span class="line">            nbr_id = node.GetNbrNId(i)</span><br><span class="line">            mean_mat[nodeId] += v_mat[nbr_id]</span><br><span class="line">            sum_mat[nodeId] += v_mat[nbr_id]</span><br><span class="line">        mean_mat[nodeId] = mean_mat[nodeId] / cnt_nbrs</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((new_mat, mean_mat, sum_mat), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>结果如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/kfYKOELZ7HrcUAy.png" alt="image-20201029200224081" style="zoom:50%;" /><h3 id="Role-Discovery"><a href="#Role-Discovery" class="headerlink" title="Role Discovery"></a>Role Discovery</h3><p>主要工作即为：根据计算出的迭代3次的cosine similarity，绘出其分布直方图；根据直方图判别有几种role（看凸起即可），并随机选一个role中的一个节点，根据其local(basic) feature绘制其2-hop子图。唯一难点可能就是2-hop子图的绘制，这里我使用了networkx来绘制。我使用了python中的set()来滤去重复节点，这种方法时间复杂度显然很高。不过对于小图而言还是能接受的。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Draw_SubGraph</span>(<span class="params">Graph, Node_id</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    绘图函数 Node 节点2-hop子图</span></span><br><span class="line"><span class="string">    :param Graph: 目标图</span></span><br><span class="line"><span class="string">    :param NI_id: 目标节点</span></span><br><span class="line"><span class="string">    :return: 无</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Nodes = []</span><br><span class="line">    Edges = []</span><br><span class="line">    center_node = Graph.GetNI(Node_id)</span><br><span class="line">    Nodes.append(Node_id)</span><br><span class="line">    colors = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有1-hop节点放进去</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(center_node.GetDeg()):</span><br><span class="line">        nbr_id = center_node.GetNbrNId(i)</span><br><span class="line">        Nodes.append(nbr_id)</span><br><span class="line">    <span class="comment"># 所有2-hop节点放进去</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Nodes)):</span><br><span class="line">        mid_node = Graph.GetNI(Nodes[i])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(mid_node.GetDeg()):</span><br><span class="line">            nbr_id = mid_node.GetNbrNId(j)</span><br><span class="line">            Nodes.append(nbr_id)</span><br><span class="line">    Nodes = list(set(Nodes))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> mid_id <span class="keyword">in</span> Nodes:</span><br><span class="line">        <span class="keyword">if</span> mid_id == Node_id:</span><br><span class="line">            colors.append(<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colors.append(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    <span class="comment"># print(Nodes)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Nodes)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            <span class="keyword">if</span> Graph.IsEdge(Nodes[i], Nodes[j]):</span><br><span class="line">                Edges.append((Nodes[i], Nodes[j]))</span><br><span class="line">    <span class="comment"># print(len(Edges))</span></span><br><span class="line">    G = nx.Graph()</span><br><span class="line">    G.add_nodes_from(Nodes)</span><br><span class="line">    G.add_edges_from(Edges)</span><br><span class="line">    nx.draw_networkx(G, node_color=colors)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure><p>运行结果如下：分别为直方图和2-hop子图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/86VsXSJYwZ3DnNR.png" alt="image-20201029200326758" style="zoom:50%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/jgJoFmLKwrCZ2QD.png" alt="image-20201029200838918" style="zoom:50%;" /></p><h3 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h3><p>最大的感觉就是光听一遍和写完一遍代码对于整个流程完全有着不同程度的理解，还是得多练啊。。。</p><p>中间也卡了不少次，花费了不少的时间T.T</p><h2 id="Part-3-Community-detection-using-the-Louvain-algorithm"><a href="#Part-3-Community-detection-using-the-Louvain-algorithm" class="headerlink" title="Part 3 Community detection using the Louvain algorithm"></a>Part 3 Community detection using the Louvain algorithm</h2><p><strong>Community is just sets of tightly connected nodes.</strong></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/KLAc6P95G8W4dxv.png" alt="image-20201103104601046" style="zoom:67%;" /><p>如上图所示，这个就是Community Detection后的结果。这类Community detection的算法一般将：原本问题变为一个优化问题。</p><p>首先是定义一个优化目标，其应该是A measure of how well a network is partitioned into communities，该值正比于划分的好坏.  即对于每一个可能的partition都给出一个分数，目标就是找到使该分数最大的一个划分。这里使用的指标为：<strong>Modularity</strong>。定义如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/KxdW8jNcHizRuSg.png" alt="image-20201103105343050" style="zoom:80%;" />公式化之后变为：注：这里的Null model使用的是<strong>configuration model</strong>。即：有同样的degree distribution但node之间的连接是uniformly random的。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/W1ZkKhylRXFou7Q.png" alt="image-20201103104747043" style="zoom: 80%;" /><p>其取值在[-1,1]之间，It is positive if the number of edges within groups exceeds the expected number. </p><p>然而直接优化这个问题是一个NP hard问题。所以提出了<strong>Louvain algorithm</strong>。这是一种greedy algorithm。并且可以提供一种<strong>Hierarchical communities</strong>. 其整体流程如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/wY4eihbvVKT65F3.png" alt="image-20201103110548639" style="zoom:80%;" /><p>值得注意的是这种方法假设：community是disjoint的。</p><h3 id="Q3-1-Proof-for-Modularity-gain-when-an-isolated-node-moves-into-a-community"><a href="#Q3-1-Proof-for-Modularity-gain-when-an-isolated-node-moves-into-a-community" class="headerlink" title="Q3-1 Proof for Modularity gain when an isolated node moves into a community"></a>Q3-1 Proof for Modularity gain when an isolated node moves into a community</h3><p>证明如图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/65UYz17gLoMFIQq.png" alt="image-20201103150122229"></p><h3 id="Q3-2-Louvain-algorithm-on-a-16-node-network"><a href="#Q3-2-Louvain-algorithm-on-a-16-node-network" class="headerlink" title="Q3-2 Louvain algorithm on a 16 node network"></a>Q3-2 Louvain algorithm on a 16 node network</h3><p>Answers for Graph H:</p><ol><li>1</li><li>12 NOTE: 6*2</li><li>$4 \cdot Q_c = 4(\frac{12}{2m}-\frac{14^2}{4m^2})$,where $m=12\cdot4+1\cdot4=52$, Q=0.158</li></ol><p>Answers for Graph J:</p><ol><li>2</li><li>26</li><li>Q=2(26/104-(28/104)^2)=0.355</li></ol><h3 id="Q3-3-Louvain-algorithm-on-a-128-node-network"><a href="#Q3-3-Louvain-algorithm-on-a-128-node-network" class="headerlink" title="Q3-3 Louvain algorithm on a 128 node network"></a>Q3-3 Louvain algorithm on a 128 node network</h3><p>与Q3-2是相同的计算方式，比较简单，不再计算一遍了。</p><h2 id="Part-4-Spectral-clustering"><a href="#Part-4-Spectral-clustering" class="headerlink" title="Part 4 Spectral clustering"></a>Part 4 Spectral clustering</h2><p>经典的谱聚类的方法。讲得还是很精彩和清晰的。我在这里简单的总结一下整体的思路框架。</p><p>首先是谱聚类的目标，对于图做一个good partition分成不同的部分，即不同的clustering。</p><p>所以我们要有一个指标来衡量一个good partition。引出了cut。</p><p>+</p><p>这个指标的最大问题是没有将每个cluster内部的连接考虑进去，在优化该指标时，即最小化cut指标时，我们总可以找到如下的“最优指标”，但该指标显然不是我们期望的。原因就是没有考虑每个cluster内部的连接。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/BWUP9Rpac2CGjdl.png" alt="image-20201104163807540" style="zoom:67%;" /><p>为了解决这个问题，引入了指标<strong>Conductance</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/aFuU1NYBLbfhvie.png" alt="image-20201104163936635"></p><p>相当于对于cluster的连接做了一个正则，这样优化该指标，我们可以找到一个更好的partition。</p><p>下面作者引入了邻接矩阵的谱，拉普拉斯矩阵的谱（拉普拉斯矩阵有很好的性质，半正定矩阵，必定有全1向量为eigenvector，其对应了eigenvalue为0），最终将寻找第二小的特征值和特征向量与一个partition挂钩（直观解释），并且从理论推导出小的特征值为最优Conductance的一个下界。这样就寻找将拉普拉斯矩阵第二小的特征值和特征向量和找到最优partition联系了起来。简单回顾Spectral Clustering后，开始完成这部分的作业。</p><h3 id="Q4-1-A-Spectral-Algorithm-for-Normalized-Cut-Minimization-Foundations"><a href="#Q4-1-A-Spectral-Algorithm-for-Normalized-Cut-Minimization-Foundations" class="headerlink" title="Q4-1 A Spectral Algorithm for Normalized Cut Minimization: Foundations"></a>Q4-1 A Spectral Algorithm for Normalized Cut Minimization: Foundations</h3><p>一些公式推导的题目，具体题目见官网源文件。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/AUeCo94uExJrwn1.png" alt="image-20201104174652449" style="zoom:80%;" /><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/OgNUo1s7FDqtGkX.png" alt="image-20201104174758916"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/dgx7PFJvmIj5Qzu.png" alt="image-20201104174838628"></p><h3 id="Q4-2-Normalized-Cut-Minimization-Solving-for-the-Minimizer（Uncertain）"><a href="#Q4-2-Normalized-Cut-Minimization-Solving-for-the-Minimizer（Uncertain）" class="headerlink" title="Q4-2 Normalized Cut Minimization: Solving for the Minimizer（Uncertain）"></a>Q4-2 Normalized Cut Minimization: Solving for the Minimizer（Uncertain）</h3><p>本部分的证明不太确定，最后部分使用的Rayleigh定理不太确定。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/CYFUQRcvtD4VZMO.png" alt="image-20201104210042302" style="zoom:67%;" /><h3 id="Q4-3-Relating-Modularity-to-Cuts-and-Volumes"><a href="#Q4-3-Relating-Modularity-to-Cuts-and-Volumes" class="headerlink" title="Q4-3 Relating Modularity to Cuts and Volumes"></a>Q4-3 Relating Modularity to Cuts and Volumes</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/FhUZy3amOexgTQi.png" alt="image-20201104205957243"></p><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>至此homework1的内容全部结束了，整体而言的难度不是很高，在编程上有snap库极大的简化了编程上的难度。整体来讲homework1涉及了很多分析图的方法和非常多的基本却关键的概念。比如：如何衡量一个图的特点（通过一些全局的指标比如：degree distribution，Clustering Coefficient，diameter），这是要和random graph做对比的。包括后面的涉及的如何衡量一个节点的特征（Role），motifs，graphlet的概念，community/group的概念，这些都是分析复杂图的基石。</p><p>整体而言，目前这部分课程也让我对于图论产生了很大的兴趣，由于本人之前没有离散数学or图论的基础，对于一些例如induced graph的概念并不是清楚，之后计划会额外自学一些关于图论方面的知识。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS224w-HomeWork-1&quot;&gt;&lt;a href=&quot;#CS224w-HomeWork-1&quot; class=&quot;headerlink&quot; title=&quot;CS224w HomeWork 1&quot;&gt;&lt;/a&gt;CS224w HomeWork 1&lt;/h1&gt;&lt;p&gt;本文旨在记录CS22
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS224w" scheme="http://canVa4.github.io/tags/CS224w/"/>
    
      <category term="GNN" scheme="http://canVa4.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>FFT idea for Displacement Measurement</title>
    <link href="http://canva4.github.io/2020/10/27/FFT-idea-for-Displacement-Measurement/"/>
    <id>http://canva4.github.io/2020/10/27/FFT-idea-for-Displacement-Measurement/</id>
    <published>2020-10-27T12:00:33.000Z</published>
    <updated>2020-10-27T13:12:02.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FFT-idea-for-Displacement-Measurement"><a href="#FFT-idea-for-Displacement-Measurement" class="headerlink" title="FFT idea for Displacement Measurement"></a>FFT idea for Displacement Measurement</h1><p>Displacement Measurement任务核心：利用加速度计、陀螺仪（加速度和陀螺仪数据）来计算位移。</p><h2 id="Origin-Paper-Reading"><a href="#Origin-Paper-Reading" class="headerlink" title="Origin Paper Reading"></a>Origin Paper Reading</h2><p>原始思路：核心：计算倾角θ。假设：底部固定，只发生上部的形变。</p><p>计算倾角使用：加速度和陀螺仪来计算，二者的计算结果结合，通过参数α来调节。</p><p>在获取加速度数据前，先通过低通滤波器滤除震动噪声；获取角加速度前，先通过高通滤波器滤除漂移。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/W3rebtwV5Ohp1Yz.png" alt="image-20201027201049406" style="zoom:50%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/x5nZbIdwqLCJVfj.png" alt="image-20201027201103750" style="zoom: 50%;" /><h2 id="Idae-Using-FFT-to-calculate-Displacement"><a href="#Idae-Using-FFT-to-calculate-Displacement" class="headerlink" title="Idae: Using FFT to calculate Displacement"></a>Idae: Using FFT to calculate Displacement</h2><p>一时域信号x(t)，设其样本长度为T。则其傅里叶变换为$X(f)=\int_{0}^{T}x(t)e^{-j2\pi ft}dt$。</p><p>信号x(t)被采集（即采样）后，变为离散形式$x(nt_s)$，$t_s$为采样周期。设在T时间内采样了N个数据，就有：</p><ul><li>$$X(k)=\sum_{n=0}^{N-1}x(n)e^{-j(\frac{2\pi}{N})nk}$$</li></ul><p>其对应的傅里叶反变换为：</p><ul><li>$$x(n)=\frac{1}{N}\sum_{n=0}^{N-1}X(k)e^{j(\frac{2\pi}{N})nk}$$</li></ul><p>傅里叶变换后得到的X(k)是一个长度为N的离散复数序列。其第k个数据为：</p><p>$$X(k)=X(k/T)=a_k+jb_k$$</p><p>其代表了x(t)中频率为k/T的简谐运动分量，该分量用$x_k$表示。应有：</p><p>$$x_k=A_k\cos{(2\pi kt/T + \phi_k)}$$</p><p>其中$A_k=\sqrt{a_k^2+b_k^2}$为其幅值，$\phi_k=arctan(b_k/a_k)$为其相角。</p><p>运动台产生的应为一系列简谐运动的叠加，假设通过建筑物系统后仍为一系列简谐运动的叠加（看成时不变系统，如果有这个性质最好了，滤波就很方便，需要一些结构方面知识来确认），那么可以方便的滤除非明显频点的幅值，来降低噪声。若没有此性质，目前初步计划是使用一个低通滤波来滤除噪声（同原始论文）。</p><p>假设我们收到了加速度信号a(n)，对其使用傅里叶变换，得到：</p><p>$$A(k)=\sum_{n=0}^{N-1}a(n)e^{-j(\frac{2\pi}{N})nk}=u_k+jv_k$$</p><p>设真实的位移（Displacement）序列为d(n)，傅里叶变换后为D(k)。D(k)自然也能也成简谐运动的形式。根据加速度和位移是积分的关系。</p><p>若加速度为$a = Acos(\omega t+\phi)$，则位移$x=\int\int adt=\frac{A}{\omega^2}cos(\omega t+\phi-\pi)$。</p><p>直接可得</p><p>$$d_{1k}=\frac{A_k}{\omega_k^2}cos(\phi_k-\pi)$$</p><p>$$d_{2k}=\frac{A_k}{\omega_k^2}sin(\phi_k-\pi)$$</p><p>其中$A_k=\sqrt{u_k^2+v_k^2}$，$\phi_k=arctan(v_k/u_k)$，$w_k=2\pi k/T$。</p><p>而$D(k)=d_{1k}+jd_{2k}$。这样我们就得到了D(k)，再将D(k)做傅里叶反变换即可得到d(n)。即一段时间内的位移情况。</p><p>TODO：</p><ol><li>测试此方法。</li><li>思考如何引入角加速度的信息。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FFT-idea-for-Displacement-Measurement&quot;&gt;&lt;a href=&quot;#FFT-idea-for-Displacement-Measurement&quot; class=&quot;headerlink&quot; title=&quot;FFT idea for Displ
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="FFT" scheme="http://canVa4.github.io/tags/FFT/"/>
    
      <category term="Displacement Measurement" scheme="http://canVa4.github.io/tags/Displacement-Measurement/"/>
    
  </entry>
  
  <entry>
    <title>Crack Detection Paper Reading</title>
    <link href="http://canva4.github.io/2020/10/23/Crack-Detection-Paper-Reading/"/>
    <id>http://canva4.github.io/2020/10/23/Crack-Detection-Paper-Reading/</id>
    <published>2020-10-23T11:51:09.000Z</published>
    <updated>2020-12-02T13:45:11.633Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Crack-Detection-Paper-Reading"><a href="#Crack-Detection-Paper-Reading" class="headerlink" title="Crack Detection Paper Reading"></a>Crack Detection Paper Reading</h1><h2 id="Paper-LIST"><a href="#Paper-LIST" class="headerlink" title="Paper LIST:"></a>Paper LIST:</h2><ol><li><p>Autonomous concrete crack detection using deep fully convolutional neural network 2019</p></li><li><p>DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection 2018</p></li><li><p>Holistically-Nested Edge Detection(经典边缘检测算法HED) 2015</p></li><li><p>Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection 2019</p></li><li><p>A review on computer vision based defect detection and condition assessment of concrete and asphalt civil infrastructure 2015 <strong>TODO: 精读</strong></p></li><li><p>CrackGAN 2020</p></li><li><p>SegNet 2016</p></li><li><p>Feature Pyramid Networks for Object Detection 2017</p></li><li><p>RCF</p></li><li><p>Bi-Directional Cascade Network for Perceptual Edge Detection 2019</p><p><strong>THE FOLLOWINGS ARE MAINLY CLASSIFICATION TASK</strong></p></li><li><p>Image based techniques for crack detection, classification and quantification in asphalt pavement: a review 2017 <strong>CLF</strong> TODO</p></li><li><p>Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning 2018 <strong>Structural Health Monitoring</strong></p></li><li><p>Review and Analysis of Crack Detection and Classification Techniques based on Crack Types 2019 TODO</p></li><li><p>Concrete Cracks Detection Based on Deep Learning Image Classification <a href="https://www.mdpi.com/2504-3900/2/8/489">LINK</a> 2019</p></li><li><p>Multi-scale classification network for road crack detection 2018 <strong>CLF</strong> TODO</p></li><li><p>Learning relaxed deep supervision for better edge detection 2016 （improvement for HED）</p></li></ol><p><strong>Semantic Segmentation:</strong></p><ol><li>Dilated Residual Networks 2017 <a href="https://arxiv.org/abs/1705.09914">LINK</a></li><li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 2017 <a href="https://arxiv.org/abs/1606.00915">LINK</a></li><li>Multi-Scale Context Aggregation by Dilated Convolutions 2016 <a href="https://arxiv.org/abs/1511.07122">LINK</a></li><li>Understanding Convolution for Semantic Segmentation 2018 <a href="https://arxiv.org/abs/1702.08502">LINK</a></li><li>Rethinking Atrous Convolution for Semantic Image Segmentation 2017 <a href="https://arxiv.org/abs/1706.05587">LINK</a></li></ol><h2 id="Reading-More："><a href="#Reading-More：" class="headerlink" title="Reading More："></a>Reading More：</h2><p>From Paper number 16：</p><ol><li>Deepedge 2015</li><li>High-for-low and low-for-high: Efficient Boundary detection from deep object features 2015</li><li>Canny A computational approach to edge detection 1986</li><li>Deeply supervised Nets 2015</li></ol><h2 id="In-General"><a href="#In-General" class="headerlink" title="In General"></a>In General</h2><p>This part is for answering some general questions for this topic.</p><h3 id="Q1-Why-using-Deep-Learning-methods-instead-of-traditional-CV-algorithm"><a href="#Q1-Why-using-Deep-Learning-methods-instead-of-traditional-CV-algorithm" class="headerlink" title="Q1: Why using Deep Learning  methods instead of traditional CV algorithm:"></a>Q1: Why using Deep Learning  methods instead of traditional CV algorithm:</h3><p>Cracks may constantly suffer from noise in the background, leading to poor continuity and low contrast. That lead to the traditional CV algorithms have bad performance on these cases.</p><h3 id="Q2-Which-kind-of-tasks-in-CV-are-corresponding-to-Crack-Detection"><a href="#Q2-Which-kind-of-tasks-in-CV-are-corresponding-to-Crack-Detection" class="headerlink" title="Q2: Which kind of tasks in CV are corresponding to Crack Detection?"></a>Q2: Which kind of tasks in CV are corresponding to Crack Detection?</h3><p>Meanly 3 task:</p><ol><li>Semantic Segmentation</li><li>Edge Detection</li><li>Classification</li></ol><h3 id="Q3-Main-DataSets"><a href="#Q3-Main-DataSets" class="headerlink" title="Q3: Main DataSets:"></a>Q3: Main DataSets:</h3><p><strong>Concrete:</strong></p><ol><li>CRACK500</li><li>GAPs384</li><li>Cracktree200</li><li>sCFD</li><li>Aigle-RN &amp; ESAR &amp; LCMS</li></ol><p><strong>Pavement:</strong></p><ol><li>CFD</li><li>CGD</li></ol><h3 id="Q4-Metric-For-Crack-Detection-Task"><a href="#Q4-Metric-For-Crack-Detection-Task" class="headerlink" title="Q4: Metric For Crack Detection Task"></a>Q4: Metric For Crack Detection Task</h3><ol><li>最常见的指标之一就是PR（precision and recall）了，将二者统一，即变为F1-measure，这个在各个论文中评价crack detection 模型的性能中经常出现。</li></ol><ul><li>$$F_1=\frac{2*PR}{P+R}$$</li></ul><p>该值位于0~1之间，约接近1认为模型性能越好。</p><p>其缺点为：无法很好的表明检测到的裂缝和地面真实情况之间的重叠程度，尤其是在裂缝较大时。</p><ol start="2"><li>ODS &amp; OIS</li></ol><p>这两个是edge detection中的指标。边缘检测领域的标准准则是固定scale or threshhold(ODS)数据集上的最佳F值，每个图像中最佳scale or threshhold数据集上的平均F值(OIS)。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/FoQfzMcXZ89ngas.png" alt="image-20201029215820897" style="zoom:80%;" /><ol start="3"><li><p>AP(准确率)</p></li><li><p>FPHBN中提出的average intersection over union(AIU)。</p></li></ol><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/jbvE5lNcuyCkQKO.png" alt="image-20201029220020769" style="zoom:80%;" /><p>其中$N_t$表示阈值t的总个数,t在区间[0.01,0.99]内，间隔是0.01;对于给定的阈值t, $N_{pg}^t{}$为预测区域与真实裂缝区域相交（重叠）区域的像素个数，$N_p^t$和$N_g^t$分别表示预测裂缝区域和真值裂缝区域的像素数。因此，AIU在0到1之间，值越大，性能越好。数据集的AIU是数据集中所有图像的AIU的平均值。</p><p>提出的主要目的就是弥补PR指标的缺点，作为一个complementary measurement。AIU takes the width information into consideration to evaluate detections and illustrates the overall overlap extent between detections and ground truth.</p><h1 id="Paper’s-Detail"><a href="#Paper’s-Detail" class="headerlink" title="Paper’s Detail:"></a>Paper’s Detail:</h1><p>This part included the detail of each paper that I have perused.</p><h2 id="Autonomous-concrete-crack-detection-using-deep-fully-convolutional-neural-network"><a href="#Autonomous-concrete-crack-detection-using-deep-fully-convolutional-neural-network" class="headerlink" title="Autonomous concrete crack detection using deep fully convolutional neural network"></a>Autonomous concrete crack detection using deep fully convolutional neural network</h2><p>Author: Cao Vu Dung, Le Duc Anh     <a href="https://www.sciencedirect.com/science/article/pii/S0926580518306745">Paper Link</a></p><p>Journal: Automation in Construction 2019</p><p>Key Point: <strong>Concrete Crack Detection,  FCN(Fully CNN), Semantic Segmentation(Use FCN), Evaluate crack density, Test the Model on a real video</strong></p><p>数据集：<a href="https://data.mendeley.com/datasets/5y9wdsg2zt/1">https://data.mendeley.com/datasets/5y9wdsg2zt/1</a></p><p>模型很简单：Classification + Semantic Segmentation(FCN)</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/23/FEscvZiNOMCJPYK.png" alt="image-20201023202412382"></p><p>Semantic Segmentation Result：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/23/jpxeIOHzhwlG67m.png" alt="image-20201023202652414"></p><p>总结：特点不多，主要将CV中的Semantic Segmentation的经典方法（FCN）引入至Crack Detection领域，并且使用了较大的预训练( VGG16, ResNet, Inception)模型，获得结论使用预训练模型可以获得更好的预测结果，在实验部分做了不少工作。最后用了一个现实中的视频来验证一下model的效果。</p><h2 id="DeepCrack-Learning-Hierarchical-Convolutional-Features-for-Crack-Detection"><a href="#DeepCrack-Learning-Hierarchical-Convolutional-Features-for-Crack-Detection" class="headerlink" title="DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection"></a>DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection</h2><p>Author: Qin Zou er al  <a href="https://ieeexplore.ieee.org/document/8517148/similar">Paper LINK</a></p><p>Journal: IEEE TRANSACTIONS ON IMAGE PROCESSING(Published at 2018 Oct)</p><p>KEY WORDS: <strong>基于SegNet，fuse the feature map from encoder and decoder at different scales，crack/line/edge detection</strong></p><p>整体模型：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/24/FZEbNWKCVmY38Ap.png" alt="image-20201024200409648"></p><p><strong>模型核心：</strong></p><p>In DeepCrack, they first pairwisely fuse the convolutional features of the encoder network and decoder network at each scale, which produces the single-scale fused feature map, and then combine the fused feature maps at all scales into a multi-scale fusion map for crack detection. 这也是模型相比于SegNet最大的改进。</p><p>下图为SegNet的模型结构：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/iTbVDvAxs74yfS3.png" alt="image-20201027092352162"></p><p>下图为encoder和decoder的feature map是如何聚合的（即论文中的skip-layer）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/24/wT7AYeSmjWBNgus.png" alt="image-20201024200734542"></p><p><strong>Loss Function</strong></p><p>使用的是cross entropy（并没有对于不平衡类别 即：imbalance classification做处理）</p><p>最终的loss是由每层聚合的feature map和多层聚合的feature map的损失相加。</p><p><strong>Experiments And Results</strong></p><p>Modeling by <strong>caffee</strong></p><p><strong>Datasets</strong>: </p><ol><li>Training: CrackTree260</li><li>Testing: CRKWH100, CrackLS315, Stone331</li></ol><p><strong>Compare With Other Models</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/cg2CBjQtMNGwLx7.png" alt="image-20201027090600436"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/l6FXZJpDygiqBWf.png" alt="image-20201027090703558"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/twXOKVsmTEg6Pvu.png" alt="image-20201027090730577"></p><p><strong>Other Experiments</strong></p><ol><li>作者也比较了每一个scale对于最终结果的影响，即：在最终的loss中对于每一层的loss增加一个权重。</li><li>是否使用预训练模型的比较。有趣的是预训练的效果并不好，作者给出解释是：because that the pretrained model is well fit for nature image segmentation and is impossible or very difficult to be fine-tuned for crack detection.</li><li>作者讨论了在真实label中加入噪声对于模型的影响。结论是：对于噪声并不敏感。</li><li>作者还比较了不同的up-sampling方法的影响。即：传统的在max pooling时记录index的方法和bilinear interpolation的方法。结论：传统的在max pooling时记录index的up-sampling方法更好。</li><li>Different Weights on the Crack and Non-Crack Background，即在计算loss时对于不同的类别（即 crack or non-crack）赋予不同的权重。结论：会对性能提升，降低了类别不平衡的影响。</li><li>作者还比较了<strong>Running Efficiency</strong>, 上图给出的FPS为对于512×512大小的图片的实时处理速度。DeepCrack为：0.153 second per image。不过其使用的设备是GeForce GTX TITAN-X GPU和2.3GHz主频的E5-2630 CPU.</li></ol><h2 id="Holistically-Nested-Edge-Detection-HED"><a href="#Holistically-Nested-Edge-Detection-HED" class="headerlink" title="Holistically-Nested Edge Detection(HED)"></a>Holistically-Nested Edge Detection(HED)</h2><p>Author: Saining Xie er al  <a href="https://arxiv.org/abs/1504.06375">Paper LINK</a></p><p>Conference: CVPR 2015</p><p>KEY WORDS: <strong>基于VGGNET，mutil-scale fuse，edge detection</strong></p><p>此paper为使用CNN(VGG)的经典边缘检测模型，后期出现在多篇Crack Detection模型的baseline中。下一篇Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection得部分思路也是来源于HED的。这里主要总结和介绍一下模型部分。</p><p>其中Holistically表示该算法试图训练一个image-to-image的网络；Nested则强调在生成的输出过程中通过不断的集成和学习得到更精确的边缘预测图的过程。HED使用VGG改造的网络，针对整张图片提取特征信息，使用multi-scale fusion，multi-loss的方法。</p><p>值得注意的是，该方法的速度还是比较快的，可以达到0.4s per image(on the NYU Depth dataset)</p><p><strong>Model Detail</strong></p><p>下图为整个HED的模型结构。特征提取使用的是pre-trained的VGG网络。</p><p>作者给出performance提升的原因是：</p><ol><li>patch-based CNN edge detection methods</li><li>FCN-like image-to-image training allows models to simultaneously train on a significantly larger amount of samples</li><li>deep supervision in model guides the learning of more transparent features</li><li>interpolating the side outputs in the end-to-end learning encourages coherent contributions from each layer(used in fuse)</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/Oh8aFMxQpDrIdje.png" alt="image-20201027155719185"></p><p>本文的一大亮点就是使用了与以往不同的multi-scale fuse的方法。如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/F2GtTpIRcxJqyDu.png" alt="image-20201027163733313"></p><p>(a)Multi-stream learning 示意图，可以看到图中的平行的网络下（不同的网络有不同的参数和receptive field），输入是同时输进去，经过不同路的network之后，再连接到一个global out layer得到输出。</p><p>(b)Skip-layer network learning 示意图，该方法主要连接各个单的初始网络流得到特征图，并将图结合在一起输出。</p><p>这里（a）和（b）都是使用一个输出的loss函数进行单一的回归预测，而边缘检测可能通过多个回归预测得到结合的边缘图效果更好。</p><p>(c)Single model on multiple inputs 示意图，单一网络，图像resize方法得到多尺度进行输入。当用于test端时，有点类似于集成学习的感觉。</p><p>(d)Training independent networks ，通过多个独立网络分别对不同深度和输出loss进行多尺度预测，该方法下训练样本量较大。</p><p>(e)Holistically-nested networks，本文提出的结构，从（d）演化来，类似地是一个相互独立多网络多尺度预测系统，但是将multiple side outputs组合成一个单一深度网络。</p><p>这种方法的好处在于：基于hidden layer的supervision更有利于performance的提升；也相对的减小了模型的复杂度；在fuse时可以自由的调配每个scale的权重，或者通过学习来学到一个更好的参数。</p><p>作者主要使用了VGG16，并对其进行小幅度修改。</p><p><strong>Loss Function</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/QbWvcajnZixkYL7.png" alt="image-20201027170404471"></p><p>总的side-output loss为每一个side-output的加权和，权重可以通过学习来获得。由于edge detection任务的特性，大部分pixel为非edge的，所以存在明显的类别不平衡，每个side-output的loss使用下图的公式来解决这个问题。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/F3lSQWU8Ndg2J5b.png" alt="image-20201027170511603"></p><p>另外一部分的loss是fuse loss。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/8yiP3qKzN2dwOrS.png" alt="image-20201027170911639"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/BJA6Lk8pPaTo1dN.png" alt="image-20201027170938292"></p><p>h为fuse的一组参数。最终的loss和学习目标为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/Wa4JQD8bkIrnVhN.png" alt="image-20201027171037647"></p><p><strong>Experiment Results:</strong></p><p>Results on BSDS500 datasets.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/HoQmpjAEGfNy5LC.png" alt="image-20201027195428396"></p><h2 id="Feature-Pyramid-and-Hierarchical-Boosting-Network-for-Pavement-Crack-Detection"><a href="#Feature-Pyramid-and-Hierarchical-Boosting-Network-for-Pavement-Crack-Detection" class="headerlink" title="Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection"></a>Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection</h2><p>Author: Fan Yang er al  <a href="https://arxiv.org/abs/1901.06340">Paper LINK</a></p><p>Conference: CVPR 2019</p><p>KEY WORDS: <strong>Pavement Crack Detection，主要将Feature Pyramid Network(FPN)应用于Crack Detection task，提出新的 measurement for crack detection: average intersection over union (AIU)</strong></p><p><strong>模型细节：</strong></p><p>FPHBN使用了HED（本篇文档中也有HED的介绍与总结）作为其backbone net，在其之上引入了FPN的思想（FPN在本文档中也有介绍与总结）。</p><p>下图就是HED与FPHBN的模型对比图，可以很明显的看到，在HED后面加入了类似于FPN的思想（top-down + skip connection）和Hierarchy Boostings。Hierarchy Boostings可以起到平衡top layer与bottom layer的权重，可以让模型pay attention to hard examples.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/tQwqR5f9MONEAGk.png" alt="image-20201029210048041" style="zoom: 80%;" /><p>整个模型分为四个部分：</p><ol><li>A bottom-up architecture for hierarchical feature extraction</li><li>A feature pyramid for merging context information to lower layers using a top-down architecture</li><li>Side networks for deep supervision learning. The side network at each level performs crack prediction individually.</li><li>A hierarchical boosting module to adjust sample weights in a nested way</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/J6YuzEiAtFc49Le.png" alt="image-20201029212056151"></p><p>这里主要介绍一下Hierarchical Boosting，1~3部分即为HED+FPN，这二者在本文中都有介绍。</p><p>使用Hierarchical Boosting是为了解决HED的一个缺点，即：the network cannot effectively learn parameters from misclassified samples during training phase. 这个是因为edge detection task 本质的类别不平衡导致的，虽然在HED中引入了系数β来损失函数对不同类别的权重，但这种方法并不能区分easy and hard samples，因为类别不平衡很容易导致损失函数主要被不平衡类别占据。</p><p>作者针对这个问题（还有一些其他解决方法），提出了Hierarchical Boosting来reweigh samples。</p><p>想法来源于，回看feature pyramid层，可以看到，上层的信息会传递到下层，这种信息会告诉下层那些样本是hard的，这样这些下层的网络就可以pay attention to这些hard样本了。于是作者就想到来facilitating communication between adjacent side networks.</p><p>实现起来就是重写每一个side network的损失函数：$d_i^{m+1}$ 为第m+1个side network的预测值与真实值在第i个像素的差异。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/BtihxYVnDHAX2yJ.png" alt="image-20201029215013161" style="zoom:80%;" /><p><strong>Experiments And Results</strong></p><p>可以看到在inference时的速度也比HED有了提升。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/Jgt6xp2SoLkrzAG.png" alt="image-20201029215415393" style="zoom:80%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/q5Y9OGf8UuWPetN.png" alt="image-20201029215633706" style="zoom:80%;" /><p>Related Paper:</p><p>Y. Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai, “Richer convolutional features for edge detection,”</p><p>Deeply-supervised nets</p><h2 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h2><p>Author: Lin er al   <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">Paper Link</a></p><p>Journal: CVPR 2017</p><p>Key Point: <strong>Adding Pyramid representation into Deep Learning Methods, Multi-scale and pyramidal hierarchy</strong></p><p>Other’s Note: <a href="https://zhuanlan.zhihu.com/p/78160468">ZhiHu Link</a></p><p><strong>总结：</strong> FPN本质是另一种特征提取的结构，获得的representation可以用于很多不同的task，例如object detection 或者 semantic segmentation(crack detection就可以看为是这种任务)。其最大的好处是：在保持原有性能的情况下，极大的降低了模型的复杂度，让模型更feasible。现在FPN已经经常作为各种Detecton和segmentation算法的标准组件。</p><p>下图给出了几种常见的pyramid形式的特征提取方法。例如b方法就是常见的CNN所使用的结构。</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201028201708812.png" alt="image-20201028201708812" style="zoom:67%;" /><p>值得注意的是，在图中边框更粗的代表了更high-level的语义的特征（很符合直观）。个人理解：许多常见的multi-scale方法就是类似c的方法，将每层CNN的结果聚合作为预测结果，如SSD(Single Shot Detector)，SegNet和上面的HED。作者给出的(c)方法的缺点为：This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p><p>(a) 中的 Featurized image pyramid结构在ImageNet 或 COCO上取得了很好的表现，因为这种方法产生了multi-scale的representation而且每层特征提取都semantically strong。这种方法的主要问题就是：inference time过长，导致实际难以应用。</p><p>下图为FPN网络的具体图示，作者也引入了类似ResNet的skip connection。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/brWwTLKR79zapf2.png" alt="image-20201028204220471" style="zoom: 67%;" /><p>整体结构主要包含三个部分：</p><ol><li>Bottom-up pathway 正常使用backbone网络，如Resnet</li><li>Top-down pathway 使用nearest neighbor upsampling</li><li>Lateral connections 如下图所示。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/1QGbFcIAe9O67vE.png" alt="image-20201028210642456"></p><h2 id="SegNet-SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation"><a href="#SegNet-SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation" class="headerlink" title="SegNet: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"></a>SegNet: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</h2><p>Author: Vijay Badrinarayanan er al  </p><p>Journal: TPAMI 2015</p><p>Key Point: <strong>Semantic Segmentation，compare different upsampling methods</strong></p><ul><li>本文提出的SegNet和FCN、DeepLab-LargeFOV、DeconvNet做了比较，这种比较揭示了在实现良好分割性能的前提下内存使用情况与分割准确性的权衡。</li><li>SegNet的主要动机是场景理解的应用。因此它在设计的时候考虑了要在预测期间保证内存和计算时间上的效率。</li><li>定量的评估表明，SegNet在和其他架构的比较上，时间和内存的使用都比较高效。</li></ul><p><strong>模型细节</strong></p><p>Encoder: VGG16，移除全连接层(基本操作)</p><p>Decoder: Use pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. </p><p>特点：相比于最经典的FCN等，有更少的参数，相对更快的inference time。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/LyvwAGCUunghDzl.png" alt="image-20201101113757989"></p><p>实际上，SegNet与FCN最大的差别就在于upsampling的不同。作者在文章中还探讨了不同upsampling对性能的影响和每个方法可能的优劣。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/RI9yD4W1tjLhTk6.png" alt="image-20201101121932268"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/iVmDakgBtvPA9rG.png" alt="image-20201101121952246"></p><p>通过上表分析，可以得到如下分析结果：</p><ul><li>bilinear interpolation 表现最差，说明了在进行segmentation时，decoder是可学习的还是非常重要的。</li><li>SegNet-Basic与FCN-Basic对比，均具有较好的精度，不同点在于SegNet存储空间消耗小，FCN-Basic由于feature map进行了降维，所以时间更短。</li><li>SegNet-Basic与FCN-Basic-NoAddition对比，两者的decoder有很大相似之处，SegNet-Basic的精度更高，一方面是由于SegNet-Basic具有较大的decoder,同时说明了encoder过程中低层次feature map的重要性。</li><li>FCN-Basic-NoAddition与SegNet-Basic-SingleChannelDecoder：证明了当面临存储消耗，精度和inference时间的妥协的时候，我们可以选择SegNet，当内存和inference时间不受限的时候，模型越大，表现越好。</li></ul><p>作者总结到：</p><ul><li>encoder特征图全部存储时，性能最好。 这最明显地反映在语义轮廓描绘度量（BF）中。</li><li>当限制存储时，可以使用适当的decoder（例如SegNet类型）来存储和使用encoder特征图（维数降低，max-pooling indices）的压缩形式来提高性能。</li><li>更大更复杂的decoder提高了网络的性能。</li></ul><h2 id="Concrete-Cracks-Detection-Based-on-Deep-Learning-Image-Classification"><a href="#Concrete-Cracks-Detection-Based-on-Deep-Learning-Image-Classification" class="headerlink" title="Concrete Cracks Detection Based on Deep Learning Image Classification"></a>Concrete Cracks Detection Based on Deep Learning Image Classification</h2><p>Author: Wilson Ricardo Leal da Silva 1,OrcID andDiogo Schwerz de Lucena 2  <a href="https://www.mdpi.com/2504-3900/2/8/489">LINK</a></p><p>Journal: The 18th International Conference on Experimental Mechanics (ICEM 2018) 材料领域的会议</p><p>总结：模型真的非常简单。使用VGG16作为Backbone Net，在其上面做transfer Learning（pre-train model）。</p><p>整体感觉，没想到2018年的会议上也有这么水的文章，不过引用了一些可以扩展阅读的文献。</p><p>模型训练使用NVIDIA Tesla K80（约12G显存）</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/diWuIegAlKoED4L.png" alt="image-20201105220013434"></p><h3 id="Related-Paper"><a href="#Related-Paper" class="headerlink" title="Related Paper"></a>Related Paper</h3><p>使用无人机（UAV）做Crack detection</p><ol><li>Robust crack detection for unmanned aerial vehicles inspection in an acontrario decision framework 2015</li><li>Development of Crack Detection System with Unmanned Aerial Vehicles and Digital Image Processing 2015</li></ol><p>Deep Learning System for Automated Cracking</p><ol start="3"><li>Basis and Design of a Deep Learning System for Automated Cracking Survey 2017</li><li>Automatic Pavement Crack Detection Based on Structured Prediction with the Convolutional Neural Network. 2018</li></ol><h2 id="Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning"><a href="#Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning" class="headerlink" title="Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning"></a>Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning</h2><p>Author: Hyunjun Kim er al  <a href="https://www.researchgate.net/profile/Myoungsu_Shin2/publication/324707011_Crack_and_Noncrack_Classification_from_Concrete_Surface_Images_Using_Machine_Learning/links/5bec1eec92851c6b27bde0c2/Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning.pdf">LINK</a></p><p>Journal: Structural Health Monitoring 2018</p><p>Key Point: <strong>Concrete crack identification/classification，feature preprocessing(use speeded-up robust features) VS CNN, Detect existence and location of cracks</strong></p><p>作者给出的contribution为：</p><ul><li>an efficient classification framework based on a <strong>crack candidate region (CCR)</strong> is proposed to effectively categorize cracks and noncracks</li><li>comparative analysis between SURF-based and CNN-based methods is conducted to evaluate the classification performances</li><li>a comprehensive crack identification in the presence of crack-like noncracks is conducted for practical applications</li></ul><p>作者给出的SURF（speeded-up robust feature）是一种计算机视觉领域中的<strong>特征点检测</strong>的传统算法（无学习能力）。可以获得distinctive features。其主要包含以下两个步骤：</p><ol><li>interest point detection</li><li>interest point description</li></ol><h3 id="SURF-based-classification"><a href="#SURF-based-classification" class="headerlink" title="SURF-based classification"></a>SURF-based classification</h3><p>这部分与CV中的Bag-of-Words(Visual categorization with bags of keypoints 2004)的流程很相似，共分为三个阶段：</p><ol><li><p>Feature Extraction: 使用SURF对每个4*4的子区域生成一个64维feature vector。</p></li><li><p>Visual vocabulary construction(K-meas Clustering): 所有interest point的feature vector都用于visual word，该词用作代表性的小图像段，用来描述诸如颜色，形状和表面纹理之类的特征。之后在使用K-means来进行聚类，聚类的结果被分为许多组，这些组就是visual vocabulary or the bag of features.</p></li><li><p>Classification(SVM): 训练时，先试用feature extraction+k-means生成词汇表，然后BoW将每一个图片在每一个组内的数目统计下来，生成一个feature histograms来输入给SVM训练。</p></li></ol><p>下图为SURF-based classification和CNN-based classification(使用Fast RCNN)的示意图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/09/8DeSxBLqHNQuTr3.png" alt="image-20201109222216350"></p><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><p>主要由两部分组成：</p><ol><li>generation of CCRs</li><li>SURF-based or CNN-based classifications</li></ol><p>具体的流程为：</p><p>先将图片binarization，这样可以找到crack和crack-like noncrack，并对得到的结果的每个部分赋予类别。然后，使用CNN or SURF来提取特征，之后在进行分类。</p><h4 id="CCR（crack-candidate-region）"><a href="#CCR（crack-candidate-region）" class="headerlink" title="CCR（crack candidate region）"></a>CCR（crack candidate region）</h4><p>下图为生成CCR的流程图。 其中image binarization: 将所有pixel的取值二值化，0为黑色，1为白色计算时基于一个阈值，使用的方法为Sauvola’s binarization（该方法在noisy和high-contrast的图像上性能出色）来计算该阈值。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/Ll2toTmuNYOd1fp.png" alt="image-20201110123707934"></p><p>作者给出这种方法的好处为：</p><ol><li>获得的CCR只关注cracks和crack-liked noncrack.</li><li>计算时更加高效。因为只有被选出的CCR才会进入train and test stages.</li></ol><h4 id="SURF-based-and-CNN-based-classification-models"><a href="#SURF-based-and-CNN-based-classification-models" class="headerlink" title="SURF-based and CNN-based classification models"></a>SURF-based and CNN-based classification models</h4><p>大体流程和对比如下图所示：</p><p>CNN框架使用AlexNet。生成feature后，进入各自不同的分类模型，分类模型的介绍在上一节有写。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/g6vuGe5lzKSOREW.png" alt="image-20201110125058886"></p><p>最终模型的流程图为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/dkMLwv1hEfZ5TD7.png" alt="image-20201110125734790"></p><h3 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h3><p>居然是使用MATLAB来implement的 ！训练i7-7700 GTX1080，最终的分类结果。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/kvsPg3I82JhSyRc.png" alt="image-20201110134830501"></p><p>作者在多种超参数组合下，计算了各种性能指标，包括：P、R，F1、Acc，计算时间等。</p><p>结论是CNN-based methods的性能更好的。不过在训练速度上难以对比，因为CNN based method使用的是GPU，而SURF使用的是CPU。训练速度对比图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/LS1G8NUK6knc3tY.png" alt="image-20201110160403602"></p><p>只不过在有些样例上，使用CNN的方法效果不如SURF，作者给出的原因是：the local features extracted using the SURF can in some instances correctly classify the CCRs  hat were incorrectly categorized using the CNN-based method.</p><p>作者猜测使用：使用DeepLearning提取的feature和local feature的结合会进一步提高魔性的性能。</p><p>这里作者给出了一个很有趣、也很合理的结论，作者使用CCR后，相当于在训练时的样本基本上只包括crack 和 crack-like noncrack，这样相比于使用crack和intact surface来训练，极大的提升了模型性能。正如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/FYqUpxRWsHr2X3z.png" alt="image-20201110161951740"></p><h2 id="Richer-Convolutional-Features-for-Edge-Detection-RCF"><a href="#Richer-Convolutional-Features-for-Edge-Detection-RCF" class="headerlink" title="Richer Convolutional Features for Edge Detection(RCF)"></a>Richer Convolutional Features for Edge Detection(RCF)</h2><p>Author: Yun Liu er al  </p><p>Journal: CVPR 2017</p><p>另一篇经典的edge detection的算法。算法的特点：性能优于HED，速度基本与HED持平。其中也提出的Fast RCF：**在BSDS500数据集上，ODS指标可以达到0.806 with(30 FPS)**。人类在这个benchmark上的水平为：0.803.</p><p>相比于HED的改进就在于以下三点：</p><ol><li>使用了每次卷积后的结果，而不是同HED只使用每个stage的最后一个输出</li><li>改变了loss，引入了对于类别不平衡的修正</li><li>引入Multiscale Hierarchical Edge Detection，会从一定程度上提升准确度，显然会导致运算速度的下降。</li></ol><p>本质：exploits multi-scale and multi-level information</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>同HED，也是使用VGG16network，将VGG16的5个conv阶段分开，分别产生一个side-output.（这部分与HED很相似）。</p><p>下图为RCF的网络结构：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/9Uh32sRWfJScHju.png" alt="image-20201122205002664"></p><p>整体就是一个VGG16，与HED的不同之处在于HED是在每一个conv stage的最后取最后的输出做multi-scale fuse，拼接后再过一个1*1的卷积得到最终的结果。</p><p>而RCF进一步的使用了网络中每次卷积的结果，在每个stage的conv的结果都被输出而聚合。这也正是paper的标题：Richer Convolutional Features的由来。</p><p>NOTE：每个stage的划分标准是根据max pooling layer来分割的。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>使用的每一个loss如下图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/xVEfwpaOdFC9zUv.png" alt="image-20201122211255637"></p><p>可以看到本质就是一个二分类的交叉熵，训练数据本身的正例和反例的不均匀性（即edge在图中的pixel的数量是远远小于非edge的），所以α和β来起到一个类别平衡的作用。而超参数λ用来balance positive and negative samples.</p><p>最终的loss是各个loss的和，如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/VKw1AUyZ8CYexHR.png" alt="image-20201122211636629"></p><h3 id="Multiscale-Hierarchical-Edge-Detection"><a href="#Multiscale-Hierarchical-Edge-Detection" class="headerlink" title="Multiscale Hierarchical Edge Detection"></a>Multiscale Hierarchical Edge Detection</h3><p>在test time时，为了提高模型的准确性，使用了image pyramids，有点类似于使用集成学习的方法。具体来讲：we resize an image to construct an image pyramid, and each of these images is input to our single-scale detector separately. Then, all resulting edge probability maps are resized to original image size using bilinear interpolation. At last, these maps are averaged to get a final prediction map. </p><p>即如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/Um81vzqQBEafOW9.png" alt="image-20201122212050249"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>训练时也使用了同HED一样的数据增强。在NYUD数据集上，为了使用深度信息，作者使用HHA，来进一步做对比。</p><h2 id="CrackGAN"><a href="#CrackGAN" class="headerlink" title="CrackGAN"></a>CrackGAN</h2><p>Author: Kaige Zhang, Yingtao Zhang, and H. D. Cheng  <a href="https://arxiv.org/abs/1909.08216">LINK</a></p><p>Journal: CVPR 2020</p><p>Key Point: <strong>Pavement crack detection，generative adversarial learning, partially accurate ground truths(GTs)</strong></p><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul><li><p>Why label is <strong>partially accurate GTs</strong>?</p><p>在我们获取数据集的时候（这里尤其是pavement数据），往往是通过在一辆车上固定一个相机来拍摄，之后再由人工手工标注而获得的。由于是通过这种方式来获取label的，crack往往都非常的细，相对于background而言是很少的，而且他们的边界也相对模糊。这也导致了对于pixel-level的ground truth标记困难。所以在实践中，往往只将crack标记为1-pixel的曲线，这种GT也被称为labor-light GT（减小了人工标注的成本）。所以很明显，这种GT并不是完全准确的（pixel-level），所以被称为<strong>partially accurate ground truths(GTs)</strong></p></li><li><p>Then What Happened when using FCN-based methods？</p><p>显然，这种情况下，这是一个明显的类别不平衡问题，前人的解决方法往往是使用对于loss做一个修正。作者认为这种方法是无法解决这个问题的，因为<strong>partially accurate GTs</strong>的存在。所以就导致了the network will simply converge to the status that treats the entire crack image as BG (labeled with zero), and still can achieve a good detection accuracy or loss (BG-samples dominate the accuracy calculation). 这就是<strong>ALL BLACK</strong> problem。此问题也经常出现在其他的pixel-level pavement crack detection之中。</p></li></ul><p>So, CrackGAN is solving:</p><ol><li>solving ALL BLACK issue</li><li>proposes the crack-patch-only (CPO) supervised adversarial learning and the asymmetric U-Net architecture to perform<br>the end-to-end training</li><li>the network can be trained with partially accurate GTs generated by labor-light method which can reduce the workload of preparing GTs significantly</li><li>solve data imbalance problem which is the byproduct of the proposed approach</li></ol><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/qHBsKmReYXWkt2V.png" alt="image-20201127102112858"></p><p>D is a pretrained discriminator obtained directly from a pre-trained DC-GAN using crack-GT patches only. 这类的discriminator会令网络一直产生包含crack GT的image，作者认为这个是解决ALL BLACK问题的关键。</p><p>作者为了解决这个问题，在loss中加入了额外的generative adversarial loss.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/JYL5G6yNo7kd2zj.png" alt="image-20201127110512690" style="zoom: 67%;" /><p>实际上就是使用一个预训练好的discriminator（训练一个只使用crack  ground-truth的data，这样如果出现没有crack就认为其是fake，这样的discriminator作者称为one-class）来防止出现将整个图片都预测为ground truth的情况。</p><p>下图就是对于discriminator预训练的过程（训练一个DC-GAN），其中CPO是由人工产生的。文中多次提到的COP-supervision指的是：the training data are prepared with crack patches only, without involving any non-crack patches and “all black” patches.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/tu3zsFVK2onCfvq.png" alt="image-20201127213218128" style="zoom:80%;" /><p>训练好这个discriminator后，这个discriminator和模型的核心 U-net一起训练。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/WvV6nFyAfQJENlr.png" alt="image-20201127215836597" style="zoom: 80%;" /><p>至于为什么只使用CPO-supervision就可以让整个模型也处理不包含crack的图像，作者在C. Asymmetric U-Net for BG-image translation给出了解释。目前这部分还不是很明白，涉及到了pix2pix GAN的Receptive field. 而我本人对于GAN的了解不是很多，所以部分之后会在了解了GAN之后继续更新的。</p><p>最后还有一部分是加入了一个pixel-level的loss. y是经过处理的partially accurate GT（将1-pixel width的crack变为3-pixel宽）。直观来讲：就是U-net生成结果的crack在扩展后的真实label内。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/tprUJ3eMCKsLqFW.png" alt="image-20201127223942711" style="zoom:80%;" /><p>作者还指出，这个asymmetric U-net可以处理任意尺寸的input image.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li>K. Zhang, H. D. Cheng, and B. Zhang, “Unified approach to pavement crack and sealed crack detection using pre-classification based on transfer learning,” J. Comput. Civil Eng., vol. 32, no. 2 (04018001), 2018.</li><li>K. Zhang, H. D. Cheng, and S. Gai, “Efficient Dense-Dilation Network for Pavement Crack Detection with Large Input Image Size,” in Proc. IEEE ITSC 2018, Maui.</li><li>F. C. Chen, and M. R. Jahanshahi, “NB-CNN: Deep learning-based crack detection using convolutional neural network and Nave Bayes data fusion,” IEEE Transactions on Industrial Electronics, vol. 65, no. 5, pp.4392-4400, 2018.</li><li>S. Park, S. Bang, and H. Kim, “Patch-Based Crack Detection in Black Box Images Using Convolutional Neural Networks,” Journal of Computing in Civil Engineering, vol. 33, no. 3 (040190170), 2019.</li><li>N. D. Hoang, Q. L. Nguyen, and V. D. Tran, “Automatic recognition of asphalt pavement cracks using metaheuristic optimized edge detection algorithms and convolution neural network,” Automation in Construction, vol. 94, pp. 203-213, 2018.</li><li>K. Gopalakrishnan, S. K. Khaitan, A. Choudhary, and A. Agrawal, “Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection,” Construction and Building Materials, vol. 157, pp. 322-330, 2017.</li><li>Q. Zou, Z. Zhang, Q. Li, X. Qi, Q. Wang, S. Wang, “DeepCrack: Learning hierarchical convolutional features for crack detection,” IEEE Transactions on Image Processing, vol. 28, no. 3, pp. 1498-512, 2019.</li><li>F. Yang, L. Zhang, S. Yu, D. Prokhorov, X. Mei, H. Ling, “Feature pyramid and hierarchical boosting network for pavement crack detection,” arXiv preprint arXiv:1901.06340. 2019.</li><li>M. Mirza, and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014.</li><li>P. Isola, J. Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proc. IEEE CVPR, 2017.</li><li>A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” in Proc. ICLR, 2016.</li></ol><h2 id="Learning-relaxed-deep-supervision-for-better-edge-detection"><a href="#Learning-relaxed-deep-supervision-for-better-edge-detection" class="headerlink" title="Learning relaxed deep supervision for better edge detection"></a>Learning relaxed deep supervision for better edge detection</h2><p>Author: Yu Liu er al  <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Learning_Relaxed_Deep_CVPR_2016_paper.pdf">LINK</a></p><p>Journal: CVPR 2016</p><p>文中很有特点的一点就是使用：relaxed supervision，即：使用额外的relaxed label，这些relaxed label是使用simple detector来获取的（比如：Canny算子）之后再与真实的ground truth 相结合。</p><p>本质来讲就是RDS(本篇文章提出的模型)使用relaxed labels 和额外的训练数据来 retrain HED，这样可以获得更好的性能的提升。</p><p><strong>问题引出：</strong></p><p>作者认为HED的模型对于所有的intermediate layer只使用了一个general supervision（有真实标记的）。作者认为这样让网络的diversities消失了，即hierarchical layer(CNN)不同层所获取特征不再具有多样性。所以作者就想引入：diversities supervision 来重新赋予网络的CNN layer重新获得coarse to fine的表示。这种diversities supervision就是作者本文提出的relaxed deep supervision(RDS). </p><p>RDS 加入了额外的label（原有的label是edge为positive，non-edge为negative）。额外加入的label使用simple detector来获取的（比如：Canny算子）。之后将这些relaxed label加入的原始的ground truth来训练。</p><p><strong>Contribution：</strong></p><ol><li>提出了RDS这种relaxed label来提升HED算法的性能</li><li>因为在许多edge detection任务中，数据标注的成本很高，导致训练数据很少，例如BSDS500只有200个训练数据。作者还证明了使用使用一些在大型数据集上预选练的模型，之后fine-tune，可以提升模型的性能。</li></ol><p><strong>Related Work：</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/oPQBKhdMUfVkzLy.png" alt="image-20201130120942864"></p><ul><li>pixel-level的work有: DeepEdge和其的一篇改进。extract deep features peer pixel and classify it to edge and non-edge class.</li><li>patch-level的work有：DeepContour。这类方法：estimate edge maps for the input patches and then integrate them for the whole edge map.</li><li>image-level主要就是：HED了。一类end-to-end的方法。</li></ul><p>作者主要工作就是将HED与DSN（deeply supervised net）相结合</p><p><strong>Model‘s Architecture：</strong></p><p>与HED极其的相似，不同点在于：</p><ul><li>HED：每个side output和fuse output都用真实label</li><li>RDS：side output用自己生成的relaxed label+positive label；最终的fuse output的label为真实label</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/YSa9TElGLCigH5Z.png" alt="image-20201130122707987"></p><p><strong>实际上就是作者手工让hierarchical layer强行学到不一样的东西。</strong> 结果显示这样可以提升性能。</p><p>作者在生成relaxed label时有两种选择，分别为<strong>Canny算子</strong>和<strong>SE Detector</strong>。</p><p>下图为这些方法在不同参数下提取到的特征的结果。通常来讲，SE detector可以获得更稀疏的relaxed label.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/ltocrCOV1w4xkgq.png" alt="image-20201130131508627"></p><p><strong>Loss：</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/FrHNJYcyz5XVBxp.png" alt="image-20201130133627195"></p><p>其中$L_{side}$如下图所示，这里Relaxed label为2，GT为1，non-edge为0。 α和β是用来平衡类别不平衡带来的影响</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/qEomyO8Rp6ftIK3.png" alt="image-20201130132143508"></p><p><strong>Pretrain with CEA:</strong></p><p>CEA（coarse edge annotation）指的是在大型的例如semantic segmentation数据集PSACAL VOC上，是有一些edge的label的，但其并不精细，如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/6oGWkhFStfT7IgR.png" alt="image-20201130133516278"></p><p> 使用这些大型的数据集来预训练RDS模型，之后在BSDS500使用RDS的方法进行fine tune，可以达到更好的效果。</p><p>整体流程如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/Rgs1t9GDuCOvfEx.png" alt="image-20201130133653994"></p><h2 id="Multi-Scale-Context-Aggregation-by-Dilated-Convolutions"><a href="#Multi-Scale-Context-Aggregation-by-Dilated-Convolutions" class="headerlink" title="Multi-Scale Context Aggregation by Dilated Convolutions"></a>Multi-Scale Context Aggregation by Dilated Convolutions</h2><p>Author: Fisher Yu 2016  <a href="https://arxiv.org/abs/1511.07122">LINK</a>  第一篇提出空洞卷积的方法。空洞卷积是下面的那篇BDCN使用到了，这也是我第一次了解这个卷积操作。</p><p>Journal: ICLR 2016</p><p><strong>提出Dilated Conv的paper. 一种专门design for dense prediction（例如：semantic segmentation）的方法</strong></p><p>这种空洞卷积（Dilated Conv）<strong>support exponential expansion of the receptive field without loss of resolution or coverage!</strong></p><p>作者还指出，将这种Dilated Conv可以作为插入模块，来提升这种dense prediction的性能。</p><p><strong>背景：</strong></p><p>在图像分割领域，图像输入到CNN（典型的网络比如FCN）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像（feature）尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。</p><p><strong>Detail About Dilated Conv：</strong></p><p>对于公式化卷积的过程可以如下所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/x4Upe38VFKAMzLm.png" alt="image-20201202204630447"></p><p>其使用的卷积核与普通CNN一直，只不过对于每一个Dilated Conv存在一个factor l.</p><p>下图就是Dilated Conv的一个直观实例，也显示出了其特点：<strong>The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly.</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/V1mB5KvAc8dSpMr.png" alt="image-20201202210406002"></p><p>dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv。</p><p>下图是传统的卷积：或者说是1-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201202211039151.png" alt="image-20201202211039151"></p><p>而下图是2-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/ojTmcJvQiIMV8ep.png" alt="image-20201202211056595"></p><p><strong>值得注意的是这里的这里对于filter的初始化不能是随机的。</strong></p><h2 id="Bi-Directional-Cascade-Network-for-Perceptual-Edge-Detection"><a href="#Bi-Directional-Cascade-Network-for-Perceptual-Edge-Detection" class="headerlink" title="Bi-Directional Cascade Network for Perceptual Edge Detection"></a>Bi-Directional Cascade Network for Perceptual Edge Detection</h2><p>Author: Jianzhong He er al  <a href="https://arxiv.org/pdf/1902.10903.pdf">LINK</a>  一篇比较新的2019年的edge detection的方法。</p><p>Journal: CVPR 2019</p><p><strong>此方法基本上是目前edge detection的state-of-art的方法！</strong></p><p>作者给出的改进在于：</p><ol><li>Bi-Directional Cascade Network：增强不同scale可以学到的不同的东西。即让模型focus on meaningful details and deep layers should depict object-level boundaries.</li><li>加入额外的Scale Enhancement Module（SEM）：目的是增强每个scale的学习能力。</li></ol><p>整体的模型大致如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201201221204410.png" alt="image-20201201221204410"></p><p>其中的ID Block是Incremental Detection Block，使用VGG网络的每个卷积部分加上SEMs组成的。This bi-directional cascade structure enforces each layer to focus on a specific scale, allowing for a more rational training procedure.</p><p>在我看来，模型相比于HED，RCF这种经典的multi-scale方法的改进之处在于：</p><ol><li><strong>改进了loss function，引入了不同scale layer的communicate。使用的是每层之间的supervise是相互关联的</strong></li><li><strong>增加每个scale的表示能力（每个scale额外使用 dilated convolution）</strong></li></ol><p>每个ID Block的样子如下图所示：由VGG+SEM（本质就是一组卷积+dilated convolution）</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201201233106975.png" alt="image-20201201233106975"></p><p>Reference: 本部分对应的paper在pdf中用波浪线给出</p><ol><li><p>L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915, 2016.</p></li><li><p>Learning deep structured multi-scale features using attention-gated crfs for contour prediction. In NIPS, pages 3964–3973, 2017.  About Edge detection</p></li><li><p>Attention to scale: Scale-aware semantic image segmentation. In CVPR, pages 3640–3649, 2016.</p></li><li><p>dilated convolution</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Crack-Detection-Paper-Reading&quot;&gt;&lt;a href=&quot;#Crack-Detection-Paper-Reading&quot; class=&quot;headerlink&quot; title=&quot;Crack Detection Paper Reading&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Works/Papers/"/>
    
    
      <category term="Crack Detection" scheme="http://canVa4.github.io/tags/Crack-Detection/"/>
    
      <category term="Deep Learning" scheme="http://canVa4.github.io/tags/Deep-Learning/"/>
    
      <category term="CV" scheme="http://canVa4.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Running Neural Networks On Embedding Systems</title>
    <link href="http://canva4.github.io/2020/10/04/Running-Nuerual-Networks-On-Embedding-Systems/"/>
    <id>http://canva4.github.io/2020/10/04/Running-Nuerual-Networks-On-Embedding-Systems/</id>
    <published>2020-10-04T09:11:39.000Z</published>
    <updated>2020-11-02T10:25:24.017Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Running-Neural-Networks-On-Embedding-Systems"><a href="#Running-Neural-Networks-On-Embedding-Systems" class="headerlink" title="Running Neural Networks On Embedding Systems"></a>Running Neural Networks On Embedding Systems</h1><p>我检索资料时，核心的关键词是：<strong>云平台+边缘计算、IoT、AI加速</strong>。所以主要看的是有GPU或者有ASIC（专用集成电路）的方案。</p><p>经过资料的查找之后，我发现许多做硬件的大公司比如：高通、NVIDIA，Google、海思都有自己的解决方案。</p><p>以下为我找到的一些资料。资料来源：各个公司的官网；淘宝，论坛、博客。</p><p>同时给出一些我个人的简易分析，这些分析可能不是很准确，因为只是根据网上资料所得的结果。</p><p>TODO:</p><ul><li>各个芯片的加速程度对比</li></ul><h2 id="Qualcomm-高通"><a href="#Qualcomm-高通" class="headerlink" title="Qualcomm 高通"></a>Qualcomm 高通</h2><p>高通的优势之一就是其提供了一整套完整的硬件和软件的解决方案。<a href="https://www.qualcomm.com/products/artificial-intelligence">高通AI LINK</a> </p><p>其中Qualcomm NPE(Neural Processing Engine) 使用Caffe or tensorflow写好模型后，用Qualcomm NPE SDK添加到晓龙CPU/GPU上运行模型。整个开发使用流程如下所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/jar7ecdq9KwPXGT.png" alt="image-20201004205847255"></p><p>其主要应用领域就用手机AI、IoT领域等。 </p><p><strong>优势：</strong></p><ul><li>有完整的软件+硬件方案</li><li>模型建立后部署方便</li><li>支持AI的晓龙芯片性能强大，因为是SOC，一个芯片实现了非常多的功能</li></ul><p><strong>不足：</strong></p><ul><li>Data Center支持有限</li><li>目前国际形势可能导致不方便购买其解决方案（还未详细了解）<h2 id="Cambricon-寒武纪"><a href="#Cambricon-寒武纪" class="headerlink" title="Cambricon 寒武纪"></a>Cambricon 寒武纪</h2></li></ul><p>寒武纪是国内的一家目前在智能芯片市场上很出名的企业。是一家国内提供智能芯片，满足：有终端硬件+云平台的需求。<a href="http://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=71">寒武纪 LINK</a> </p><p>寒武纪同样也有几款主打的AI加速芯片。同时他也有一个云平台——寒武纪人工智能开发平台（Cambricon NeuWare）是寒武纪专门针对其云、边、端的智能处理器产品打造的软件开发平台， Neuware采用端云一体的架构，可同时支持寒武纪云、边、端的全系列产品。寒武纪终端IP、边缘端芯片及云端芯片共享同样的软件接口和完备生态，可以方便地进行智能应用的开发，迁移和调优。</p><p>开发者可以借助云端丰富的计算资源进行算法模型的解析与调试，利用Neuware生成离线模型，并能够在任意搭载寒武纪智能终端IP的设备运行，解决了终端调试手段受硬件资源限制的问题。</p><p>下图为其端云一体业务部署流程。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/Xxi7QG9Cale16OP.png" alt="image-20201004211854078"></p><p>下图为其软件栈。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/YnSeXP6Cs2UGE4f.png" alt="image-20201004211515460"></p><p><strong>优势：</strong></p><ul><li>国内的方案，购买、实施、使用相对方便</li><li>有专门的云平台</li></ul><p><strong>不足：</strong></p><ul><li>真实性能和效果未知</li><li>其云端的重点好像是主要用于调试和部署</li></ul><h2 id="NVIDIA-Jetson-系列"><a href="#NVIDIA-Jetson-系列" class="headerlink" title="NVIDIA Jetson 系列"></a>NVIDIA Jetson 系列</h2><p>NVIDIA Jetson 系列。GPU大厂的解决方法，据我所知，在机器人领域使用很多。其Jetson系列都有GPU，且可运行Linux操作系统。比如：我原来所在北邮机器人队，需要摄像头进行一些较大运算量算法时，就使用的是Jetson系列的TX2。</p><p>其主打：适用于新一代自主机器的嵌入式系统；NVIDIA Jetson：适用于一切自主机器的 AI 平台。Jetson系统所提供的性能和能效可提高自主机器软件的运行速度，而且功耗更低。每个系统都是一个完备的模块化系统 (SOM)，具备 CPU、GPU、PMIC、DRAM 和闪存，可节省开发时间。自然也具有可扩展性，比如：支持USB，串口，HDMI等接口（不同的型号不太一样）。</p><p>其主要有四个如下的产品，从左至右性能依次提升。（当然价格和功耗也会提升）</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/JX2FyS5eZEABtPT.png" alt="image-20201005210658302"></p><p>例如：广泛用于智能小车、比较需要计算量的智能产品中的Jetson Nano。</p><p>Jetson Nano 模块是一款低成本的 AI 计算机，具备超高的性能和能效，可以运行现代 AI 工作负载，并行运行多个神经网络，以及同时处理来自多个高清传感器的数据。这使其成为向嵌入式产品中添加高级 AI 的理想的入门级选项。</p><p>下图为Jetson Nano的照片，可以看到其支持多种外设接口。<a href="https://detail.tmall.com/item.htm?id=608609593274&ali_refid=a3_430582_1006:1268380158:N:sH2jsRQiqncfQma5KNTJH3QCsGA2TDnC:33fec7c4be41b2615b4987bf0b673408&ali_trackid=1_33fec7c4be41b2615b4987bf0b673408&spm=a230r.1.14.13&skuId=4314923092276">淘宝链接</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/slfVR6Y3WiXoSAZ.png" alt="image-20201005210954346"></p><p>而且其另一大优势时都是NVIDIA一家的，其GPU支持CUDA，这样例如：pytorch等可以很方便的部署。</p><p>Jetson 平台由 Jetpack SDK 提供支持，包括板级支持包 (BSP)、Linux 操作系统、NVIDIA CUDA(R)，并且兼容第三方平台。开发者可以利用 DeepStream SDK 在 Jetson 上快速构建和部署高效的视频分析管线。</p><p>下面是其软件栈：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/RCc2L8bsJ4uS3YK.png" alt="image-20201005211204653"></p><p><strong>优势：</strong></p><ul><li>产品种类多，可选择空间大</li><li>模型部署方便</li><li>购买比较方便（淘宝即可）</li><li>教程和应用实例比较多</li></ul><p><strong>不足：</strong></p><ul><li>加速性能由于不同的GPU架构可能差距很大</li></ul><p>TODO: Data Center 调研</p><h2 id="Google-TPU"><a href="#Google-TPU" class="headerlink" title="Google TPU"></a>Google TPU</h2><h3 id="Cloud-TPU"><a href="#Cloud-TPU" class="headerlink" title="Cloud TPU"></a>Cloud TPU</h3><p>张量处理单元 (TPU) 是专门设计用于处理机器学习应用计算需求的 ASIC 设备。有着很完善的文档，教程等。<a href="https://cloud.google.com/tpu/docs/quickstart?hl=zh-cn">链接</a></p><h3 id="Edge-TPU"><a href="#Edge-TPU" class="headerlink" title="Edge TPU"></a>Edge TPU</h3><p>作为Cloud TPU的补充，目前Edge TPU主要作用于推理，专为在边缘运行TensorFlow Lite ML模型而设计。</p><p>AIY Edge TPU 加速器是一个适用于现有系统的神经网络协处理器，一个加速棒。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/pm8INj3TZcfRiME.png" alt="image-20201005214027249"></p><p>AIY Edge TPU Dev开发板 是一个带搭载Edge TPU的单板计算机。类似于NVIDIA Jetson Nano。下图左边为树莓派，中间为Google Edge TPU Dev，右边为NVIDIA Jetson Nano</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/c5OB3J7nwXb6WFP.png" alt="image-20201005214053059"></p><p>其Edge TPU是对于TensorFlow Lite有专门优化的，在计算速度上强于Jetson Nano。</p><p><strong>优势：</strong></p><ul><li>云端非常强大</li><li>文档支持良好</li><li>对于TensorFlow有专门优化</li></ul><p><strong>不足：</strong></p><ul><li>国内可能不支持</li></ul><h2 id="瑞芯微电子"><a href="#瑞芯微电子" class="headerlink" title="瑞芯微电子"></a>瑞芯微电子</h2><p>国内的一家提供相关解决方案的厂家。主要为为高端智能硬件、手机周边、平板电脑、电视机顶盒、工控等多个领域提供专业芯片解决方案。</p><p>目前许多门禁系统的人脸识别部分，有许多使用的是瑞芯的RK3288或者RK3399（性能更强）系列。均使用arm内核。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/20/eqaQcPkpIGWxgLX.png" alt="image-20201020114444744"></p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>AI加速计算，如：Intel Movidius神经计算棒（可以结合树莓派使用，即支持Linux操作系统）</p><p>华为旗下的高端芯片企业。主打AI处理器。也提供了许多领域的解决方案：如：IoT，Face Cam，门禁系统等的解决方法。主要偏向于高端市场。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Running-Neural-Networks-On-Embedding-Systems&quot;&gt;&lt;a href=&quot;#Running-Neural-Networks-On-Embedding-Systems&quot; class=&quot;headerlink&quot; title=&quot;Runn
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="TPU" scheme="http://canVa4.github.io/tags/TPU/"/>
    
      <category term="嵌入式系统" scheme="http://canVa4.github.io/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="智能终端" scheme="http://canVa4.github.io/tags/%E6%99%BA%E8%83%BD%E7%BB%88%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>单片机解决方案调研（2）</title>
    <link href="http://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/"/>
    <id>http://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/</id>
    <published>2020-09-14T12:36:48.000Z</published>
    <updated>2020-11-26T11:41:35.503Z</updated>
    
    <content type="html"><![CDATA[<h1 id="解决方案调研（2）"><a href="#解决方案调研（2）" class="headerlink" title="解决方案调研（2）"></a>解决方案调研（2）</h1><p>GOAL：针对上一次老师提出的点进行补充调研。<a href="https://ieeexplore.ieee.org/document/8812785">https://ieeexplore.ieee.org/document/8812785</a></p><p>KEY WORDS: <strong>具体通信细节、耗电量估算、MSP系列、Google edge TPU（另写一篇）</strong></p><p>在认真读完SnowFort这篇文章后，我更进一步了解了我们设计的预期，想相比于SnowFort有更进一步的提升，可能可以在如下方面做一些升级：</p><ul><li>提升Mote的运算性能，以便可以在mote上进行更多的数据处理</li><li>提升Mote的可扩展性，可以让每个mote接入更多种类的传感器</li><li>令Mote有更长or维持当前的功率（使用时间）</li><li>提升Base Station的运算性能，可以不用考虑其功率情况</li><li>组网能力提升、通信距离提升</li><li>云端功能、数据处理算法提升</li></ul><p>综上，除了对于老师上次提出的点进行补充，我也尝试着从这些方面来尝试改进。</p><h2 id="具体通信细节"><a href="#具体通信细节" class="headerlink" title="具体通信细节"></a>具体通信细节</h2><p>在上一篇调研的最后有一个简易的实现方案。<a href="https://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/">LINK</a></p><h3 id="STM32-PART"><a href="#STM32-PART" class="headerlink" title="STM32 PART"></a>STM32 PART</h3><p>无限通信部分STM32系列方案使用的是 <a href="https://detail.tmall.com/item.htm?id=609757779633&ali_refid=a3_430582_1006:1267360122:N:9/mfWI1BJMLzXLT4ATlUnA==:de4e276b258975c722c4a03cf64e8c17&ali_trackid=1_de4e276b258975c722c4a03cf64e8c17&spm=a230r.1.14.8">ATK-ESP8266</a> 。</p><p>该模块的核心性能指标如图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/14/VsDwkx5XSnAEjLQ.png" alt="image-20200914204633006" style="zoom: 67%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/14/RWDBFwVb9e6svLY.png" alt="image-20200914204839819" style="zoom:67%;" /><h3 id="M5STACK-PART"><a href="#M5STACK-PART" class="headerlink" title="M5STACK PART"></a>M5STACK PART</h3><p>其核心是esp32系列芯片。esp32是esp8266的升级版，其WIFI支持：802.11 b/n/g，与ATK-ESP8266相似。其相比于ATK-ESP8266还支持蓝牙。</p><ul><li>传统蓝牙支持 L2CAP，SDP，GAP，SMP，AVDTP，AVCTP，A2DP (SNK) 和 AVRCP (CT) 协议</li><li>低功耗蓝牙 (Bluetooth LE) 支持 L2CAP，GAP，GATT，SMP，和 GATT 之上的 BluFi，SPP-like 协议等</li></ul><h2 id="STM32L-低功耗系列"><a href="#STM32L-低功耗系列" class="headerlink" title="STM32L 低功耗系列"></a>STM32L 低功耗系列</h2><p><strong>产品栈</strong></p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201014114115216.png" alt="image-20201014114115216"></p><p><strong>超低功耗模式中的不同产品系列</strong></p><p>有M0+内核的STM32L0，有Cortex-M3内核的L1以及Cortex-M4内核的L4和L4+，其中L0和L1都有5种低功耗模式，这5种低功耗模式分别是低功耗的运行、睡眠、低功耗睡眠、停止和待机。对于L4和L4+，它们在5种低功耗模式基础上又添加了停止模式下的stop 1、stop 2和shutdown关断模式。可以通过内部的寄存器配置，来切换工作模式，在不同的模式下会有不同的唤醒时间。尤其是L4产品中shutdown模式，做到了非常低的功耗。</p><p><strong>低功耗运行模式</strong></p><p>其实低功耗运行模式还是一种运行模式，只是它的电流消耗很低，它与运行模式最大的区别是给内核供电的内部电压调节器电压要低于正常的运行模式下的电压值，即使用的低功耗的电压器来供电。在此情况下，系统最大的运行频率也会明显降低，例如L4在低功耗运行模式时最大的频率不超过2MHz。</p><p><strong>睡眠模式</strong></p><p>在睡眠模式，系统的CPU也就是Cortex-M内核的时钟被关闭了，但外设是可以继续保持运转的，它整个I/O的引脚状态与运行模式下也是相同的。</p><p><strong>低功耗睡眠模式</strong></p><p>低功耗睡眠模式是基于睡眠模式下的低功耗模式，是具有极低电流消耗的睡眠模式，它内核的时钟也是被关闭的，同时外设时钟频率受到了限制，因为它的电压调节器属于低功耗状态，内部的FLASH是要被停止的，所以低功耗睡眠模式只能从低功耗运行模式进入，这个是和其他模式不同的，其他模式都可以从运行模式直接做切换。</p><p>在低功耗运行和睡眠模式下，可以有一个BAM模式，它的工作方式是通过RTC加一个外设加DMA加SRAM，在不需要CPU干预的情况下就可以自行做数据采集，一旦到了数据采集需要到CPU处理的条件时，然后再把CPU唤醒做处理，所以这整个一个小系统就实现了一个协处理器的功能。</p><p><strong>停止模式</strong></p><p>首先说一下其的供电系统，其中有一个Vcore，它是内核的一个供电区域，负责给CPU内核供电，并且还给系统内部的存储器和它的数字外设供电。</p><p>停止模式中，除了CPU，也就是Cortex-M内核的时钟被关闭外，内核供电域的时钟也被停止，在停止模式下，内核供电域的时钟全部都停掉，PLL内部、外部的高速时钟全部都停掉，电压调节器为内核供电域供电，保留寄存器和内部SRAM中的内容。</p><p>在L4和L4+系列中，停止模式被细分为stop 0、stop 1和stop 2三种模式，按照功耗从低到高来说，stop 2是功耗最低的一个stop模式，它整个Vcore电源域放在了更低的漏电流模式下，使用了低功耗的电压调节器，只有最少的外设可以工作，所以它的功耗相对来说是最低的，但是唤醒时间是最长的。</p><p>Stop 1模式提供了更多的外设和唤醒源，唤醒时间也会更长一些；</p><p>Stop 0模式主电压调节器打开，可以得到最快的唤醒时间；</p><p>在所有的stop模式下，所有的高速振荡器停止，而低速振荡器保持活动，外设设置为active，需要的时候就可以使用这些高速时钟，能保证它在一些特定的事件下去唤醒设备。</p><p><strong>待机模式</strong></p><p>在待机模式下，内核的供电是直接断电的，电压调节器掉电区寄存器的内容会完全丢失，包括内部的SRAM，所以最大的区别即，系统从待机模式下的低功耗唤醒的时候，系统是要复位的。</p><p>默认条件的待机模式下，SRAM的内容是会丢失的，但是在L4里增加了SRAM 2，如果需要在待机模式后系统唤醒的时候有SRAM能保存一些内容，那就可以使用SRAM 2，它需要有多余220nA的额外电流消耗。</p><p><strong>Shutdown模式</strong></p><p>在shutdown模式，系统达到了最低的功耗，电压调节器的供电就被关断了，内核的供电也完全被断开，只有备份域的LSE、RTC可以工作所以在L4器件实现了一个新的模式，这个模式主要实现的目的就是为了延长电池供电之后整个器件的使用寿命，它其实是通过关闭内部的稳压器以及禁止使用耗电的监控，所以这个模式可以达到最低的功耗电流。</p><h2 id="MSP系列芯片"><a href="#MSP系列芯片" class="headerlink" title="MSP系列芯片"></a>MSP系列芯片</h2><p>MSP430的电压已经降到了3.3v，且MSP430比芯片分成了许多不同的模块部分，不用的部分功能模块可以关闭，电流近似为零，这样就极大的节省了能耗；另一个值得注意的是，其可以有三个时钟源，并产生更多的内部可用工作频率，让内部各个模块工作在不同的频率，不用的时钟也可以关掉。<strong>具体寄存器和模式切换TODO</strong></p><h2 id="MSP430-VS-STML4"><a href="#MSP430-VS-STML4" class="headerlink" title="MSP430 VS STML4"></a>MSP430 VS STML4</h2><p><strong>ULP Benchmark</strong></p><p>在超低功耗MCU领域，有一些评测不同芯片功耗水平的Benchmark。</p><p>其中比较有名和有代表性的是：ULP Benchmark <a href="https://www.eembc.org/ulpmark/scores.php">https://www.eembc.org/ulpmark/scores.php</a></p><p>NOTE：不同的编译方法，在真实功耗上会有很大的差异性。例如使用IAR和ARM GCC编译器在同一个芯片上的表现可能还不同。</p><p>从表中可以看到，STM32L4的大部分产品的在该Benchmark上的得分都是高于MSP430系列的。</p><p>其中，我选择了STM32L433（L4中较常见的一款，淘宝可买到）和MSP430FR5969（为MSP430Core）对比，可以看到STM32L4的功耗在该Benchmark下更低。<a href="https://www.eembc.org/viewer/?benchmark_seq=2760,2679">DETAIL</a></p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201014121221114.png" alt="image-20201014121221114"></p><h2 id="耗电量估算"><a href="#耗电量估算" class="headerlink" title="耗电量估算"></a>耗电量估算</h2><p>ESP8266系列功耗</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/27/gjav2t9mG5BpFUT.png" alt="image-20200927172219640" style="zoom:67%;" /><p>TODO：</p><p>LINKS：</p><p>ULP Benchmark <a href="https://www.eembc.org/ulpmark/scores.php">https://www.eembc.org/ulpmark/scores.php</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;解决方案调研（2）&quot;&gt;&lt;a href=&quot;#解决方案调研（2）&quot; class=&quot;headerlink&quot; title=&quot;解决方案调研（2）&quot;&gt;&lt;/a&gt;解决方案调研（2）&lt;/h1&gt;&lt;p&gt;GOAL：针对上一次老师提出的点进行补充调研。&lt;a href=&quot;https://ie
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="单片机" scheme="http://canVa4.github.io/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="STM32" scheme="http://canVa4.github.io/tags/STM32/"/>
    
      <category term="msp" scheme="http://canVa4.github.io/tags/msp/"/>
    
      <category term="TPU" scheme="http://canVa4.github.io/tags/TPU/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment3 遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-27T09:04:38.000Z</published>
    <updated>2020-09-13T09:23:11.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment3-遇到的问题"><a href="#CS231n-Assignment3-遇到的问题" class="headerlink" title="CS231n Assignment3 遇到的问题"></a>CS231n Assignment3 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Image-Captioning-with-RNNs"><a href="#Image-Captioning-with-RNNs" class="headerlink" title="Image Captioning with RNNs"></a>Image Captioning with RNNs</h2><p>本部分主要是实现RNN的基础版本。即如下的RNN，不过需要注意的是在代码中实现时要注意矩阵相乘的顺序。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/FQu3U9OsD5gtvYi.png" alt="image-20200903233248910"></p><p>首先是每次time step时的forward的backward，这里比较简单，按照上图公式implement一下就ok了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span>(<span class="params">x, prev_h, Wx, Wh, b</span>):</span></span><br><span class="line">    next_h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    next_h = np.tanh(np.dot(prev_h, Wh) + np.dot(x, Wx) + b)</span><br><span class="line">    cache = Wx, Wh, next_h, prev_h, x, b</span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span>(<span class="params">dnext_h, cache</span>):</span></span><br><span class="line">    dx, dprev_h, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    Wx, Wh, next_h, prev_h, x, b = cache</span><br><span class="line">    dmid = (<span class="number">1</span> - np.square(next_h)) * dnext_h</span><br><span class="line">    dprev_h = np.dot(dmid, Wh.T)</span><br><span class="line">    dx = np.dot(dmid, Wx.T)</span><br><span class="line">    dWh = np.dot(prev_h.T, dmid)</span><br><span class="line">    dWx = np.dot(x.T, dmid)</span><br><span class="line">    db = np.sum(dmid, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>然后是在一定time sequence上的forward和backward，forward就是多层step forward的叠加，backward计算梯度就是将每次对x的梯度持续回传，将对W权值矩阵的梯度叠加即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span>(<span class="params">x, h0, Wx, Wh, b</span>):</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    cache = []</span><br><span class="line">    h = np.zeros((N, T, h0.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, c = rnn_step_forward(x[:, i, :], h0, Wx, Wh, b)</span><br><span class="line">        h[:, i] += h0</span><br><span class="line">        cache.append(c)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span>(<span class="params">dh, cache</span>):</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    D = cache[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, H))</span><br><span class="line">    dWh = np.zeros((H, H))</span><br><span class="line">    db = np.zeros((H,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        dx[:, i], dh0, dWx_mid, dWh_mid, db_mid = rnn_step_backward(dh[:, i] + dh0, cache.pop())</span><br><span class="line">        dWx += dWx_mid</span><br><span class="line">        dWh += dWh_mid</span><br><span class="line">        db += db_mid</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>实现这些基本核心组件后，还需要实现的就是根据word 生成 embedding，这里使用的是类似于查询的方法，有一个对应的生成embedding的矩阵，这个也是可以学习的。forward很简单，就是类似的查询，backward的实现我遇到了实现上的问题，最后借鉴了一下别人的code。:)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_forward</span>(<span class="params">x, W</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    out = W[x]</span><br><span class="line">    cache = x, W</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dW = <span class="literal">None</span></span><br><span class="line">    x, W = cache</span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line">    N, T, D = dout.shape</span><br><span class="line">    np.add.at(dW, x.flatten(), dout.reshape(<span class="number">-1</span>, D))     <span class="comment"># 这里借鉴了一下别人的代码</span></span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure><p>最后要实现的就是class RNN了。这个部分只要认真看代码中的提升，注意下细节根据流程和之前实现好的模块实现即可了。</p><p><strong>forward 函数</strong>，位于<code>rnn.py</code> rnn类内。这里的处理是将caption分为两部分：captions_in除了最后一个单词外，所有内容都将被输入到RNN； 而captions_out只不包含第一个单词。这就是期望RNN生成的东西。 它们彼此相对偏移一个，因为RNN在接收到单词t之后会产生单词（t + 1）。 captions_in的第一个元素将是START caption，我们的期望是captions_out的第一个元素将是第一个单词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, features, captions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute training-time loss for the RNN. We input image features and</span></span><br><span class="line"><span class="string">    ground-truth captions for those images, and use an RNN (or LSTM) to compute</span></span><br><span class="line"><span class="string">    loss and gradients on all parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Input image features, of shape (N, D)</span></span><br><span class="line"><span class="string">    - captions: Ground-truth captions; an integer array of shape (N, T) where</span></span><br><span class="line"><span class="string">      each element is in the range 0 &lt;= y[i, t] &lt; V</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Scalar loss</span></span><br><span class="line"><span class="string">    - grads: Dictionary of gradients parallel to self.params</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Cut captions into two pieces: captions_in has everything but the last word</span></span><br><span class="line">    <span class="comment"># and will be input to the RNN; captions_out has everything but the first</span></span><br><span class="line">    <span class="comment"># word and this is what we will expect the RNN to generate. These are offset</span></span><br><span class="line">    <span class="comment"># by one relative to each other because the RNN should produce word (t+1)</span></span><br><span class="line">    <span class="comment"># after receiving word t. The first element of captions_in will be the START</span></span><br><span class="line">    <span class="comment"># token, and the first element of captions_out will be the first word.</span></span><br><span class="line">    captions_in = captions[:, :<span class="number">-1</span>]</span><br><span class="line">    captions_out = captions[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You&#x27;ll need this</span></span><br><span class="line">    mask = (captions_out != self._null)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the affine transform from image features to initial</span></span><br><span class="line">    <span class="comment"># hidden state</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Word embedding matrix</span></span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input-to-hidden, hidden-to-hidden, and biases for the RNN</span></span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the hidden-to-vocab transformation.</span></span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward and backward passes for the CaptioningRNN.   #</span></span><br><span class="line">    <span class="comment"># In the forward pass you will need to do the following:                   #</span></span><br><span class="line">    <span class="comment"># (1) Use an affine transformation to compute the initial hidden state     #</span></span><br><span class="line">    <span class="comment">#     from the image features. This should produce an array of shape (N, H)#</span></span><br><span class="line">    <span class="comment"># (2) Use a word embedding layer to transform the words in captions_in     #</span></span><br><span class="line">    <span class="comment">#     from indices to vectors, giving an array of shape (N, T, W).         #</span></span><br><span class="line">    <span class="comment"># (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #</span></span><br><span class="line">    <span class="comment">#     process the sequence of input word vectors and produce hidden state  #</span></span><br><span class="line">    <span class="comment">#     vectors for all timesteps, producing an array of shape (N, T, H).    #</span></span><br><span class="line">    <span class="comment"># (4) Use a (temporal) affine transformation to compute scores over the    #</span></span><br><span class="line">    <span class="comment">#     vocabulary at every timestep using the hidden states, giving an      #</span></span><br><span class="line">    <span class="comment">#     array of shape (N, T, V).                                            #</span></span><br><span class="line">    <span class="comment"># (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #</span></span><br><span class="line">    <span class="comment">#     the points where the output word is &lt;NULL&gt; using the mask above.     #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># In the backward pass you will need to compute the gradient of the loss   #</span></span><br><span class="line">    <span class="comment"># with respect to all model parameters. Use the loss and grads variables   #</span></span><br><span class="line">    <span class="comment"># defined above to store loss and gradients; grads[k] should give the      #</span></span><br><span class="line">    <span class="comment"># gradients for self.params[k].                                            #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># Note also that you are allowed to make use of functions from layers.py   #</span></span><br><span class="line">    <span class="comment"># in your implementation, if needed.                                       #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    caches = []</span><br><span class="line">    out, cache = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    word_in, cache = word_embedding_forward(captions_in, W_embed)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:  <span class="comment"># must rnn or lstm</span></span><br><span class="line">        out, cache = rnn_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out, cache = lstm_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    out, cache = temporal_affine_forward(out, W_vocab, b_vocab)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss, dx = temporal_softmax_loss(out, captions_out, mask)</span><br><span class="line"></span><br><span class="line">    dx, grads[<span class="string">&#x27;W_vocab&#x27;</span>], grads[<span class="string">&#x27;b_vocab&#x27;</span>] = temporal_affine_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = rnn_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = lstm_backward(dx, caches.pop())</span><br><span class="line">    grads[<span class="string">&#x27;W_embed&#x27;</span>] = word_embedding_backward(d_caption, caches.pop())</span><br><span class="line">    _, grads[<span class="string">&#x27;W_proj&#x27;</span>], grads[<span class="string">&#x27;b_proj&#x27;</span>] = affine_backward(dx, caches.pop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p><strong>sample 函数</strong>，位于<code>rnn.py</code> rnn类内。其要实现的效果如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/AtirSuRMczObNC9.png" alt="image-20200903234531509"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, features, max_length=<span class="number">30</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Run a test-time forward pass for the model, sampling captions for input</span></span><br><span class="line"><span class="string">    feature vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep, we embed the current word, pass it and the previous hidden</span></span><br><span class="line"><span class="string">    state to the RNN to get the next hidden state, use the hidden state to get</span></span><br><span class="line"><span class="string">    scores for all vocab words, and choose the word with the highest score as</span></span><br><span class="line"><span class="string">    the next word. The initial hidden state is computed by applying an affine</span></span><br><span class="line"><span class="string">    transform to the input image features, and the initial word is the &lt;START&gt;</span></span><br><span class="line"><span class="string">    token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For LSTMs you will also have to keep track of the cell state; in that case</span></span><br><span class="line"><span class="string">    the initial cell state should be zero.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Array of input image features of shape (N, D).</span></span><br><span class="line"><span class="string">    - max_length: Maximum length T of generated captions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - captions: Array of shape (N, max_length) giving sampled captions,</span></span><br><span class="line"><span class="string">      where each element is an integer in the range [0, V). The first element</span></span><br><span class="line"><span class="string">      of captions should be the first sampled word, not the &lt;START&gt; token.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = features.shape[<span class="number">0</span>]</span><br><span class="line">    captions = self._null * np.ones((N, max_length), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unpack parameters</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement test-time sampling for the model. You will need to      #</span></span><br><span class="line">    <span class="comment"># initialize the hidden state of the RNN by applying the learned affine   #</span></span><br><span class="line">    <span class="comment"># transform to the input image features. The first word that you feed to  #</span></span><br><span class="line">    <span class="comment"># the RNN should be the &lt;START&gt; token; its value is stored in the         #</span></span><br><span class="line">    <span class="comment"># variable self._start. At each timestep you will need to do to:          #</span></span><br><span class="line">    <span class="comment"># (1) Embed the previous word using the learned word embeddings           #</span></span><br><span class="line">    <span class="comment"># (2) Make an RNN step using the previous hidden state and the embedded   #</span></span><br><span class="line">    <span class="comment">#     current word to get the next hidden state.                          #</span></span><br><span class="line">    <span class="comment"># (3) Apply the learned affine transformation to the next hidden state to #</span></span><br><span class="line">    <span class="comment">#     get scores for all words in the vocabulary                          #</span></span><br><span class="line">    <span class="comment"># (4) Select the word with the highest score as the next word, writing it #</span></span><br><span class="line">    <span class="comment">#     (the word index) to the appropriate slot in the captions variable   #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># For simplicity, you do not need to stop generating after an &lt;END&gt; token #</span></span><br><span class="line">    <span class="comment"># is sampled, but you can if you want to.                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You will not be able to use the rnn_forward or lstm_forward       #</span></span><br><span class="line">    <span class="comment"># functions; you&#x27;ll need to call rnn_step_forward or lstm_step_forward in #</span></span><br><span class="line">    <span class="comment"># a loop.                                                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> we are still working over minibatches in this function. Also if   #</span></span><br><span class="line">    <span class="comment"># you are using an LSTM, initialize the first cell state to zeros.        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    next_h, _ = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    next_c = np.zeros((N, W_proj.shape[<span class="number">1</span>]))</span><br><span class="line">    word = self._start * np.ones((N,), dtype=np.int32)</span><br><span class="line">    <span class="comment"># generate start token</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">        word, _ = word_embedding_forward(word, W_embed)</span><br><span class="line">        <span class="comment"># embed the word to vector</span></span><br><span class="line">        <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">            next_h, _ = rnn_step_forward(word, next_h, Wx, Wh, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_h, next_c, _ = lstm_step_forward(word, next_h, next_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">        out, _ = affine_forward(next_h, W_vocab, b_vocab)</span><br><span class="line">        <span class="comment"># get the output</span></span><br><span class="line">        word = out.argmax(axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># sample</span></span><br><span class="line">        captions[:, i] = word</span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> captions</span><br></pre></td></tr></table></figure><h2 id="Image-Captioning-with-LSTMs"><a href="#Image-Captioning-with-LSTMs" class="headerlink" title="Image Captioning with LSTMs"></a>Image Captioning with LSTMs</h2><p>本部分主要就是将vanilla RNN变为LSTM在重复上述的任务。</p><p>首先是forward，根据公式敲一下就ok了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span>(<span class="params">x, prev_h, prev_c, Wx, Wh, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that a sigmoid() function has already been provided for you in this file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - prev_h: Previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - prev_c: previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - next_c: Next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for backward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    next_h, next_c, cache = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, H = prev_h.shape</span><br><span class="line">    a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    i = sigmoid(a[:, :H])</span><br><span class="line">    f = sigmoid(a[:, H:<span class="number">2</span>*H])</span><br><span class="line">    o = sigmoid(a[:, <span class="number">2</span>*H:<span class="number">3</span>*H])</span><br><span class="line">    g = np.tanh(a[:, <span class="number">3</span>*H:])</span><br><span class="line">    next_c = f * prev_c + i * g</span><br><span class="line">    next_h = o * np.tanh(next_c)</span><br><span class="line">    cache = i, f, o, g, next_c, Wh, Wx, prev_c, prev_h, x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_h, next_c, cache</span><br></pre></td></tr></table></figure><p>经历过那么多次求导，出现的问题越来越少了，实现起来也比较顺畅，下面给出我整理后的求导过程。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/06/Z4HB6QspSybWL9A.png" alt="image-20200906111112135"></p><p>然后就是敲一下代码了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span>(<span class="params">dnext_h, dnext_c, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    i, f, o, g, next_c, Wh, Wx, prev_c, prev_h, x = cache</span><br><span class="line">    dprev_c = dnext_c * f + dnext_h * o * f * (<span class="number">1</span> - np.tanh(next_c)**<span class="number">2</span>)</span><br><span class="line">    dc = dnext_c + (<span class="number">1</span> - np.tanh(next_c)**<span class="number">2</span>) * o * dnext_h     <span class="comment"># 这里遇到了问题</span></span><br><span class="line">    di = dc * g * i * (<span class="number">1</span> - i)</span><br><span class="line">    df = dc * prev_c * f * (<span class="number">1</span> - f)</span><br><span class="line">    do = dnext_h * np.tanh(next_c) * o * (<span class="number">1</span> - o)</span><br><span class="line">    dg = dc * i * (<span class="number">1</span> - g**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    da = np.hstack((di, df, do, dg))</span><br><span class="line">    dprev_h = np.dot(da, Wh.T)</span><br><span class="line">    dx = np.dot(da, Wx.T)</span><br><span class="line">    db = np.sum(da, axis=<span class="number">0</span>)</span><br><span class="line">    dWx = np.dot(x.T, da)</span><br><span class="line">    dWh = np.dot(prev_h.T, da)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>接下来就是对于一个sequence而不是单独的time step使用LSTM了。首先是forward</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span>(<span class="params">x, h0, Wx, Wh, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></span><br><span class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></span><br><span class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    N, H = h0.shape</span><br><span class="line">    h = np.zeros((N, T, H))</span><br><span class="line">    cache = []</span><br><span class="line">    c0 = np.zeros_like(h0)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, c0, c = lstm_step_forward(x[:, i, :], h0, c0, Wx, Wh, b)</span><br><span class="line">        h[:, i, :] = h0</span><br><span class="line">        cache.append(c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br></pre></td></tr></table></figure><div class="note warning">            <p>需要注意的是，在backward时，如何传递梯度：up stream的当前time step的loss + 上一个step step传回来的loss</p>          </div><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span>(<span class="params">dh, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    _, D = cache[<span class="number">0</span>][<span class="number">-1</span>].shape    <span class="comment"># cache[0][-1]对应x</span></span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dc = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, <span class="number">4</span> * H))</span><br><span class="line">    dWh = np.zeros((H, <span class="number">4</span> * H))</span><br><span class="line">    db = np.zeros((<span class="number">4</span> * H,))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        dx[:, i, :], dh0, dc, dWx_, dWh_, db_ = lstm_step_backward(dh[:, i, :] + dh0, dc, cache.pop())</span><br><span class="line">        db += db_</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>剩下的部分就比较ez了，只需对于上一个task的代码稍作修改即可，具体的代码就不再列出。</p><h2 id="Network-Visualization-PyTorch"><a href="#Network-Visualization-PyTorch" class="headerlink" title="Network Visualization (PyTorch)"></a>Network Visualization (PyTorch)</h2><p>本部分主要实现<strong>Saliency Maps,  Fooling image, Class visualization</strong> 三种的实现本质都是计算输入图片的梯度，Saliency Map是直接将关于正确label的loss的梯度的绝对值显示出来；Fooling Image 则是利用梯度信息，将一个A类别的输入变为网络识别为B类别；Class Visualization则是输入一个噪声，使用gradient ascent利用梯度信息将该图片在期望变成的类别下，神经网络的输出的期望类别的class score最大。由于使用pytorch，可以直接计算grad，整体实现比较简单，理解好这几个过程就可以了。代码部分不再单独给出，详情见我的github仓库。</p><div class="note warning">            <p>在代码运行时，可能会遇到使用numpy.load()函数报错的情况，提示需要将allow_pickle=Ture，此时只需要在该np.load的参数内加入‘allow_pickle=True’即可。具体细节可见<code>cs231n/data_utils.py/load_imagenet_val</code></p>          </div><h2 id="Style-Transfer"><a href="#Style-Transfer" class="headerlink" title="Style Transfer"></a>Style Transfer</h2><p>本部分就是实现style transfer的部分了！难度不高，核心就是实现好3个loss，并将整个流程梳理下来即可。实现后真的非常好玩！</p><p>首先是Style Transfer（2016）整体的框图</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/6FYEV92PxcUlXDH.png" alt="image-20200910100801040"></p><p>从图中就可以看出，包含两个loss，分别是：style image每层的每个filter的activation map的gram matrix（用来衡量相似性）和input image的gram matrix之间产生的loss（<strong>Style Loss</strong>）和 input image和content image之间的差异性（<strong>Content Loss</strong>）。最后为了使生成的图片更加真实，会加入一个正则项，这里使用的是<strong>Total-variation regularization</strong>。所以最终的loss就是：**Style Loss+Content Loss + Total-variation regularization **。之后使用这个loss计算输入图片的梯度，并使用Adam或者SDG等方法更新输入图片即可。</p><p>提取特征的网络使用的是squeeze net，因为其模型参数少，运算快且性能适中。</p><h3 id="Content-Loss"><a href="#Content-Loss" class="headerlink" title="Content Loss"></a>Content Loss</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/3cBroD8hxIkadGQ.png" alt="image-20200910101647611"></p><p>比较简单，如框图所示，就是将input image和content image在某一层的activation map做一下reshape，从1*C*H<em>W变为1\</em>C*(H*W)即可。相减后逐元素平方再求和即可。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span>(<span class="params">content_weight, content_current, content_original</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Compute the content loss for style transfer.</span></span><br><span class="line"><span class="string">Inputs:</span></span><br><span class="line"><span class="string">- content_weight: Scalar giving the weighting for the content loss.</span></span><br><span class="line"><span class="string">- content_current: features of the current image; this is a PyTorch Tensor of shape</span></span><br><span class="line"><span class="string">  (1, C_l, H_l, W_l).</span></span><br><span class="line"><span class="string">- content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">- scalar content loss</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    loss = content_weight * torch.sum(torch.square(content_current.squeeze() - content_original.squeeze()))</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Style-Loss"><a href="#Style-Loss" class="headerlink" title="Style Loss"></a>Style Loss</h3><p>稍微复杂一点，核心就是计算一个Gram matrix，该矩阵用来衡量similarity。下图为Gram matrix计算的示意图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/LSUCjJ5Y19N2AtF.png" alt="image-20200910102135973"></p><p>左边是某一层的activation map，之后将其变为C*(H*W)，所有行向量直接做內积，就有了C*C的gram matrix。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">features, normalize=True</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the Gram matrix from features.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: PyTorch Tensor of shape (N, C, H, W) giving features for</span></span><br><span class="line"><span class="string">      a batch of N images.</span></span><br><span class="line"><span class="string">    - normalize: optional, whether to normalize the Gram matrix</span></span><br><span class="line"><span class="string">        If True, divide the Gram matrix by the number of neurons (H * W * C)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - gram: PyTorch Tensor of shape (N, C, C) giving the</span></span><br><span class="line"><span class="string">      (optionally normalized) Gram matrices for the N input images.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N,C,H,W = features.size()</span><br><span class="line">    new_features = features.reshape((N,C,H*W))</span><br><span class="line">    gram_mat = torch.zeros(N,C,C)</span><br><span class="line">    gram_mat = torch.bmm(new_features, new_features.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        <span class="keyword">return</span> gram_mat / (H*W*C)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> gram_matm</span><br></pre></td></tr></table></figure><p>之后就是如框图所示，将style image各层的gram matrix和input image各层的gram matrix相减后逐元素平方再求和即可。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span>(<span class="params">feats, style_layers, style_targets, style_weights</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the style loss at a set of layers.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - feats: list of the features at every layer of the current image, as produced by</span></span><br><span class="line"><span class="string">      the extract_features function.</span></span><br><span class="line"><span class="string">    - style_layers: List of layer indices into feats giving the layers to include in the</span></span><br><span class="line"><span class="string">      style loss.</span></span><br><span class="line"><span class="string">    - style_targets: List of the same length as style_layers, where style_targets[i] is</span></span><br><span class="line"><span class="string">      a PyTorch Tensor giving the Gram matrix of the source style image computed at</span></span><br><span class="line"><span class="string">      layer style_layers[i].</span></span><br><span class="line"><span class="string">    - style_weights: List of the same length as style_layers, where style_weights[i]</span></span><br><span class="line"><span class="string">      is a scalar giving the weight for the style loss at layer style_layers[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - style_loss: A PyTorch Tensor holding a scalar giving the style loss.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tensor = torch.tensor(())</span><br><span class="line">    loss = tensor.new_zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(style_layers)):</span><br><span class="line">        gram_mat = gram_matrix(feats[style_layers[i]])</span><br><span class="line">        loss += style_weights[i] * torch.sum((gram_mat - style_targets[i]).square())</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Total-variation-regularization"><a href="#Total-variation-regularization" class="headerlink" title="Total-variation regularization"></a>Total-variation regularization</h3><p>该项是一个正则化项，公式如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/FXuz216PhIvrbx9.png" alt="image-20200910102605899"></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span>(<span class="params">img, tv_weight</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute total variation loss.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.</span></span><br><span class="line"><span class="string">    - tv_weight: Scalar giving the weight w_t to use for the TV loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - loss: PyTorch Variable holding a scalar giving the total variation loss</span></span><br><span class="line"><span class="string">      for img weighted by tv_weight.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = tv_weight * (torch.sum((img[<span class="number">0</span>,:,<span class="number">1</span>:,:] - img[<span class="number">0</span>,:,:<span class="number">-1</span>,:]).square()) + torch.sum((img[<span class="number">0</span>,:,:,<span class="number">1</span>:] - img[<span class="number">0</span>,:,:,:<span class="number">-1</span>]).square()))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Over-ALL"><a href="#Over-ALL" class="headerlink" title="Over ALL"></a>Over ALL</h3><p>完成的上面的部分后就是整合了，note book中的代码已经给出。不再赘述。值得注意的是，使用同样的代码，我们还可以完成<strong>Feature Inversion</strong>和<strong>texture synthesis</strong>这两个任务。对于<strong>texture synthesis</strong>，只需将content loss置零即可。对于<strong>Feature Inversion</strong>只需将style loss置零即可。下面是一些自己测试的结果。</p><div class="fig figcenter fighighlight">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/MojOJTfCVnYA46L.png" width="30%">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/BrTLcfMZoCpSU98.png" width="20%" style="border-left: 1px solid black;">  <div class="figcaption">Style Transfer Left: Style img and Input img. Right: result(200 iteration).</div></div><div class="fig figcenter fighighlight">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/yLZAnhrJ5VItsux.png" width="30%">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/enD34UK7J1oqut5.png" width="20%" style="border-left: 1px solid black;">  <div class="figcaption">texture synthesis Left: Style img. Right: result(200 iteration).</div></div><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>本部分的GAN实现了报过最原始的2014年 GoodFellow的原始GAN，以及Last Square GAN（优化了损失函数）和Deep Convolutional GANs。整体来讲使用pytorch建立模型和训练都比较简单，note book中主要实现的就是discriminator和generator的loss，将这个实现好即可。这部分的难度不是很高，就不再列出了，具体代码和结果可以参考我的github仓库。</p><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>至此，整个CS231n的课程就结束啦！为期了一个多月，中间因为各种事前耽误了几天，本来计划1个月内就搞定的。感觉CS231n课程整体来讲的难度适中，在数学推倒部分设计的稍微少了一些，但可以建立很多intuition的东西，总之还是收获颇丰的。</p><p>最大的收获就是他的assignment了，有一定难度的同时也极大的加深了对于这些知识的理解。</p><p>TODO：</p><ul><li>下一步可能会写一个GAN相关的小总结。</li><li>看一些more mathematic的东西</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment3-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment3-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment3 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment2 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-12T09:16:42.000Z</published>
    <updated>2020-08-26T14:09:37.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment2-遇到的问题"><a href="#CS231n-Assignment2-遇到的问题" class="headerlink" title="CS231n Assignment2 遇到的问题"></a>CS231n Assignment2 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Fully-connected-Neural-Network"><a href="#Fully-connected-Neural-Network" class="headerlink" title="Fully-connected Neural Network"></a>Fully-connected Neural Network</h2><p>相比于Assignment1，对于整个网络的实现进行了进一步的封装，可以实现任意深度，大小的MLP。</p><div class="note info">            <p>值得一看的代码部分！solver.py中实现调用更新规则函数（在optim.py中实现），实现的很有趣！</p>          </div><p>基本思路为：使用getattr获取定义在optim.py中定义好的update rule函数！我是第一次见这种写法，感觉很巧妙，值得学习一波:)</p><p>当然，要先import optim。</p><p>Core Code(extract):</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.update_rule = kwargs.pop(<span class="string">&#x27;update_rule&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>)</span><br><span class="line">...</span><br><span class="line">   <span class="comment"># Make sure the update rule exists, then replace the string</span></span><br><span class="line">   <span class="comment"># name with the actual function</span></span><br><span class="line">   <span class="keyword">if</span> <span class="keyword">not</span> hasattr(optim, self.update_rule):</span><br><span class="line">       <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)</span><br><span class="line">self.update_rule = getattr(optim, self.update_rule)</span><br></pre></td></tr></table></figure><p>其余部分的实现（如：affine，ReLU的forward和backward；优化算法）注意好细节后都比较容易实现，因为比较复杂的代码框架已经提供好了。</p><p>值得注意的是，官方github仓库中的关于课程内容的markdown笔记值得一读。<a href="http://github.com/cs231n/cs231n.github.io" title="官方课程github仓库">课程github仓库</a>. </p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>本部分的实现我遇到了一些问题，在完成上也花费了很多时间，主要遇到的问题还是导数的求取，以及如何将其变化为numpy数组的形式。</p><h3 id="The-Gradient-of-Batch-Normalization"><a href="#The-Gradient-of-Batch-Normalization" class="headerlink" title="The Gradient of Batch Normalization"></a>The Gradient of Batch Normalization</h3><p>由于本部分想知道自己的代码是正确与否需要变为代码后，进行numerical check才能验证。所以，在第一次求导后，我花了很长时间debug，但最后发现是导数求错了。。。</p><p>求导中还是遇到了不少的问题，尤其是在使用链式法则的时候遇到了问题。看来是好久没有好好推公式了QuQ，于是重新复习了一下chain rule和矩阵求导之类的；并重新推到了一下公式。NOTE：以下推导可能并不非常严谨（部分可能不符合矩阵相乘维数），但作为示意和实现代码足够了。</p><p>首先是正向（forward pass）的公式。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/heQRF4Um6j7pOxk.png" alt="image-20200819013718586"></p><p>然后是backward计算梯度。这里在计算关于x的偏导时，我遇到了一些问题，很容易丢掉一个导数项；画出变量之间的关系图可以很好地解决这个问题。推导如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ovXSVBY5bPkcyA3.png" alt="image-20200819013948134.png"></p><p>有了这些部分，就可以实现第一个函数batchnorm_backward()和batchnorm_forward()了！</p><p>实际上，上式还可以继续化简，化简的结果更加简洁，省去很多中间变量。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/eudk5aMOFLJCApj.png" alt="image-20200819014219098"></p><p>至此，batch normalization的公式推导部分就OK了，下面就是代码实现了。</p><p>这里需要注意的是，由于在predict的时候，一般是没有batch数据的，所以此时没法直接使用batch normalization，所以一种方法就是利用训练时得到的均值和方差来作为predict时的均值和方差。其更新方法使用momentum更新为：</p><pre><code>running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var</code></pre><h3 id="Code-Implement-of-Batch-Normalization"><a href="#Code-Implement-of-Batch-Normalization" class="headerlink" title="Code Implement of Batch Normalization"></a>Code Implement of Batch Normalization</h3><p>forward没啥可说的，很easy，分开train与test即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span>(<span class="params">x, gamma, beta, bn_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: &#x27;train&#x27; or &#x27;test&#x27;; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mode = bn_param[<span class="string">&#x27;mode&#x27;</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        sample_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line"></span><br><span class="line">        x_norm = (x - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">        cache = x, x_norm, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        x_norm = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid forward batchnorm mode &quot;%s&quot;&#x27;</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward实现两个版本，我是一个根据未化简公式来计算，另一个是根据化简后公式来计算，对比后明显可以看到化简后大量减少了运算次数，可以达到原来速度的3倍。</p><p>未化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_norm.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_norm * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx_norm = dout * gamma</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - sample_mean), axis=<span class="number">0</span>) * np.power(sample_var + eps, <span class="number">-1.5</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(sample_var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">-1</span>/N) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=<span class="number">0</span>) + dL_dvar * np.sum(<span class="number">-2</span>*(x - sample_mean)/N, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / N) * (x - sample_mean) * dL_dvar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><p>化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dx = (<span class="number">1</span> / N) * mid * (N * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>LN按照如下公式来输出，实际上就是把BN倒过来。。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ugYP7bKtTAvhzOj.png" alt="image-20200819015437962"></p><p>LN的操作类似于将BN做了一个“<strong>转置</strong>”，对同一层网络的输出做一个标准化。注意，同一层的输出是单个图片的输出，比如对于一个batch为32的神经网络训练，会有32个均值和方差被得出，<strong>每个均值和方差都是由单个图片的所有channel之间做一个标准化</strong>。这么操作，就使得LN不受batch size的影响。</p><p>在代码的实现上只需将所有的相关矩阵装置一下就OK啦，即对于转置过来的输入x做BN即可！注意要保证输出的维数正确。</p><p>Layer Normalization Forward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_forward</span>(<span class="params">x, gamma, beta, ln_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During both training and test-time, the incoming data is normalized per data-point,</span></span><br><span class="line"><span class="string">    before being scaled by gamma and beta parameters identical to that of batch normalization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note that in contrast to batch normalization, the behavior during train and test-time for</span></span><br><span class="line"><span class="string">    layer normalization are identical, and we do not need to keep track of running averages</span></span><br><span class="line"><span class="string">    of any sort.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - ln_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">        - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    sample_var = np.var(x.T, axis=<span class="number">0</span>)</span><br><span class="line">    x_norm = (x.T - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">    out = gamma * x_norm.T + beta</span><br><span class="line">    cache = x, x_norm.T, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>Layer Normalization Backward：这里我实现了两个版本，分别是基于化简后的公式和未化简后的公式，均通过测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you can heavily rely on the work you&#x27;ve done already</span></span><br><span class="line"><span class="string">    for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from layernorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line"></span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dxhat = dxhat.T</span><br><span class="line">    x_hat = x_hat.T</span><br><span class="line">    dx = (<span class="number">1</span> / D) * mid * (D * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line">    dx = dx.T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment">#    Another vision of  LN backward (based on the origin vision of bn backward)     #</span></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment"># x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span></span><br><span class="line">    <span class="comment"># N, D = x_norm.shape</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dbeta = np.sum(dout, axis=0)</span></span><br><span class="line">    <span class="comment"># dgamma = np.sum(x_norm * dout, axis=0)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># x = x.T</span></span><br><span class="line">    <span class="comment"># dout = dout.T</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dx_norm = dout.T * gamma</span></span><br><span class="line">    <span class="comment"># dx_norm = dx_norm.T</span></span><br><span class="line">    <span class="comment"># dL_dvar = -0.5 * np.sum(dx_norm * (x - sample_mean), axis=0) * np.power(sample_var + eps, -1.5)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx = dx_norm / np.sqrt(sample_var + eps)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (-1/D) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=0) + dL_dvar * np.sum(-2*(x - sample_mean)/N, axis=0)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;var--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (2 / D) * (x - sample_mean) * dL_dvar</span></span><br><span class="line">    <span class="comment"># dx = dx.T</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="BN-vs-LN"><a href="#BN-vs-LN" class="headerlink" title="BN vs LN"></a>BN vs LN</h3><h4 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h4><p>首先是BN，BN是通过mini-batch来对相应的activation做规范化操作，使得输出的各个维度的均值为0，方差为1（标准化）。而最后的“scale and shift”，即加入一个放射变换，则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入，同时也缓解因为数据可能会因此丢失了一些信息，所以再加上beta和gama来恢复原始数据，这里beta和gama是可学习的。</p><p><strong>BN的好处：</strong></p><p>(1) 减轻了对参数、权重初始化的依赖。</p><p>(2) 训练更快，可以使用更高的学习率。</p><p>(3) BN一定程度上增加了泛化能力。</p><p><strong>BN的缺点：</strong></p><p>batch normalization依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。会引入很多噪声误差，若网络队伍差很敏感，则会难以训练和收敛。</p><p>这一个特性，导致batch normalization不适合以下的几种场景。</p><p>(1)batch非常小，比如训练资源有限无法应用较大的batch。</p><p>(2)RNN，因为它是一个动态的网络结构，即输入的size是不固定的，同一个batch中训练实例有长有短，无法根据BN的公式进行标准化。</p><p>关于Normalization的<strong>有效的原因</strong>：</p><p>Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，目前大部分资料都这样解释，比如BN的原始论文认为的缓解了Internal Covariate Shift(ICS)问题。加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。</p><h4 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h4><p>BN 的一个缺点是需要较大的 batchsize 才能合理估训练数据的均值和方差，这在计算资源比较有限的时候往往不能达到，同时它也很难应用在数据长度不同的 RNN 模型上。Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化，他是针对于per datapoint的更新。</p><p>整体而言，LN用于RNN效果比较明显，但是在CNN上，不如BN。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout的代码部分非常简单，material中已经给出了代码实现，只需要实现一下forward和backw以及更新一下计算loss的函数即可。需要注意的是增加loss的部分，这里我使用caches当做<strong>堆栈</strong>存储前向计算loss时产生的caches，这样反向传播时只需要依次pop并根据网络结构计算梯度即可。本部分代码位于：<code>cs231n/classifiers/fc_net.py</code>。 该loss位于为FullyConnectedNet类内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute loss and gradient for the fully-connected net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / output: Same as TwoLayerNet above.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X = X.astype(self.dtype)</span><br><span class="line">    mode = <span class="string">&#x27;test&#x27;</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">&#x27;train&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">    <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">    <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">        self.dropout_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    <span class="keyword">if</span> self.normalization==<span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">            bn_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    scores = X</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">        W = self.params[<span class="string">&#x27;W&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        b = self.params[<span class="string">&#x27;b&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            scores, cache = affine_forward(scores, W, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                scores, cache = affine_relu_forward(scores, W, b)</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;batchnorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_bn_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;layernorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_ln_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cache = <span class="literal">None</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout <span class="keyword">and</span> i != self.num_layers<span class="number">-1</span>:</span><br><span class="line">            scores, cache = dropout_forward(scores, self.dropout_param)</span><br><span class="line">            caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If test mode return early</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    reg = self.reg</span><br><span class="line">    loss, dx = softmax_loss(scores, y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(self.num_layers)):</span><br><span class="line">        w = <span class="string">&#x27;W&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        b = <span class="string">&#x27;b&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        gamma = <span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        beta = <span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        loss += <span class="number">0.5</span> * reg * np.sum(W * W)  <span class="comment"># add reg term</span></span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            dx, grads[w], grads[b] = affine_backward(dx, caches.pop())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">                dx = dropout_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                dx, grads[w], grads[b] = affine_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_bn_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;layernorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_ln_relu_backward(dx, caches.pop())</span><br><span class="line">        grads[w] += reg * self.params[w]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><h2 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h2><p>这部分就是实现CNN了！核心就是实现好卷积层和pooling层。同时也修改batch normalization以便适用于CNN，同时增加group normalization。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>由于在实现时并不需要考虑计算复杂度和时间复杂度，我使用了最简单直接的方法，在forward时，同官方给的note一样，每次更新一个激活神经元的值，即使用4层循环嵌套，直观的实现卷积的过程。TODO：向量化方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span>(<span class="params">x, w, b, conv_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - &#x27;pad&#x27;: The number of pixels that will be used to zero-pad the input. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, &#x27;pad&#x27; zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) / stride)</span><br><span class="line">    out = np.zeros((N, F, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.sum(</span><br><span class="line">                        x_pad[n, :, h * stride:h * stride + HH, w_mid * stride:w_mid * stride + WW] * w[f, :, :, :]) + b[f]</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>在实现backward时，我也写出了简单情况下更新的公式，并根据这个最简单的展开形式以此来反向求梯度。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/26/7R2qvGbEiOLPgZw.png" alt="image-20200826005100903"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, F, H_out, W_out = dout.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    dx_pad = np.zeros_like(x_pad)</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.sum(dout, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW]</span><br><span class="line">                    dx_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW] += \</span><br><span class="line">                        dout[n, f, h_mid, w_mid] * w[f, :, :, :]</span><br><span class="line">                    dw[f, :, :, :] += window * dout[n, f, h_mid, w_mid]</span><br><span class="line">    dx = dx_pad[:, :, pad:pad + H, pad:pad + W]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>forward很简单，只需取respect field中最大的即可；backward时，将取最大值的位置处的梯度直接回传，其余置一即可。比较简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span>(<span class="params">x, pool_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;pool_height&#x27;: The height of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;pool_width&#x27;: The width of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here. Output size is given by </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H - pool_height) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W - pool_width) / stride)</span><br><span class="line">    out = np.zeros((N, C, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.max(</span><br><span class="line">                        x[n, f, h * stride:h * stride + pool_height, w_mid * stride:w_mid * stride + pool_width])</span><br><span class="line"></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H_out, W_out = dout.shape</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                             stride * w_mid:stride * w_mid + pool_width]</span><br><span class="line">                    mask = window == np.max(window)</span><br><span class="line">                    dx[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                    stride * w_mid:stride * w_mid + pool_width] = mask * dout[n, f, h_mid, w_mid]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h3 id="Spatial-Batch-Normalization"><a href="#Spatial-Batch-Normalization" class="headerlink" title="Spatial Batch Normalization"></a>Spatial Batch Normalization</h3><p>实现起来非常简单，只需要重新reshape输入，之后使用之前实现过的正常的Batch Normalization就OK了，代码请看我的github仓库，这部分没有遇到问题。</p><h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><p>forward只需要稍微修改正常的Batch Normalization即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_forward</span>(<span class="params">x, gamma, beta, G, gn_param</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = gn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_new = x.reshape((N, G, C // G, H, W))</span><br><span class="line">    mean = np.mean(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var = np.var(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_norm = (x_new - mean) / np.sqrt(var + eps)</span><br><span class="line">    x_norm = x_norm.reshape((N, C, H, W))</span><br><span class="line">    gamma_new = gamma.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    beta_new = beta.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    out = gamma_new * x_norm + beta_new</span><br><span class="line">    cache = G, x, x_norm, mean, var, gamma, beta, eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward也并不复杂，本质上的求导与正常的batch normalization一致，不过在多个导数求和时，需要注意怎么进行sum。这里如果想要通过Gradient check也有一个小坑。。。 ps: 这里我调试了很久。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    G, x, x_norm, mean, var, gamma, beta, eps = cache</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout * x_norm, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.reshape(N, G, C // G, H, W)</span><br><span class="line">    <span class="comment"># 这里想通过Gradientcheck必须需要将其reshape为(1, C, 1, 1)</span></span><br><span class="line">    dbeta = np.sum(dout, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    dx_norm = (dout * gamma).reshape(N, G, C // G, H, W)</span><br><span class="line">    mean = mean.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    var = var.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - mean), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)) * np.power(var.squeeze() + eps, <span class="number">-1.5</span>)</span><br><span class="line">    dL_dvar = dL_dvar.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    mid = H * W * C // G</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += ((<span class="number">-1</span> / mid) * np.sum(dx_norm / np.sqrt(var + eps), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>) + dL_dvar * (</span><br><span class="line">        np.sum(<span class="number">-2</span> * (x - mean) / mid, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / mid) * (x - mean) * dL_dvar</span><br><span class="line">    dx = dx.reshape((N, C, H, W))</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h2 id="PyTorch-on-CIFAR-10"><a href="#PyTorch-on-CIFAR-10" class="headerlink" title="PyTorch on CIFAR-10"></a>PyTorch on CIFAR-10</h2><p>这部分比较简单，我在实现时没有遇到问题。偷了懒，没有实现最后的CIFAR-10 open-ended challenge。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment2-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment2-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment2 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>单片机解决方案调研</title>
    <link href="http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/"/>
    <id>http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/</id>
    <published>2020-08-12T02:35:36.000Z</published>
    <updated>2020-09-14T12:36:24.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单片机解决方案调研"><a href="#单片机解决方案调研" class="headerlink" title="单片机解决方案调研"></a>单片机解决方案调研</h1><p>目标：<strong>模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线通信；具有一定的算力</strong></p><p>本文主要讨论3种不同解决方案。Arduino，stm32和C51系列。列出的这三种都是我有过使用经历的。我经验较多的是stm32，没有用arduino做过比较大型的东西。</p><p>实际上三者并不能直接比较，Arduino算是一个硬件平台，他的早期，也是最广泛的核心是基于AVR单片机（这种芯片我没单独用过）。</p><p>后两者stm32与C51则是两种特定系列的单片机了。</p><table><thead><tr><th></th><th>Arduino</th><th>stm32</th><th>C51</th></tr></thead><tbody><tr><td>模块化</td><td>强（有很多各种各样现成的模块）</td><td>中（配合开发板使用，可以达到部分模块化的效果）</td><td>中（同stm32）</td></tr><tr><td>运算能力</td><td>中、高（一般使用AVR的算力差，现在有支持STM32系列的和esp32的了）</td><td>中、高</td><td>低</td></tr><tr><td>功耗</td><td>低、电池供电足够</td><td>低（极低）、电池供电足够</td><td>极低、电池供电足够</td></tr><tr><td>开发难度</td><td>低、开发速度快、代码易于迭代更新，不必考虑寄存器层面编程</td><td>较高（寄存器复杂，但ST提供封装的的库函数）</td><td>中（硬件资源少，编程难度高）</td></tr><tr><td>价格</td><td>较高</td><td>中</td><td>极低</td></tr><tr><td>支持无线通信</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>优点</td><td>开放周期较短，模块化强，代码移植性强，社区丰富</td><td>功能强大，增加功能灵活，社区丰富</td><td>极其便宜，功耗低</td></tr><tr><td>缺点</td><td>扩展模块可选有限；算力有限；</td><td>开发难度大；程序移植性差；</td><td>算力低下，框架老旧</td></tr><tr><td>总结&amp;建议</td><td><strong>可选方案</strong>。方便开发，算力比较OK；但价格较高。</td><td><strong>可选方案</strong>。芯片功能极其强大；但开发周期和难度可能较长。</td><td>不建议使用，如果要批量生成且算力要求不高，可以考虑</td></tr></tbody></table><h2 id="Arduino"><a href="#Arduino" class="headerlink" title="Arduino"></a>Arduino</h2><p>Arduino准确的说是一个单片机及其外设的集合，比较经典板子的主控是ATMEL出的AVR单片机，比51系列性能强一点。这个集合之所以出名在于其操作简单，不需要涉及很多底层、寄存器层面的编程。例如，stm32库函数的一大堆命令，在这里只需要一句即可完成功能，并且有相当丰富的外设模块。</p><p>总体而言做原型，快速开发的时候，硬件搭设方便，基本不用去设计电路板，画板子之类的，基本上导线连接模块就OK了。代码比较简单易懂的。基本不涉及到寄存器级的操作。总得来说就是开发快。小量定制化还是划算，做产品或者较多数量的成本很高；且由于其代码的高度封装会导致程序效率底下以及资源开销大。</p><p>我个人对于Arduino的使用不是很多，还需进一步调研。</p><p>该图为比较常见的Arduino型号的单片机的性能参数。<a href="https://www.arduino.cn/thread-42417-1-1.html">原文链接</a> 原文发布于2017年</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/aodKOxTjRWQztpb.jpg" alt="211543xjgg8attjataqkgk"></p><h2 id="STM32系列单片机"><a href="#STM32系列单片机" class="headerlink" title="STM32系列单片机"></a>STM32系列单片机</h2><p>stm32是st半导体公司向arm公司购买了核心(嵌入式)版权，加上自己的外设生产的一个系列的芯片。其特点是：功能强大、速度快、外设多。STM32比较常见的框架是ARM CORTEX-m3或m4。并且其：寄存器复杂，直接用汇编操作比较麻烦，但ST官方了提供封装的的库函数，现在还出了专门的代码生成软件cube来简化操作。</p><p>一个STM32常用型号之间对比：<a href="https://blog.csdn.net/ybhuangfugui/article/details/88266385">https://blog.csdn.net/ybhuangfugui/article/details/88266385</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/nkjBiR6fT2lSQb7.jpg" alt="en.microcontrollers_FM141.jpg"></p><p>上图为ST官网对于其系列芯片的简介与分类。</p><p>我对于STM32系列芯片的使用比较有经验，我使用的型号为主要为：STM32F1系列和F4系列。</p><p>STM32F407这款芯片，使我们机器人队使用的主控芯片，其最高主频可达<strong>168MHz</strong>（远远大于Arduino的常见型号的16MHz），可见其算力的强大。</p><p>我们队内并没有使用STM32系列的开发板，而是买了裸的芯片，之后自己设计了电路板（设计部分我不是很擅长），画板子、印板子、焊板子、改板子这样进行开发，导致开发一代新的主控板周期比较长。</p><p>不过市面上也有很多STM32现成的开发板，预留了很多IO接口，初步观察感觉基本满足需求，价格适中，如STM32F4系列的开发板不足100元。使用这种开发板一般也不需要自己设计电路，只需购买不同的模块即可交互使用。</p><center class="half">    <img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/7DGvqR42cTMiNVA.jpg width="400"/><img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/gTPjAvzL4KEnIFC.jpg width="400"/> </center><p>左图的为我近期购买的STM32F1系列的的开发板；右图为STM32F4系列的开发板。例如左边的F1开发板，可以看到这种开发板也像Arduino一样支持很多的扩展功能，而且只需要插接即可。</p><p>整体而言，使用STM32系列也绝对可以实现我们的预期目标，使用开发板也基本可以避免电路的设计等工作。由于STM32系列芯片本身功能强大，其上限应该是高于Arduino方案的。但其开发难度会比Arduino方案难上不少。这里指的STM32开发方案是指使用STM32 CUBEMX硬件配置和生成代码模板（HAL库），之后在代码模板上进行开发（一般使用IAR作为IDE）。</p><h2 id="Arduino-与-STM32"><a href="#Arduino-与-STM32" class="headerlink" title="Arduino 与 STM32"></a>Arduino 与 STM32</h2><p>通过进一步的了解，我发现了arduino支持了STM32的开发！即可以使用Arduino的IDE来编程。这样可能会降低部分开发难度。</p><p>github链接<a href="https://github.com/rogerclarkmelbourne/Arduino_STM32">https://github.com/rogerclarkmelbourne/Arduino_STM32</a>。目前支持STM32F4和F1系列，其可以将Stm32F103（主要）系列单片机刷入Arduino的Bootloader，并且使用Arduino的编译器和IDE来完成代码的编写，省去了一大部分配置寄存器和学习的时间，完整的性能和灵活性还有待探究。</p><p>同时也有一个类似于arduino+STM32的project，其链接如下。<a href="https://www.leaflabs.com/maple">https://www.leaflabs.com/maple</a>。该板子在淘宝有售，其芯片使stm32f103 arm cortex-M3 32位处理器，主频最高可达72MHz，远远大于常见的Arduino的8位（AVR）MCU。</p><p>Leaf Maple 是一个类似Arduino的开发平台，使用的Cotrex M3内核的32位MCU，所以要比Arduino的8位（AVR）MCU强悍很多，有更高的主频，更丰富的资源。 Leaf Maple也提供了一个类似Arduino IDE的IDE， 并且很多简单上层函数兼容Arduino的函数库，让移植库和代码变得相当简单。比起使用CubeMX+IAR来开发一个STM32项目，使用leaf maple会节省很多时间，更适合初学者和需要快速原型开发的用户。</p><h2 id="Arduino新品M5Stack（使用esp32）"><a href="#Arduino新品M5Stack（使用esp32）" class="headerlink" title="Arduino新品M5Stack（使用esp32）"></a>Arduino新品M5Stack（使用esp32）</h2><p>Arduino的生态总体来讲还是很好用的，目前了解到一款<strong>模块化极强</strong>，性能出色，上市时间不长的支持Arduino开发平台的开发板。M5系列。</p><p>M5Stack是一种模块化、可堆叠扩展的开发板，每个模块一般为5cmX5cm的尺寸，这也是M5Stack名字的由来。与常规的开发板不同，M5Stack更注重产品形态的完整性，更注重用户的应用场景和研发的简易性，M5没有密密麻麻的飞线，没有错乱无章的接口插头，不需要繁琐的开发流程，简简单单、轻轻松松地完成高质量的电子原型创作。（官方介绍）</p><p>M5Stack主要采用ESP32芯片体系，CORE主机内已集成了240M双核主频CPU（esp32）、 WiFi、蓝牙、2.0寸彩色屏幕LCD、扬声器、按键、TF卡、陀螺仪以及内置电池（部分有）。CORE基本满足一般的功能需求，功能模块Function Module则根据应用的情况选择，比如电机驱动、信号采集、通信等功能。另外，也会配备不同的应用底座及配件，方便用户做出高质量的研发。</p><p><a href="https://github.com/m5stack">官方github</a> 与 <a href="https://m5stack.taobao.com/index.htm?spm=2013.1.w5002-22404213498.2.2149622fvBd0zs">官方淘宝链接</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/oXGRIYD8r7OMFq6.jpg" alt="img"></p><p>比如其Basic款。其内含2.4G Wi-Fi和蓝牙4.0。外设接口有Type-c，I2C，GPIO和UART接口（数量较少）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/gRsKaQhdYloI5Ut.png" alt="image-20200827162008345"></p><h2 id="简易方案"><a href="#简易方案" class="headerlink" title="简易方案"></a>简易方案</h2><p>简易方案均假定使用2个加速度传感器，并且保证支持WIFI通信，均基本不需要大量焊接（基本做线之类的还是需要的）</p><table><thead><tr><th></th><th>Arduino(M5Stack)</th><th>STM32</th></tr></thead><tbody><tr><td>CORE MCU Unit</td><td><a href="https://item.taobao.com/item.htm?spm=a1z10.1-c-s.w5003-22404213505.1.582d7ef3sJFgr9&ft=t&id=610634829528&scene=taobao_shop">M5Stack Basic</a> 支持I2C总线，内置WiFi  200￥</td><td><a href="https://detail.tmall.com/item.htm?spm=a220o.1000855.0.0.45dd72b09Nb8ev&id=609293737870&skuId=4571066386420">STM32F103最小开发板 </a> 100￥</td></tr><tr><td>Senor</td><td><a href="https://item.taobao.com/item.htm?spm=a1z10.5-c-s.w4002-22404213529.21.23df38edwHEAwd&id=610411236397">ADXL345三轴加速度</a> * 2 (I2C总线 ±16g 13位分辨率) 30￥</td><td><a href="https://item.taobao.com/item.htm?id=45567315525&ali_refid=a3_430582_1006:1103191143:N:nfpYj0PVRKdxrBfSBLtPWA==:e32ca8378afe7d9c3b105a70f8d92779&ali_trackid=1_e32ca8378afe7d9c3b105a70f8d92779&spm=a230r.1.14.13#detail">MPU6050</a> 三轴加速度+三轴陀螺仪 I2C接口 25￥</td></tr><tr><td>WiFi</td><td></td><td><a href="https://detail.tmall.com/item.htm?id=609757779633&ali_refid=a3_430582_1006:1267360122:N:9/mfWI1BJMLzXLT4ATlUnA==:de4e276b258975c722c4a03cf64e8c17&ali_trackid=1_de4e276b258975c722c4a03cf64e8c17&spm=a230r.1.14.8">ATK-ESP8266</a> 串口转WIFI模块 28￥</td></tr><tr><td>Battery</td><td>内置110mAh 锂电池</td><td><a href="https://item.taobao.com/item.htm?spm=a230r.1.14.137.5c3d9640Qtfoh2&id=546584044959&ns=1&abbucket=18#detail">5V锂电池</a> 20￥</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;单片机解决方案调研&quot;&gt;&lt;a href=&quot;#单片机解决方案调研&quot; class=&quot;headerlink&quot; title=&quot;单片机解决方案调研&quot;&gt;&lt;/a&gt;单片机解决方案调研&lt;/h1&gt;&lt;p&gt;目标：&lt;strong&gt;模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="单片机" scheme="http://canVa4.github.io/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="arduino" scheme="http://canVa4.github.io/tags/arduino/"/>
    
      <category term="STM32" scheme="http://canVa4.github.io/tags/STM32/"/>
    
  </entry>
  
  <entry>
    <title>Python学习杂记(1)</title>
    <link href="http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/"/>
    <id>http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/</id>
    <published>2020-08-08T13:09:52.000Z</published>
    <updated>2020-08-15T13:03:30.917Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python学习杂记-1"><a href="#Python学习杂记-1" class="headerlink" title="Python学习杂记(1)"></a>Python学习杂记(1)</h1><p>本篇文章是为了记录进一步学习python时遇到的一些问题和相对陌生的知识点。</p><ul><li><p>range()函数返回的是一个独特的“范围类”对象！</p></li><li><p>for 变量 in 字符串|集合|范围|任何可迭代对象:</p><p> <strong>可迭代对象：指该对象中包含一个–iter–方法</strong></p></li><li><p><strong>isinstance()函数</strong>，判断某个变量是否为指定类型的实例，前一个参数是要判断的变量，后一个参数是类型。如：isinstance(2, int)</p></li><li><p>zip()函数，将多个列表压缩为一个zip对象（可迭代对象），这样就可以使用一个循环遍历两个列表</p></li><li><p>reversed()函数：接受序列（元组、列表、区间等），然后返回一个“反序排列”的迭代器，sorted()类似</p></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python学习杂记-1&quot;&gt;&lt;a href=&quot;#Python学习杂记-1&quot; class=&quot;headerlink&quot; title=&quot;Python学习杂记(1)&quot;&gt;&lt;/a&gt;Python学习杂记(1)&lt;/h1&gt;&lt;p&gt;本篇文章是为了记录进一步学习python时遇到的一些问题
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment1 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-07T08:21:14.000Z</published>
    <updated>2020-08-08T13:01:59.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment1-遇到的问题"><a href="#CS231n-Assignment1-遇到的问题" class="headerlink" title="CS231n Assignment1 遇到的问题"></a>CS231n Assignment1 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><h2 id="Softmax-implement"><a href="#Softmax-implement" class="headerlink" title="Softmax implement"></a>Softmax implement</h2><p>不论是实现softmax，SVM损失函数，二者遇到的问题都比较相似，主要为<strong>导数的推导</strong>和<strong>numpy的使用</strong>。由于softmax的实现稍微复杂一些，这里只记录softmax实现时的问题。</p><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>使用SGD核心的工作就是计算softmax关于权值W的梯度。课程中没有给出推导过程，这里推导一下。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/mg9rJC2LzVaAxO3.png" alt="image-20200807173340447"></p><h3 id="Numeric-Stability-Trick"><a href="#Numeric-Stability-Trick" class="headerlink" title="Numeric Stability Trick"></a>Numeric Stability Trick</h3><p>为了防止出现数值计算不稳定，要在计算损失函数式加入修正项（对Gradient无影响）。</p><p>原始为：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/w469bMc7skBYd52.png" alt="image-20200807174209667" style="zoom:50%;" /></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/t7zyhReGVp6QEi2.png" alt="image-20200807174255380" style="zoom: 80%;" /><h3 id="Implement-with-numpy"><a href="#Implement-with-numpy" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><h4 id="Navie-Version"><a href="#Navie-Version" class="headerlink" title="Navie Version"></a>Navie Version</h4><p>给出naive版本的代码。如何计算的示意图已在推导过程中给出。naive版本的代码基本按照推导的公式梳理下来即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)    <span class="comment"># 一个数值修正的技巧，防止出现数值不稳定的问题</span></span><br><span class="line">        scores = np.exp(scores)</span><br><span class="line"></span><br><span class="line">        sum_scores = np.sum(scores)        <span class="comment"># 可以简化写法，节省空间，懒得修改了</span></span><br><span class="line">        P = scores / sum_scores</span><br><span class="line">        L = -np.log(P)</span><br><span class="line"></span><br><span class="line">        loss += L[y[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):    <span class="comment"># 计算梯度，分类讨论</span></span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:, j] += (<span class="number">-1</span> + P[y[i]])*X[i].T</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dW[:, j] += P[j]*X[i].T</span><br><span class="line"></span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += reg * W</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h4 id="Vectorized-Version"><a href="#Vectorized-Version" class="headerlink" title="Vectorized Version"></a>Vectorized Version</h4><p>向量化版本。这里就有非常多的细节需要注意了。首先还是给出完整代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    scores = X @ W  <span class="comment"># ( N*C )</span></span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss += np.sum(L[np.arange(num_train), y])</span><br><span class="line"></span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)   <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算gradient</span></span><br><span class="line">    mid = np.zeros_like(P)  <span class="comment"># 生成一个和P一样的0矩阵</span></span><br><span class="line">    mid[np.arange(num_train), y] = <span class="number">1</span>  <span class="comment"># 对矩阵中Y所对应的部分加一个1，因为一会要构造出需要的梯度计算</span></span><br><span class="line">    dW = X.T @ (-mid + P)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>首先应该画图明白计算中各个量的关系，以及他们是怎么来的，这个很重要。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/Knh24oeJ53Nydqj.png" alt="image-20200807175314544"></p><p>第一处就是在计算Numeric Stability Trick时，要找到每一个输入向量的最大元素，这里注意需要保证keepdims=True。</p><p>其控制了返回数组的shape，这样返回的shape为(N,1)。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>同理在sum时，也要进行类似的处理，这样在归一化时才能work。</p><h4 id="An-Important-Trick"><a href="#An-Important-Trick" class="headerlink" title="An Important Trick!!!"></a>An Important Trick!!!</h4><p>在这里我遇到了不少的问题，主要是numpy使用的不熟练。。。:( 所以特此记录下来。</p><p><strong>L[np.arange(num_train), y] **与 **L[:,y]</strong> 的区别！</p><p>一开始的代码使用的是后者，因为目标就是获得所有行i中，列位置为y[i]的元素。所以想当然的使用了后者。但实际上，后者返回的是所有行x[i]中，x[i,y[j]]的元素！！！</p><p>示例如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/dKFBsGk3qzxbV5c.png" alt="image-20200807180256834"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/QbpEhW9DK63ex1F.png" alt="image-20200807180318182"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/2meKSxGVvcsb3zA.png" alt="image-20200807180331281"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/jJGhMKFWD7iTULy.png" alt="image-20200807180431683"></p><p>而**L[np.arange(num_train), y] **则为：</p><p>如果将np.arange(num_train)看为list，则其长度必须与y相同！！！其效果就是二者分别迭代，每次返回二者迭代结果下标位置处的元素。如图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/d7nQCOvX8Goue9z.png" alt="image-20200807180731248"></p><p>所以可见，基于我们的需要，后者才能满足要求。</p><h2 id="Two-Layer-Neural-Network"><a href="#Two-Layer-Neural-Network" class="headerlink" title="Two-Layer Neural Network"></a>Two-Layer Neural Network</h2><p>本部分的工作也与之前的部分比较相似，这里遇到主要问题还是如何处理求导的问题。</p><p>由于在这里我也遇到了一些问题，所以再次给出部分求导流程。</p><p>首先先给出网络的结构。</p><h3 id="Gradient-1"><a href="#Gradient-1" class="headerlink" title="Gradient"></a>Gradient</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/8yBxLVldSgqsrmZ.png" alt="image-20200807221946519"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/pb8PvAu7RjhNJHT.png" alt="image-20200807223402245"></p><h3 id="Implement-with-numpy-1"><a href="#Implement-with-numpy-1" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><p>下面给出代码实现。由于主要难点就是loss的实现了，之后的SGD和predict函数都非常简单，我没有遇到什么问题，这里只给出遇到了部分问题的loss与grad计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None, reg=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">    W1, b1 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2, b2 = self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">    N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    h = np.maximum(X @ W1 + b1, <span class="number">0</span>)</span><br><span class="line">    scores = h @ W2 + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we&#x27;re done</span></span><br><span class="line">    <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss = np.sum(L[np.arange(N), y])</span><br><span class="line"></span><br><span class="line">    loss /= N</span><br><span class="line">    loss += <span class="number">1</span> * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算W2，b2</span></span><br><span class="line">    dscore = P</span><br><span class="line">    dscore[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">    dscore /= N        <span class="comment"># 这里需要注意！！！</span></span><br><span class="line">    <span class="comment"># 计算梯度时只需要除一次N，这里debug花了很久。。</span></span><br><span class="line">    grads[<span class="string">&#x27;W2&#x27;</span>] = h.T @ dscore + <span class="number">2</span> * reg * W2</span><br><span class="line">    grads[<span class="string">&#x27;b2&#x27;</span>] = np.sum(dscore, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 计算W1，b1</span></span><br><span class="line">    dh = dscore @ W2.T      <span class="comment"># 目标函数对于h的偏导</span></span><br><span class="line">    dh[h &lt;= <span class="number">0</span>] = <span class="number">0</span>          <span class="comment"># 此时dh变为关于w1@x+b1的导数</span></span><br><span class="line">    grads[<span class="string">&#x27;W1&#x27;</span>] = X.T @ dh + <span class="number">2</span> * reg * W1</span><br><span class="line">    grads[<span class="string">&#x27;b1&#x27;</span>] = np.sum(dh, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p>基本上按照公式并注意矩阵维数和细节就OK了，遇到不太会的画个图就解决了。</p><div class="note warning">            <p>需要注意的是，在除以输入个数的时候，只需要除一次</p>          </div><p>这里一开始没有注意到，我一开始在每次计算梯度的时候都除了N，导致出现了误差，这里居然debug了很久。。</p><h3 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h3><p>有点懒，这部分工作没有完成。</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>其余的部分（k-Nearest Neighbor classifier, SVM, Higher Level Representations: Image Features）并未遇到很多的问题。具体详情代码见我的github仓库。<a href="https://github.com/canVa4/CS231n-Assignments">https://github.com/canVa4/CS231n-Assignments</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment1-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment1-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment1 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR论文复现</title>
    <link href="http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    <id>http://canva4.github.io/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-08-05T16:31:51.000Z</published>
    <updated>2020-08-08T03:30:00.768Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p>整体的代码使用pytorch实现，基于<a href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）</p><a id="more"></a><p>训练为：<em>run.py</em>；修改训练参数、Loss、数据集等需要修改：<em>config.yaml</em> ；评估使用<em>evluation.py</em>（测试训练分开的原因是因为我租了GPU，用GPU训练，用我的PC测试，这样可以更快一些）</p><p>个人运行环境：win10 + pytorch 1.5 + cuda 10.2（租的GPU 1080ti）</p><table><thead><tr><th>日期</th><th>进度</th></tr></thead><tbody><tr><td>5-19 Tue（基本满课+实验）</td><td>论文阅读，选定使用pytorch实现和决定基于上文链接实现代码</td></tr><tr><td>5-20 Wed</td><td>熟悉基础知识、了解代码整体框架，理解loss function，并进行初步尝试编写loss，未调试</td></tr><tr><td>5-21 Thu（满课+实验）</td><td>写完了evaluation部分</td></tr><tr><td>5-22 Fri（基本满课）</td><td>跑代码，发现只用CPU究极龟速；于是装cuda，结果装了一白天的cuda T.T，晚上测试代码并初步验证loss function是否书写正确；初步移植LARS</td></tr><tr><td>5-23 Sat</td><td>测试三个Loss并尝试调参，尝试使用resnet18作为backbone网络，旁晚开始租了个GPU来跑模型，实现triplet loss(sh)</td></tr><tr><td>5-24 Sun</td><td>调参、修复bug、跑代码、微调loss（Logistic loss增加欠采样和改变权重）</td></tr><tr><td>5-25 Mon</td><td>调参、跑代码</td></tr></tbody></table><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Linear evaluation均使用Logistic Regression，均train from scratch（no pretrain）</p><p>GPU: 1080ti    resnet50训练+测试一次需5.5h；resnet18训练+测试一次需2.6h；总代码运行时间：约75h（包括未列出测试）</p><table><thead><tr><th>batch</th><th>epoch</th><th>out dim</th><th>optimizer</th><th>Loss</th><th>BackBone</th><th>t/m</th><th>CIARF10 Top-1</th></tr></thead><tbody><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>0.1</td><td>78.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>79.3%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>1</td><td>77.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.4</td><td>65.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.8</td><td>70.7%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss(sh)</td><td>resnet50</td><td>0.8</td><td>73.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(none)</td><td>resnet50</td><td>0.2</td><td>37.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.2</td><td>62.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.5</td><td>69.9%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>1</td><td>65.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>LARS</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>TODO</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet18</td><td>0.5</td><td>71.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(weight)</td><td>resnet18</td><td>0.2</td><td>66.5%</td></tr></tbody></table><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于每一个输入图片，模型会生成两个representation，最终优化的目标可以理解为：同一个batch内来自同一张图片的两个representation的距离近，让来自不同输入图片的representation的距离远。注意，论文中给出的是negative loss function</p><h3 id="Logistic-Loss"><a href="#Logistic-Loss" class="headerlink" title="Logistic Loss"></a>Logistic Loss</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ log \sigma(u^Tv^+/\tau) + log\sigma(-u^T v^-/ \tau) $$</li></ul><p>这里对于此公式，我一开始是没有理解的，于是自己尝试推理了一下。</p><p>对于每一个输入样本，模型会生成两个representation，对于一个有N个输入的batch的，就会产生2*N个representation，对于每一对representation计算一个cosine similarity。而每一对representation（下文用 $(i,j)$ 序偶来表示他们）可以根据他们的来源来确定他们label（即：来自同一输入的为正类，来自不同输入的为反类），这样就构成了一个监督任务。</p><p>将这个任务看为监督后，因为论文中提到的这个损失函数的名字是logistic loss，我自然地想到了logistics regression。于是从这个角度入手，来推理这个loss function。</p><p>用$ P(i,j) $表示一对representation为正类的概率。设正类y=1，反类y=0</p><p>那么写出整个数据集的对数似然函数$$ LL(\theta;X)=\sum_{each(i,j)} (y_{(i,j)} logP(i,j)+(1-y_{(i,j)})log(1-P(i,j)) )$$</p><p>对上式化简可以得到：$$ LL(\theta;X)=\sum_{正类} logP(i,j)+\sum_{反类}log(1-P(i,j)) $$</p><p>而cosine similarity并不是一个[0,1]之间的数（或者说没有概率的意义），参照logistics regression，将cosine similarity经过一个sigmoid函数$$ \sigma( \cdot) $$ 之后就变为了一个[0,1之间的数]，而且对于sigmoid有$$ \sigma(-x)=1-\sigma(x) $$,所以有：$$ LL(\theta;X)=\sum_{正类} log[\sigma(sim(i,j))]+\sum_{反类}log[\sigma(-sim(i,j))] , sim(i,j)为(i,j)的相似度指标$$</p><p>只需引入temperature就可将上式变为与论文中公式相同的形式。</p><p>在使用原版loss时，发现最终结果效果很差（见result中的NT-Logistics none）。个人猜测原因如下：</p><ul><li>样本非常不均衡，正例对远远少于反例。</li></ul><p>解决办法：</p><ul><li>对反例样本对使用简单的<em>under-sampling</em>（欠采样）</li><li>对于loss计算时，正反例样本<em>设置不同的权重</em>（效果更好，因为欠采样会丢失部分信息）</li></ul><p>（注：由于训练时间太久，没有来得多次跑weight测试效果）</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line">    representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line">    l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line">    r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line">    positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line">    negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>) * <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    logits_pos = self.sigmoid(positives / self.temperature).log_()</span><br><span class="line">    logits_neg = self.sigmoid(negatives / self.temperature).log_()</span><br><span class="line">    <span class="keyword">if</span> self.method == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># under-sampling</span></span><br><span class="line">        all_one_vec = np.ones((<span class="number">1</span>, <span class="number">2</span> * self.batch_size,))</span><br><span class="line">        all_zero_vec = np.zeros((<span class="number">1</span>, <span class="number">2</span> * self.batch_size * (<span class="number">2</span> * self.batch_size - <span class="number">3</span>)))</span><br><span class="line">        under_sampling_matrix = np.column_stack((all_one_vec, all_zero_vec)).flatten()</span><br><span class="line">        np.random.shuffle(under_sampling_matrix)</span><br><span class="line">        under_sampling_matrix = torch.tensor(under_sampling_matrix).view(</span><br><span class="line">            (<span class="number">2</span> * self.batch_size, <span class="number">2</span> * self.batch_size - <span class="number">2</span>)).type(torch.bool).to(self.device)</span><br><span class="line"></span><br><span class="line">        logits_neg = logits_neg[under_sampling_matrix]</span><br><span class="line">        loss = torch.sum(logits_pos) + torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    <span class="keyword">elif</span> self.method == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># change weight</span></span><br><span class="line">        neg_count = <span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>)</span><br><span class="line">        pos_count = <span class="number">2</span>*self.batch_size</span><br><span class="line">        loss = neg_count * torch.sum(logits_pos) + pos_count*torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss/(pos_count+neg_count)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># none</span></span><br><span class="line">        total_logits = torch.cat((logits_pos, logits_neg), dim=<span class="number">1</span>)</span><br><span class="line">        loss = torch.sum(total_logits)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Margin-Triplet"><a href="#Margin-Triplet" class="headerlink" title="Margin Triplet"></a>Margin Triplet</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ -max(u^Tv^–u^Tv^+m,0)$$</li></ul><p>此公式理解起来相对直观，即对于一个输入样本，计算其和一个负样本相似度减去和正样本的相似度在加上m，并与0取max。该m可以理解：m越大为希望正反样本分开的距离越大。其目标是希望输入样本和正样本的相似度减去和负样本的相似度可以大于阈值m值。下图很形象的描述了这些关系。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/08/NwCTG5rc3J4D7R9.png" alt="triplet1"></p><p>所以，对于每一个输入样本k，该样本的<strong>margin tripl loss</strong>为$$ \sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^+m,0) $$</p><p>所以总的loss就是将所有输入样本的loss加起来</p><ul><li><p>$$ \frac{1}{2N*(2N-2)}\sum_{k}^{所有样本}\sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^++m,0) $$</p></li><li><p>同时也实现了semi-hard negative mining. 即计算loss（梯度）时，只考虑上图中semi-hard negatives的loss。即选择满足：$$ u^Tv^++m&gt;u^Tv^-$$</p></li></ul><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line"> representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"> similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"> <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line"> l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line"> r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line"> positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"> mid = similarity_matrix[self.mask_samples_from_same_repr]</span><br><span class="line"> negatives = mid.view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</span><br><span class="line"> zero = torch.zeros(<span class="number">1</span>).to(self.device)</span><br><span class="line"> triplet_matrix = torch.max(zero, negatives - positives + self.m_param)</span><br><span class="line"> <span class="comment"># max( sim(neg) - sim(pos) + m, 0)</span></span><br><span class="line"> <span class="comment"># 2N,2N-2 每一行代表了对于一个z关于其正类（z+batch）和其他反类的triplet loss</span></span><br><span class="line"> <span class="keyword">if</span> self.semi_hard == <span class="literal">True</span>:</span><br><span class="line">     <span class="comment"># semi-hard</span></span><br><span class="line">     semi_hard = - negatives + positives + self.m_param</span><br><span class="line">     <span class="comment"># print(semi_hard)</span></span><br><span class="line">     semi_hard_mask = torch.max(semi_hard, zero).type(torch.bool)</span><br><span class="line">     <span class="comment"># print(semi_hard_mask)</span></span><br><span class="line">     triplet_matrix_sh = triplet_matrix[semi_hard_mask]</span><br><span class="line">     <span class="comment"># print(triplet_matrix)</span></span><br><span class="line">     <span class="comment"># print(triplet_matrix_sh)</span></span><br><span class="line">     loss = torch.sum(triplet_matrix_sh)</span><br><span class="line">     <span class="keyword">return</span> loss</span><br><span class="line"> <span class="keyword">else</span>:    <span class="comment"># normal</span></span><br><span class="line">     loss = torch.sum(triplet_matrix)     </span><br><span class="line">     <span class="keyword">return</span> loss / (<span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="NT-Xent"><a href="#NT-Xent" class="headerlink" title="NT-Xent"></a>NT-Xent</h3><p>论文中的形式：</p><ul><li>$$l(i,j)=-log \frac{exp(s_{i,j}/\tau)}{\sum^{2N}<em>{k=1}1</em>{k\not=i}exp(s_{i,j}/\tau)}$$  </li><li>$$ L = \frac{1}{2N} \sum^{N}_{k=1}[l(2k-1,2k)+l(2k,2k-1)]$$</li></ul><p>代码实现未进行修改。</p><h2 id="simCLR模型"><a href="#simCLR模型" class="headerlink" title="simCLR模型"></a>simCLR模型</h2><p>主要使用ResNet-50来实现，参照论文B.9中所写：将Resnet第一个卷积层改为了3*3的Conv，stride=1，并去除第一个max pooling层；在augmentation中去除了Guassian Blur。</p><p>projection head同论文中一样，使用两层的MLP。</p><h2 id="遇到的问题与解决方法"><a href="#遇到的问题与解决方法" class="headerlink" title="遇到的问题与解决方法"></a>遇到的问题与解决方法</h2><p>Q1：使用个人笔记本训练，显存不足，使用cpu训练耗时过久。</p><p>A1：尝试使用过resnet18，仍时间仍很长，最终决定租GPU（1080ti）来训练。</p><p>Q2：训练时发现最终测试结果不好。</p><p>A2：最终控制变量，与未修改的代码对比测试，发现个人版本在sampler的时候不小心去掉了很多的训练样例，已修复为同原版。修复后，基本同原版效果</p><p>Q3：使用LARS效果不好，loss不能稳定下降，震荡严重。（unsolve）</p><p>A3：尝试修改debug，修改学习率，由于时间不足，暂未解决。</p><h2 id="关于Loss的个人想法"><a href="#关于Loss的个人想法" class="headerlink" title="关于Loss的个人想法"></a>关于Loss的个人想法</h2><p>从测试结果和论文结果可以看出，NT-xent的效果更佳。个人认为其主要的优势在于：</p><ul><li>NT-xent（cross entropy）利用的是相对相似度，而其余二者不是。这样可以缓解个别样本差异过大导致的不均衡（感觉类似于input的normalization）。</li><li>NT-xent计算了所有positive pair的loss。而NT-logistic和Margin Triplet则使用全部的pair来计算，不使用semi-hard mining的话，可能会造成坍塌。对于此模型生成的样本，可以看到其样本类别并不均衡，对于NT-logistic，这可能会导致训练效果下降。（使用semi-hard negative mining、采样、改变权重可以缓解这个问题）</li></ul><p>经过自己的implement之后，实在是羡慕google的TPU集群了！</p><p>这是我第一次真正接触self-supervised learning，之前只是有所耳闻，感觉这种contrastive learning的想法真的很有趣。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在开头&quot;&gt;&lt;a href=&quot;#写在开头&quot; class=&quot;headerlink&quot; title=&quot;写在开头&quot;&gt;&lt;/a&gt;写在开头&lt;/h2&gt;&lt;p&gt;整体的代码使用pytorch实现，基于&lt;a href=&quot;https://github.com/sthalles/SimCLR&quot;&gt;https://github.com/sthalles/SimCLR&lt;/a&gt; （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Papers/"/>
    
    
      <category term="论文复现" scheme="http://canVa4.github.io/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>First test Blog</title>
    <link href="http://canva4.github.io/2020/08/05/First-test-Blog/"/>
    <id>http://canva4.github.io/2020/08/05/First-test-Blog/</id>
    <published>2020-08-05T15:54:29.000Z</published>
    <updated>2020-08-05T15:57:36.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一个测试博客！！！"><a href="#第一个测试博客！！！" class="headerlink" title="第一个测试博客！！！"></a>第一个测试博客！！！</h1><p>语无伦次语无伦次语无伦次</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一个测试博客！！！&quot;&gt;&lt;a href=&quot;#第一个测试博客！！！&quot; class=&quot;headerlink&quot; title=&quot;第一个测试博客！！！&quot;&gt;&lt;/a&gt;第一个测试博客！！！&lt;/h1&gt;&lt;p&gt;语无伦次语无伦次语无伦次&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://canva4.github.io/2020/08/05/hello-world/"/>
    <id>http://canva4.github.io/2020/08/05/hello-world/</id>
    <published>2020-08-05T15:41:38.862Z</published>
    <updated>2020-08-05T15:41:38.862Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
