<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://canva4.github.io/"/>
  <updated>2021-04-22T03:24:43.607Z</updated>
  <id>http://canva4.github.io/</id>
  
  <author>
    <name>阿翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neural ODE</title>
    <link href="http://canva4.github.io/2021/04/17/Neural-ODE/"/>
    <id>http://canva4.github.io/2021/04/17/Neural-ODE/</id>
    <published>2021-04-17T01:41:36.000Z</published>
    <updated>2021-04-22T03:24:43.607Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><ol><li>DeepXDE: A deep learning library for solving differential equations 2019 <a href="https://arxiv.org/abs/1907.04502?context=stat">LINK</a></li><li><strong>Neural ODE</strong> 2018 <a href="https://arxiv.org/abs/1806.07366">LINK</a></li><li>Augmented Neural ODEs 2019 <a href="https://arxiv.org/abs/1904.01681">LINK</a> <a href="https://github.com/EmilienDupont/augmented-neural-odes">Github</a></li><li>Dynamically Constrained Motion Planning Networks for Non-Holonomic Robots 2020 <a href="https://arxiv.org/abs/2008.05112">LINK</a></li><li>Normalizing Flows for Probabilistic Modeling and Inference 2019 <a href="https://arxiv.org/pdf/1912.02762.pdf">LINK</a> <a href="https://www.zhihu.com/question/376122890/answer/1399139778">ZHIHU_LINK</a></li><li>Deep learning theory review: An optimal control and dynamical systems perspective 2019</li></ol><p>Resource:</p><ol><li>Neural ODE Code <a href="https://nbviewer.jupyter.org/github/urtrial/neural_ode/tree/master/">https://nbviewer.jupyter.org/github/urtrial/neural_ode/tree/master/</a></li><li>Neural ODE Code <a href="https://github.com/Rachnog/Neural-ODE-Experiments/blob/master/Neural_ODE_Basic.ipynb">https://github.com/Rachnog/Neural-ODE-Experiments/blob/master/Neural_ODE_Basic.ipynb</a></li><li><a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations">https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations</a></li><li>Paper List <a href="https://zhuanlan.zhihu.com/p/87999707">https://zhuanlan.zhihu.com/p/87999707</a></li></ol><h1 id="Neural-ODE"><a href="#Neural-ODE" class="headerlink" title="Neural ODE"></a>Neural ODE</h1><p>实际上是引入了一种新的网络结构。Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. 神经网络的输出是一个black box，用于求解ODE.</p><p>实际上，微分方程与神经网络的结合已经被不少人探索过了，许多神经网络都可以理解为微分方程的离散化形式，ResNet其实就是ODE的前向欧拉法，类似的还有PolyNet(后向欧拉)、FractalNet(龙格-库塔)。<strong>More reading here？</strong></p><p>个人感觉：这是另一种训练网络的方法？</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>residual network, RNN, normalizing flows 都可以 build complicated transformations by composing a sequence of transformations to a hidden state。</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422095041680.png" alt="image-20210422095041680"></p><p>如果将step t取极限小，即连续时间，此时，hidden state间的变化可以用一个ODE来描述。即：</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422095241837.png" alt="image-20210422095241837"></p><p>给定一个初始时刻的值，h(0), 那么最终时刻h(T)的值就是这个ODE的解（初始状态为h(0)）。 解h(T)的过程可以适用一个可以视为black box的ODE solver。</p><p>使用Neural ODE的好处：</p><ol><li>Memory efficiency: Not storing any intermediate quantities of the forward pass allows us to train our models with constant memory cost as a function of depth, a major bottleneck of training deep models.</li><li>Adaptive computation: 现代ODE求解器可保证近似误差的变换程度、监视误差水平、并即时调整其评估策略以达到要求的精度水平。 这使得评估模型的成本可以随着问题的复杂性而变化。 训练后，对于实时或低功耗应用，可能会降低准确性。</li><li>Scalable and invertible normalizing flows: can use it to construct a new class of invertible density models that avoids the single-unit bottleneck of normalizing flows, and can be trained directly by maximum likelihood.</li><li>Continuous time-series models: 与RNN做对比，Neural ODE 可以获得dynamics任意时间的值。</li></ol><h2 id="Reverse-mode-automatic-differentiation-of-ODE-solutions"><a href="#Reverse-mode-automatic-differentiation-of-ODE-solutions" class="headerlink" title="Reverse-mode automatic differentiation() of ODE solutions"></a>Reverse-mode automatic differentiation() of ODE solutions</h2><p>主要介绍利用一个black box的ODE solver如何求取梯度。Note: reverse-mode differentiation 就是 backpropagation)</p><p>主要是使用了<strong>adjoint sensitivity method</strong>这个方法。</p><p>假设使用了一个scaler的loss，这个loss是一个关于ODE solver输出的loss，即之前的h(T), 这里的z(t1)</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422101445840.png" alt="image-20210422101445840"></p><p>这里可以用基于梯度的优化方法来优化该损失函数，这要求我们求出ODE中的关于θ的梯度，而常规神经网络使用的反向传播算法要求我们首先要求出每层隐藏状态对损失的梯度，虽然我们可以用简单的Euler法将ODE转换为类似于ResNets的形式，但这样就必须存储所有的隐层状态，另外，这样我们也无法利用那些更高级的不可微的ODE求解器。最严重的问题是，想要在给定精度下求解一个复杂的ODE，可能需要非常大的迭代步数。</p><p>所以使用Pontryagin提出的<strong>伴随灵敏度方法(Adjoint Sensitivity Method)</strong> 来解决这类动力系统优化的问题，Adjoint Method将网络参数相对于损失的梯度的计算问题转化为了求解另一个ODE的问题，这样就避免了反向传播要求保存中间隐层信息的问题。</p><h3 id="Adjoint-Methods"><a href="#Adjoint-Methods" class="headerlink" title="Adjoint Methods"></a>Adjoint Methods</h3><p>NOTE：该Paper中有详细的推导。</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102138437.png" alt="image-20210422102138437"></p><p>定义伴随状态（adjoint）</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102146820.png" alt="image-20210422102146820"></p><p>继续改写：注意这个</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102234259.png" alt="image-20210422102234259"></p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102402203.png" alt="image-20210422102402203"></p><p>代入一下结果可得：</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102414617.png" alt="image-20210422102414617"></p><p>对a(t)求导</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422102512630.png" alt="image-20210422102512630"></p><p>可以看到，如此一来就得到了a(t)的dynamics。而t1时刻的adjoint是已知的。即：</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422103122508.png" alt="image-20210422103122508"></p><p>所以就可以再用一个ODE solver来求解adjoint</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422103159181.png" alt="image-20210422103159181"></p><p>在更新网络参数的时候，除了关于hidden state的梯度，还需要关于θ和t的梯度（在neural ODE中，t也是一个参数）。仿照上面的操作，也定义</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422103512434.png" alt="image-20210422103512434"></p><p>其最终也是可以化为类似的形式，变为一个ODE，用ODE solver来求解。</p><p>将这三者结合起来，就是更新参数所需的梯度了。<img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422103618636.png" alt="image-20210422103618636"></p><p>所以综合起来，每更新一次参数，就需要求解三个ODE即可。因为是使用数值方法求解ODE，其速度（精度等）可以根据需求调节，且有较小的memory cost.</p><p>若有多段ODE，求解起来大概就是这样的一个效果</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422103919754.png" alt="image-20210422103919754"></p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Neural-ODE.assets\image-20210422104645895.png" alt="image-20210422104645895"></p><h2 id="Replacing-residual-networks-with-ODEs-for-supervised-learning"><a href="#Replacing-residual-networks-with-ODEs-for-supervised-learning" class="headerlink" title="Replacing residual networks with ODEs for supervised learning"></a>Replacing residual networks with ODEs for supervised learning</h2><h2 id="Continuous-Normalizing-Flows"><a href="#Continuous-Normalizing-Flows" class="headerlink" title="Continuous Normalizing Flows"></a>Continuous Normalizing Flows</h2><h2 id="A-generative-latent-function-time-series-model"><a href="#A-generative-latent-function-time-series-model" class="headerlink" title="A generative latent function time-series model"></a>A generative latent function time-series model</h2><h2 id="Using-Neural-ODE-to-Learn-true-dynamics-function"><a href="#Using-Neural-ODE-to-Learn-true-dynamics-function" class="headerlink" title="Using Neural ODE to Learn true dynamics function"></a>Using Neural ODE to <em>Learn true dynamics function</em></h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Good Blog:</p><ol><li>about adjoint methods: <a href="https://zhuanlan.zhihu.com/p/337575425">https://zhuanlan.zhihu.com/p/337575425</a></li><li>general stuff about Neural ODE: <a href="https://zhuanlan.zhihu.com/p/340681521">https://zhuanlan.zhihu.com/p/340681521</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&amp;mid=2247495906&amp;idx=1&amp;sn=33cacbaf156bb8152fe300f2dbc7250e&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&amp;mid=2247495906&amp;idx=1&amp;sn=33cacbaf156bb8152fe300f2dbc7250e&amp;source=41#wechat_redirect</a></li><li><a href="https://zhuanlan.zhihu.com/p/51514687">https://zhuanlan.zhihu.com/p/51514687</a></li></ol><p>More Reading:</p><ol><li>Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations. 2017</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Resources&quot;&gt;&lt;a href=&quot;#Resources&quot; class=&quot;headerlink&quot; title=&quot;Resources&quot;&gt;&lt;/a&gt;Resources&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;DeepXDE: A deep learning library for
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Note for: OMPL-A Primer</title>
    <link href="http://canva4.github.io/2021/04/04/Note-for-OMPL-A-Primer/"/>
    <id>http://canva4.github.io/2021/04/04/Note-for-OMPL-A-Primer/</id>
    <published>2021-04-04T10:02:25.000Z</published>
    <updated>2021-04-05T15:25:32.382Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Note-for-“OMPL-A-Primer”"><a href="#Note-for-“OMPL-A-Primer”" class="headerlink" title="Note for: “OMPL: A Primer”"></a>Note for: “OMPL: A Primer”</h1><p>This note is about a tutorial book for the ‘Open Motion Planning Library(OMPL)’. This book can be found in the OMPL webpage. Additionally, some other note about OMPL will recorded here.</p><p>Here’s the webpage of OMPL: <a href="http://ompl.kavrakilab.org/">OMPL_LINK</a></p><h1 id="OMPL-A-Primer"><a href="#OMPL-A-Primer" class="headerlink" title="OMPL: A Primer"></a>OMPL: A Primer</h1><h2 id="Chap-1-Introduction"><a href="#Chap-1-Introduction" class="headerlink" title="Chap 1 Introduction"></a>Chap 1 Introduction</h2><p>OMPL支持sampling-based methods, 其库中已经包含了部分现有的sampling-based methods。整个doc的目的是：users should be able to use OMPL.app to solve motion planning queries in 2D and 3D workspaces, and utilize the OMPL framework to develop their own algorithms for state sampling, collision checking, nearest neighbor searching, and other components of sampling-based methods to build a new planner.</p><p>**NOTE: OMPL specializes in sampling-based motion planning! **</p><h2 id="Chap-2-Introduction-to-Sampling-based-Motion-Planning"><a href="#Chap-2-Introduction-to-Sampling-based-Motion-Planning" class="headerlink" title="Chap 2 Introduction to Sampling-based Motion Planning"></a>Chap 2 Introduction to Sampling-based Motion Planning</h2><p>首先是回顾一下robotic motion planning的目标：<strong>seeks to find a solution to the problem of “Go from the start to the goal while respecting all of the robot’s constraints.”</strong></p><p>接下来介绍motion planning的一种分类方式：</p><ol><li>Exact and Approximate Cell Decomposition</li></ol><p>这类方法的目标就是通过精确计算or近似的方法构建出state space or joint space（主要构建的目标是free space即:无碰撞且满足约束的space）。也就是将不碰撞的workspace分成一系列discrete cell。可以通过graph or tree来构建这个space（vertices代表free cell，edge代表一个可行的trajectory可以让机器人从一个cell前往至另一个cell）。这样就可以在该space中搜索一个从start to goal的解。然后这种直接显式构建state space的方法基本上</p><p>这类方法also works well in “controllable” systems (e.g., omni-directional base). 但如果需要考虑robot的dynamics，或者robot的dynamics是复杂且non-linear的，就会导致not clear to find how to move the system from one cell to an adjacent cell.</p><ol start="2"><li>Control-based Methods（不太熟悉）</li></ol><p>Control-based Methods是希望建立一个系统的运动方程（例如：y=Ax+b）,之后使用控制的方法，对这个系统进行控制，并希望达到特定的trajectory，例如使用optimal control的方法，来达到一个minimal error。其好处在于：基本上可以实现online。缺点在于：如果应用于complex dynamics或者cluttered environments时会导致往往无法计算出结果。</p><ol start="3"><li>Potential Fields</li></ol><p>传统的Potential Fields方法就是对于workspace中的每一个点计算一个vector，该vector由：calculating the sum of an attractive force emanating from the goal, and a repulsive force from all of the obstacles. 之后机器人可以根据当前位置的梯度前往goal configuration。其缺点在于极其容易陷入local mininum。理想情况下在系统的state space中计算这个field，但这等效于解决原始问题。 一些方法考虑了navigation function，其中保证了Potential Fields具有单个最小值，但是仅在低维空间中才有可能计算这样的函数并且这是non-trivial的.</p><ol start="4"><li>Randomized Planning</li></ol><p>在deterministic planners中，使用随机化已被证明是非常有效的。 例如，在Potential Fields中，在特定时间范围内应用随机动作的布朗运动已被证明在引导系统脱离局部极小值方面非常有效。 随机方法也已应用于非完整（non-holonomic）系统，尤其是应用于sampling-based planning。 </p><p>Sampling-based methods were inspired by randomization, and the use of samples, random or deterministic, when planning are particularly effective for high degree of freedom systems or those with complex dynamics.</p><p>接下来介绍OMPL主要应用的核心：Sampling-based Motion Planning</p><h3 id="Sampling-based-Motion-Planning"><a href="#Sampling-based-Motion-Planning" class="headerlink" title="Sampling-based Motion Planning"></a>Sampling-based Motion Planning</h3><p>传统方法在往往会在high-dimensional spaces和负责constraints时出现问题，而sampling-based motion planning reasons over <strong>a finite set of configurations</strong> in the state space。大部分sampling-based methods provide <strong>probabilistic completeness</strong>. 即意味着：if a solution exists, the probability of finding a solution converges to one as the number of samples reasoned over increases to infinity. </p><p>NOTE: Sampling-based approaches cannot recognize a problem with no solution.</p><p>The goal of a sampling-based motion planning query: the task of finding a collision path in the state space of the robot from a distinct start state to a specific goal state, utilizing a path composed of configurations connected by collision free paths.</p><p>主要的sampling-based methods主要有以下两种：</p><ol><li>Probabilistic Roadmap(PRM)</li></ol><p>本质是构建roadmap（graph）来表示一个free state space，之后在该roadmap上寻找一个最小路径。本质一个multi-query，即构建好的结果可以多次用于motion planning</p><p>NTOE: 对于任意的sampling-based methods，其free state space都是非显式可知的。**Each sample that is generated is checked for collision, and only collision free samples are retained. ** 其中如何sampling和如何进行碰撞检测都可以有user来自定义。</p><p>生成完samples之后，the roadmap itself can be constructed by connecting the random samples to form edges. 经典的PRM方法attempts to connect each sample to the k samples nearest to it by using a local planner that is tasked with finding short collision free paths. 其中这个local planner使用两个samples之间的差值来find path，并进行碰撞检测。</p><ol start="2"><li>Tree-based Planners</li></ol><p>本质是构建树型结构来表示一个free state space，之后在该tree上寻找一个最小路径。不同于PRM，其是一个single-query，即每次query都要重新构建一次树。</p><p>这类方法主要包含以下步骤：These methods begin by rooting a tree at the starting configuration of the robot. With the first node of the tree intact, random sampling of the free space then occurs. The planner employs an <strong>expansion heuristic</strong>, which typically gives the method its name, from which the sample is connected to the tree along a collision free path. </p><ol start="3"><li>二者的不同之处<ul><li>multi-query vs single-query</li><li>These trees do not normally cover the free space in the same manner that a roadmap would. However, when planning with differential constraints, it is not easy to encode control information into an undirected edge.</li><li>Controls are usually directed commands, and require a specific pre-condition in order for a particular control to be valid. Tree-based methods, on the other hand, excel at planning with complex dynamics because of the directed, acyclic nature of the underlying data structure. Control information can be encoded for each edge of the tree, with the vertices of the tree satisfying the prerequisites for the valid controls.</li></ul></li></ol><p>NOTE: sampling based methods并不会对state space做一个explicit representation。</p><h3 id="Primitives-of-Sampling-based-Planning"><a href="#Primitives-of-Sampling-based-Planning" class="headerlink" title="Primitives of Sampling-based Planning"></a>Primitives of Sampling-based Planning</h3><ul><li><p>存在许多Sampling-based Planner，且它们具有许多共性，但所有这些方法的关键在于：如何对状态空间进行采样。</p></li><li><p>Collision checking。It is used not only in the local planner when attempting to find collision free paths between samples, but also during the<br>sampling process itself. Collision checking的任务往往只是：accept a configuration of the robot and quickly determine whether or not this state is in collision.</p></li><li><p>Nearest neighbor searching。It is from the ability of determining whether two states of the robot are close that many of the common approaches<br>are able to effectively find paths through a high-dimensional space.</p></li></ul><h2 id="Chap-3-Getting-Started-with-OMPL-app"><a href="#Chap-3-Getting-Started-with-OMPL-app" class="headerlink" title="Chap 3 Getting Started with OMPL.app"></a>Chap 3 Getting Started with OMPL.app</h2><p>OMPL.app provides a graphical front-end to the core OMPL library, and allows a user to see many of the ideas from sampling-based motion planning in action。 主要简单的介绍了如何只用OMPL.app，并不是我们的重点。</p><p>此章节值得注意的点：</p><ul><li>3.1.3节 Bounding the Environment</li></ul><p>By default, the robot is constrained to move inside a tight bounding box around the environment, the start pose, and the goal pose. The bounding box is visualized in OMPL.app as the white frame around the environment and robot. <strong>These bounds apply to a reference point for the robot; the origin of the coordinate frame that defines its pose.</strong> This means that parts of the robot can stick outside the bounding box. It also means that if the reference point for your robot is far away from the robot itself, you can get rather unintuitive results. The reference point is whatever the origin is in the mesh; OMPL.app is not using the geometric center of the mesh as the reference point.</p><ul><li>3.2节 OMPL.app vs OMPL</li></ul><p>OMPL.app是基于OMPL的功能开发的，并且specifying a geometric representation for the robot and its environment, and provides a collision checking mechanism for the representation to the existing planners in the library. OMPL.app 使用了【Refer 1,2】作为其碰撞检测器。</p><p>而OMPL并没有明确表示机器人or环境，而是留给user根据其application来自定义的空间。</p><h2 id="Chap-4-Planning-with-OMPL"><a href="#Chap-4-Planning-with-OMPL" class="headerlink" title="Chap 4 Planning with OMPL"></a>Chap 4 Planning with OMPL</h2><h3 id="Design-Considerations"><a href="#Design-Considerations" class="headerlink" title="Design Considerations"></a>Design Considerations</h3><p>the library does not explicitly represent the geometry of the workspace or the robot operating in it. 而且也没有提供一个默认的explicit state validity/collision detection method. 这需要由用户根据使用的数据类型来自己定义。</p><p>OMPL也尽量的减少第三方的依赖，主要的依赖为：Boost：which allows OMPL to function in most major operating systems.</p><h3 id="OMPL-Foundations"><a href="#OMPL-Foundations" class="headerlink" title="OMPL Foundations"></a>OMPL Foundations</h3><p>再次回顾Sampling-based Motion Planning的三个关键组件：</p><ul><li>a sampler to compute valid configurations of the robot</li><li>a state validity checker to quickly evaluate a specific robot configuration</li><li>a local planner to connect two samples along a collision free path</li></ul><p>而OMPL将这些组件定义为了一些class，主要的class的功能如下：</p><ol><li>StateSampler</li></ol><p>provides methods for uniform and Gaussian sampling in the most common state space configurations</p><ol start="2"><li>NearestNeighbors</li></ol><p>This is an abstract class that is utilized to provide a common interface to the planners for the purpose of performing a nearest neighbor search among samples in the state space.</p><ol start="3"><li>StateValidityChecker</li></ol><p>evaluating a single state to determine if the configuration collides with an environment obstacle and respects the constraints of the robot. <strong>A default checker is not provided by OMPL</strong></p><ol start="4"><li>MotionValidator(analogous to the local planner)</li></ol><p>checks to see if the motion of the robot between two states is valid. At a high level, the MotionValidator must be able to evaluate whether the motion between two states is collision free and respects all the motion constraints of the robot.</p><ol start="5"><li>OptimizationObjective</li><li>ProblemDefinition</li></ol><p>A motion planning query is specified by the ProblemDefinition object. Instances of this class define a start state and goal configuration for the robot, and the optimization objective to meet, if any.</p><h3 id="Solving-a-Query"><a href="#Solving-a-Query" class="headerlink" title="Solving a Query"></a>Solving a Query</h3><p>整个OMPL的框架。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/04/05/sWN1Ut8nH5VCOKa.png" alt="image-20210405230510848"></p><p>When solving a particular query, the user is not required to instantiate all of the objects detailed in top figure. In fact, most of these objects have default implementations that are sufficient for general planning.</p><h3 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>OMPL更是一个极好的用于compare two or more planners的平台。</p><h2 id="Chap-5-Advanced-Topics-in-OMPL"><a href="#Chap-5-Advanced-Topics-in-OMPL" class="headerlink" title="Chap 5 Advanced Topics in OMPL"></a>Chap 5 Advanced Topics in OMPL</h2><p>实际上是要向user提供有关库的一些常用可选功能以及如何将它们添加至代码中的。</p><h3 id="State-Space-Construction"><a href="#State-Space-Construction" class="headerlink" title="State Space Construction"></a>State Space Construction</h3><p>例如：n维欧氏空间，SO(3)，SE(3)等。</p><h3 id="Planner-Customization"><a href="#Planner-Customization" class="headerlink" title="Planner Customization"></a>Planner Customization</h3><p>可以比较方便的定制化自己的planner？</p><h3 id="Python-Bindings"><a href="#Python-Bindings" class="headerlink" title="Python Bindings"></a>Python Bindings</h3><p>完全支持使用python来调用。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Eric Larsen, Stefan Gottschalk, Ming C. Lin, and Dinesh Manocha. Fast proximity queries with swept sphere volumes. In IEEE International Conference on Robotics and Automation, pages 3719–3726, 2000.</li><li>Jia Pan, Sachin Chitta, and Dinesh Manocha. FCL: A general purpose library for collision and proximity queries. In IEEE International Conference on Robotics and Automation, May 2012.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Note-for-“OMPL-A-Primer”&quot;&gt;&lt;a href=&quot;#Note-for-“OMPL-A-Primer”&quot; class=&quot;headerlink&quot; title=&quot;Note for: “OMPL: A Primer”&quot;&gt;&lt;/a&gt;Note for: “O
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="motion planning" scheme="http://canVa4.github.io/tags/motion-planning/"/>
    
      <category term="OMPL" scheme="http://canVa4.github.io/tags/OMPL/"/>
    
  </entry>
  
  <entry>
    <title>Mobile Manipulator Paper Reading</title>
    <link href="http://canva4.github.io/2021/03/05/Mobile-Manipulator-Paper-Reading/"/>
    <id>http://canva4.github.io/2021/03/05/Mobile-Manipulator-Paper-Reading/</id>
    <published>2021-03-05T12:11:54.000Z</published>
    <updated>2021-04-17T02:50:31.428Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mobile-Manipulator-Paper-Reading"><a href="#Mobile-Manipulator-Paper-Reading" class="headerlink" title="Mobile Manipulator Paper Reading"></a>Mobile Manipulator Paper Reading</h1><p><del>关于在读Mobile Manipulator Door Pushing Task中遇到的一些paper的note</del></p><p>主要关于coordinated motion planning的一些相关方法。</p><h2 id="Difficulties"><a href="#Difficulties" class="headerlink" title="Difficulties"></a>Difficulties</h2><h3 id="For-door-Opening-Task"><a href="#For-door-Opening-Task" class="headerlink" title="For door Opening Task:"></a>For door Opening Task:</h3><ol><li>How to represent a state in this door opening task? When this representation has high dimension, it’s hard to find a solution.</li><li>The optimal position or state to open the door is unkown.</li><li>Door open problem itself need to consider a lot of stuff e.g. collide, reachability…</li><li>identified the position of the door</li></ol><h3 id="For-Coordinate-Motion-Planning"><a href="#For-Coordinate-Motion-Planning" class="headerlink" title="For Coordinate Motion Planning"></a>For Coordinate Motion Planning</h3><ol><li>base need to consider dynamics! Especially for non-omnidirectional base.</li><li>how to desgin the <strong>State</strong> that represent the base in this motion planning task.</li></ol><h2 id="Task-Summary"><a href="#Task-Summary" class="headerlink" title="Task Summary"></a>Task Summary</h2><p>TODO: </p><ol><li>Pybullet &amp; Gazebo <a href="https://github.com/oscar-lima/pybullet_ros">https://github.com/oscar-lima/pybullet_ros</a></li><li></li></ol><p>Reading List:</p><ol><li>Receding horizon control (RHC), also known as model predictive control (MPC) <a href="https://web.stanford.edu/~boyd/papers/code_gen_rhc.html">https://web.stanford.edu/~boyd/papers/code_gen_rhc.html</a></li><li></li></ol><p>Paper1 </p><ol><li>impedence control</li><li>reactive controller</li><li>compliant control</li><li>constrained planner</li><li>graph Search<ol><li><a href="https://zhuanlan.zhihu.com/p/54510444">https://zhuanlan.zhihu.com/p/54510444</a></li></ol></li></ol><p>Paper2</p><ol><li>support mapping</li></ol><h1 id="Paper-List"><a href="#Paper-List" class="headerlink" title="Paper List"></a>Paper List</h1><ol><li>Planning for Autonomous Door Opening with a Mobile Manipulator 2010</li><li>Finding Locally Optimal, Collision-Free Trajectories with Sequential Convex Optimization 2013</li><li>Safe and Coordinated Hierarchical Receding Horizon Control for Mobile Manipulators 2020</li><li>Perceptive Model Predictive Control for Continuous Mobile Manipulation 2020</li><li>CHOMP: Gradient optimization techniques for efficient motion planning. 2009</li><li>STOMP: Stochastic trajectory optimization for motion planning. 2011</li><li>Sampling-Based Methods for Motion Planning with Constraints (survey) 2018</li></ol><h1 id="Planning-for-Autonomous-Door-Opening-with-a-Mobile-Manipulator"><a href="#Planning-for-Autonomous-Door-Opening-with-a-Mobile-Manipulator" class="headerlink" title="Planning for Autonomous Door Opening with a Mobile Manipulator"></a>Planning for Autonomous Door Opening with a Mobile Manipulator</h1><p>Author: Sachin Chitta er al  <a href="https://www.cs.cmu.edu/~maxim/files/doorplanner_icra10.pdf">Paper Link</a></p><p>Journal/Conference: ICRA 2010</p><p><strong>本文核心：找到一了一种相对有更低dimension的state representation（graph representation），这样可以使用一些图上的搜索算法（planning method）生成一个trajectory，从而达到motion planning的目的。</strong></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h2><p>对于door opening这个问题本身就存在很多困难</p><ol><li><p>it is hard to identify precisely the position and size of doors and handles</p></li><li><p>it is hard to autonomously compute the right approach to grasping and manipulating the handle</p></li><li><p>it is hard to compute a coordinated arm-base motion that opens the door wide enough for the robot to navigate through it.</p></li></ol><p>本文主要针对解决第三个问题，即如何plan出一个手臂和底盘相互协作来开门的动作。对于第三个问题，如果简单的使用规定好的基于先验知识的motion来开门，会导致无法适应多种多样的门。而如果是用plan的方法，opening a door using a mobile manipulation platform typically involves tight coordination in between the motion of the base and the motion of the arm. This makes the problem high-dimensional and thus hard to plan for.</p><p>所以这篇paper的主要工作就是提出了一种对于机器人state的低维graph-based representation，在保证这种representation可以包含足够的信息（环境障碍物信息，机械臂reachability的信息等）的前提下，这种低维的、graph-based representation就可以使用在graph上的一些启发式搜索方法，把原有问题的planning变为一个在图上的搜索问题，使用一些例如A*的方法来生成一个基于这个低维state representation的trajectory，这就完成了上面所述的problem 3。因为这个低维state representation也包含了足够的信息，可以计算出真正所需的底盘和arm的motion.</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work:"></a>Related Work:</h3><p>一些方法使用例如impedence control、reactive controller、constrained planner等方法，其问题是没有使用planning的方法，而另一些使用planning的方法有没有考虑到碰撞的问题，而且以上方法只考虑了pushing, no pulling!</p><p>作者这篇paper提出的planning方法：既包含了pushing and pulling；而且还考虑了如何避免碰撞。 </p><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><p>使用的机器人PR2。全向底盘+7dof机械臂，主要使用的sensor为两个laser，一个用来为底盘提供周围的2D representation；另一个用来获取机器人前方的3D信息（如门的信息等）。</p><p>这里还提到了7dof这种redundant dof的好处，This proves useful in extending the usable workspace of the arm, especially in cluttered environments.</p><h2 id="System-Architecture"><a href="#System-Architecture" class="headerlink" title="System Architecture"></a>System Architecture</h2><p>作者将整个door opening问题分为了两个子问题：</p><ol><li>检测门和门把手  —-&gt; 使用现成的方法 见**[Reference 1]**  该方法需要知道门的一些先验信息，该方法的输出为：门和门把手以及hinge的位置信息。NOTE：门的旋转方向为一个先验知识！</li><li>规划和执行开门的动作</li></ol><p>对于第二个子问题即：<strong>规划和执行开门的动作</strong>，作者又将其进一步的分为了一些列子问题或者说是一个解决这个问题的流程：</p><ol><li>往门的方向移动机器人，目标是可以接触到门把手</li><li>伸出手臂抓住门把手，轻微的unlatch（这个过程底盘不动）</li><li>由模型决定（先验知识）决定这个门是应该拉，还是应该推；并轻微的拉开or推开门</li><li>规划一个手臂和底盘协同的motion来开门。这个规划的动作是手臂与底盘配合着来的，而且考虑了避免碰撞</li><li>如果是使用pulling的方法，检测门和手臂的碰撞。如果可能会发生碰撞，就会换一边，转而抓住门把手的另一侧。（这个动作中底盘不动）</li><li>如果没有完全打开门，则之后基于5继续执行一个pushing开门的planning</li></ol><p>这6个子问题又可以分为两类，</p><ul><li>the act of reaching out and grasping the handle and opening the door slightly （1~3）</li><li>the act of opening the door using a coordinated motion of the base and the arm of the robot. （3~6）</li></ul><p>前一个是使用现有的解决方案：**[Reference 2]**</p><p>本文主要是解决第二个问题，即如何生成一个base &amp; arm 协同开门的motion。</p><p>简单的分析一下这个子问题，执行一些列开门动作时，有一个重要的约束，即：gripper必须要抓着门把手。</p><p>另外一个难点就是：<strong>goal position是不确定的！在执行动作的时候，并没有一个确定的最优的goal，而是仅仅有一个要找到能打开门的position的这个目标。</strong></p><p>如果想实现一个对于全部configuration space的planner（底盘运动+7 dof arm），这就需要一个constrained planner，而这类constrained planner在处理高纬度有约束的情况下是非常困难的。</p><p>所有作者有了本文的核心方法，也是main contribution，即：设计了一个lower-dimensional graph-based representation，基于这个就可以比较轻松的找到一个合理的motion了。</p><p><strong>整体思路：</strong></p><ul><li>将高维的问题化简为先在平面（即只考虑底盘）上规划，找到一个手臂可以接触到门把手且比较合理的位置</li><li>之后使用IK计算arm的trajectory，并与base一起运动，这其中要保持手臂可以一直接触到门把手这个约束</li></ul><p>本质上就是将三维的规划问题变为了一个2D 规划+IK 的问题。</p><h2 id="Coordinated-Arm-Base-Motion-Planning"><a href="#Coordinated-Arm-Base-Motion-Planning" class="headerlink" title="Coordinated Arm-Base Motion Planning"></a>Coordinated Arm-Base Motion Planning</h2><p>整个Coordinated Arm-Base Motion Planning包含如何构建一个低维的Graph Representation，如何在这个graph上搜索，基于这个结果如何生成Coordinated Arm-Base Motion。</p><p>再次明确一下这个planning的目标：</p><p>Note that the final goal for the base motion is not specified. The goal for the entire task, however, is to open the door.</p><h3 id="Graph-Representation"><a href="#Graph-Representation" class="headerlink" title="Graph Representation"></a>Graph Representation</h3><p>总结来说，就是要将原问题变为一个在图上的搜索问题。所以需要定义好每个state是什么，state之间的转移情况，一起每个state的cost。</p><h4 id="State-Variables"><a href="#State-Variables" class="headerlink" title="State Variables"></a>State Variables</h4><p>明确state Variables: $s=(x,y,\theta,d)$</p><p>x,y 为底盘的位置，$\theta$为base的朝向。如果只含有这三个，显然无法包含我们的goal的信息，所以需要额外增加state的维度来包含这个信息，作者增加的额外一维即为d，即：using a single binary variable that we call <em>door interval</em>.</p><p>下图为一个关于 <em>door interval</em>的解释。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/06/sA3WIEgRmvS8foV.png" alt="image-20210306112413167" style="zoom:67%;" /><p>所有门可能到达的角度（前提是不与机器人发生碰撞）被分为了两个区间，如果门处于这两个区间中的一个，那么d这个变量就是那个对应的区间。可见这个d就表明了目前门所在的大致位置。</p><p>设置一开始门关闭的时候位于intervel 0，如果门移动到了intervel 1，那么就完成了我们open的目标。</p><p>并且基于这个$s=(x,y,\theta,d)$，也可以计算出可行的门打开的角度（因为要满足机器人抓住门这个约束），即这些角度就是机器人手可以到达的地方所对应的角度，而且不会发生碰撞，对于每一个state s这一系列角度的集合被称为：$\Lambda(s)$。作者对这个角度集合做了分度为1度的离散化。这种离散化的好处是极大的减小了搜索空间。</p><p>此时就可以定义一个state-space了，这个state-space只包含有非空$\Lambda(s)$的s。</p><p>所以用定义好的state来描述目标就是：A state s is a goal state if it belongs to interval 1 and if Λ(s) contains an angle that is closer than a threshold $\delta_d$ from the fully open door angle.</p><p>下面就定义state之间如何进行Transitions。</p><h4 id="Transitions"><a href="#Transitions" class="headerlink" title="Transitions"></a>Transitions</h4><p>作者在构建Transitions的时候，使用了lattice-based planning representation的方法**[Reference 3&amp;4]**。</p><p>这种lattice-based representation是对于C-space的一种离散化，将c-sapce离散化为了一些列state，并将这些state用一些可行的、short-term的action相连接。下图为一个示例：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/06/l5xoX2WeVtYAhQb.png" alt="image-20210306114533040"></p><p>这种lattices就将motion planning变为了一个graph search的问题。During the search (planning), these actions are translated and rotated for each state encountered by search, and the successors of the state are computed as the corresponding end configurations of these actions. The planner also checks all actions against collisions by checking the footprint of the action (constructed from the footprint of the robot) against the map of the environment.</p><p>这里要额外明确好如何定义s中的d的transition。</p><ul><li>如果$d(s)\ne d(s^{‘})$, 想要发生transition必须有：$\Lambda(s)\cap \Lambda(s^{‘}) \neq \varnothing$且x,y,θ必须相等。也就意味着：the robot is out of the way of the door and can reach the door handle</li><li>对于$d(s)=d(s^{‘})$，就相对简单了，只需要as the robot executes any action in our lattice, it will be able to maintain its gripper on the door handle.</li></ul><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>planner使用的cost function是一个包含了2D cost（表示了base到最近obstacle的距离）和一个基于arm位姿的cost（包含了该位姿是不是出于一个比较舒服的，即：每个joint的position不接近limit的位置）</p><h4 id="2D-Costmap"><a href="#2D-Costmap" class="headerlink" title="2D Costmap"></a>2D Costmap</h4><p>将三维投影至2D来构建一个2D costmap，具体方法见：**[reference 5]**。其结果代表了离最近obstacles的距离</p><h4 id="Arm-based-cost"><a href="#Arm-based-cost" class="headerlink" title="Arm-based cost"></a>Arm-based cost</h4><p>其表示了每个joint的position不接近limit的位置，具体计算过程见paper</p><h3 id="Graph-Search"><a href="#Graph-Search" class="headerlink" title="Graph Search"></a>Graph Search</h3><p>使用Anytime Repairing A* (ARA*)算法见**[reference 8]**。值得注意的是，这个方法相比于A star算法，额外考虑了时间因素，所以有时无法获得最优解（不是最低的cost），但仍然给出了一个关于该解与最优解关系的下界。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Arm planning Using: <strong>[Reference 7]</strong></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Laser-based perception for door and handle identification 2009</li><li>Autonomous Door Opening and Plugging In using a Personal Robot 2010</li><li>Generating near minimal spanning control sets for constrained motion planning in discrete state spaces 2005</li><li>Planning long dynamicallyfeasible maneuvers for autonomous vehicles 2009</li><li>The office marathon: Robust navigation in an indoor office environment 2010</li><li>A formal basis for the heuristic determination of minimum cost paths（A star算法）</li><li>Combining Planning Techniques for Manipulation Using Real-time Perception 2010</li><li>ARA*: Anytime A* with provable bounds on sub-optimality  2003</li></ol><h1 id="Finding-Locally-Optimal-Collision-Free-Trajectories-with-Sequential-Convex-Optimization-trajopt"><a href="#Finding-Locally-Optimal-Collision-Free-Trajectories-with-Sequential-Convex-Optimization-trajopt" class="headerlink" title="Finding Locally Optimal, Collision-Free Trajectories with Sequential Convex Optimization(trajopt)"></a>Finding Locally Optimal, Collision-Free Trajectories with Sequential Convex Optimization(trajopt)</h1><p>Author: John Schulman er al </p><p>Journal/Conference: </p><p><strong>本文核心：本文本质来讲是提出了一种进行机器人motion planning的考虑了碰撞的trajectory optimization方法。</strong></p><ul><li>使用了sequential convex optimization procedure（一个原本非凸的问题，变为顺序的解一系列凸的子问题）</li><li>设计了一种no-collisions constraint that directly considers continuous-time safety，与上一点结合就可与解决许多motion planning方面的问题</li></ul><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>此note的第一篇paper给出了一种生成motion trajectory的方法，而本文的方法更倾向于优化这些已经生成好的trajectory，即：Trajectory optimization algorithms。Trajectory optimization algorithms在机器人的motion planning领域内起到了以下作用：</p><ul><li>they can be used to smooth and shorten trajectories generated by some other method.</li><li>they can be used to plan from scratch: one initializes with a trajectory that contains collisions and perhaps violates constraints, and one hopes that the optimization converges to a high-quality trajectory satisfying constraints.</li></ul><p>对于这motion planning的trajectory optimization方法有以下两个关键的组成部分：</p><ul><li>numerical optimization method  <strong>本文使用</strong>：sequential convex optimization，并使用$l_1$penalties来讲不等式约束和等式约束加入到objective里面</li><li>the method of checking for collisions and penalizing them <strong>本文使用</strong>：signed distance，并且考虑了continuous-time safety</li></ul><h2 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h2><p>一下均为一些trajectory optimization的方法。</p><ol><li>Practical methods for optimal control and estimation using nonlinear programming 2010</li></ol><p>Trajectory optimization在optimal control中的作用</p><ol start="2"><li>CHOMP: Gradient optimization techniques for efficient motion planning. 2009</li><li>CHOMP: Covariant hamiltonian optimization for motion planning. 2010</li><li>STOMP: Stochastic trajectory optimization for motion planning. 2011</li></ol><h2 id="Sequential-Convex-Optimization"><a href="#Sequential-Convex-Optimization" class="headerlink" title="Sequential Convex Optimization"></a>Sequential Convex Optimization</h2><p>本质上就是将一个non-convex optimization problem变为一系列凸的问题来处理，可能无法找到optimal，但一般可以到达一个local optimal。</p><p>一个机器人的motion planning问题可以被定义为一个non-convex optimization problems</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/6zqDetrbaQRos92.png" alt="image-20210307160354865"></p><p>对于kinematic motion planning problems，状态变量x的维数应该为：T×K，T为time-step的个数，K为dof。</p><p>所以optimization parameters可以写为: $\theta_{1:T}$，其中$\theta_{t}$表示在时间t时刻，机器人的configuration</p><p>首先确定objective，这个objective的目的是：to encourage minimum-length paths, we use the sum of squared displacements</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/nZq2cLM6IgdKSXU.png" alt="image-20210307160808722"></p><p>对于对于不同的任务有不同的constraints，比如对于开门任务需要保证无碰撞，且end-effector一直与门把手相接触。</p><p>但是这个原本的优化问题仍然是非凸的，所以使用Sequential Convex Optimization来解决，本质上来就就是将原问题构建为一系列子问题，每个子问题是在当前state对于原问题的一个估计。每个subproblem的目的是generate a step $\Delta x$ 来让original problem变好一点。</p><p>其两个核心的部分如下：</p><ol><li>a method for constraining the step to be small, so the solution vector remains within the region where the approximations are valid  –&gt; 使用trust region，本质就是一个bound加载state变量上</li><li>a strategy for turning the infeasible constraints into penalties, but eventually ensuring that all of the constraint violations are driven to zero. –&gt;使用$l_1$penalties。即对于每个不等式约束，变为$|g_i(x)|^+, |x|^+=max(x,0)$；对于每个等式约束，变为$|h_i(x)|, |x|^+=|x|$；之后将这两个penalties乘以系数$\mu$加到objective上即可。</li></ol><p>整体的算法如图，大体思路就比较明确了，就是通过一次次循环增加$\mu$的值，最终使得结果满足objective，而且在迭代的子问题（由原问题通过一个近似得到，即将非线性的约束做一个一阶近似）也是一个凸的问题。更多的细节见paper</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/VlrC984m51BYPFs.png" alt="image-20210307163306993"></p><p>至此，已经确定了如何解这样一个优化问题，下一步就是确定constraint了。</p><h2 id="Discrete-Time-No-Collisions-Constraint"><a href="#Discrete-Time-No-Collisions-Constraint" class="headerlink" title="Discrete-Time No-Collisions Constraint"></a>Discrete-Time No-Collisions Constraint</h2><p>本质来讲，这个constraint就是让所有的刚体的距离都在一定范围之外。对于每一个time-step都应该满足。这个sd( , )就是文中的signed distance，本质来就是如果两个刚体没有碰撞，那么这个距离就大于0，且相聚越远值越大。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/eCXsdyTApc7wYF2.png" alt="image-20210307164102386"></p><p>变为penalties后为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/7CKYS8pOlk3a2yb.png" alt="image-20210307164301748"></p><p>可见这样的话计算量极大，左右使用了一些方法删去了部分不可能碰撞的pair，降低了复杂度。更多关于如何高效计算signed distance和减低penalties计算复杂度方法的细节见paper。</p><h2 id="ENSURING-CONTINUOUS-TIME-SAFETY"><a href="#ENSURING-CONTINUOUS-TIME-SAFETY" class="headerlink" title="ENSURING CONTINUOUS-TIME SAFETY"></a>ENSURING CONTINUOUS-TIME SAFETY</h2><p>使用上面所介绍的这个No-Collisions Constraint就可以优化这个discretely-sampled trajectory，让其无碰撞。这些trajectory上的waypoints需要转变为一个continuous-time trajectory，直观的可以使用线性插值，但这种可能会导致continuous-time trajectory发生碰撞，如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/DY6JUTgZAfFVzS5.png" alt="image-20210307165508196"></p><p>为了解决这个问题，引入swept-out volume需要再将penalties做一个修正，swept-out volume如下图所示，是一个当前时刻的位置与下一个时刻位置的一个convex hull</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/bf74qrlmRKeESPX.png" alt="image-20210307165956616"></p><p>然后将上一部分的signed distance变为如下的形式即可</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/07/XnIhZ2HbsFARQTL.png" alt="image-20210307170210716"></p><p>这个计算起来也比较复杂，文中有详细描述了如何简化这个的计算。</p><p>NOTE: 作者在implement part也说明了如何设置例如开门任务的constraint</p><h2 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h2><ol><li>A fast and robust GJK implementation for collision detection of convex objects. 1999  关于swept-out volume</li></ol><h1 id="Safe-and-Coordinated-Hierarchical-Receding-Horizon-Control-for-Mobile-Manipulators-HRHC"><a href="#Safe-and-Coordinated-Hierarchical-Receding-Horizon-Control-for-Mobile-Manipulators-HRHC" class="headerlink" title="Safe and Coordinated Hierarchical Receding Horizon Control for Mobile Manipulators(HRHC)"></a>Safe and Coordinated Hierarchical Receding Horizon Control for Mobile Manipulators(HRHC)</h1><p>Author: Jessica Leu er al</p><p>Journal/Conference: American Control Conference 2020</p><p><strong>本文核心：本文针对的情况主要是time-varying dynamic environments</strong></p><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><p>开头有一段很好的关于literature的一个简单的review，其中提到了Reference中的几篇paper，值得根据这个继续阅读。</p><p>以前的这些方法的问题是他们一般是基于一个static environment，对于time-varying的环境并不适用。可以进一步通过加入prior knowledge来降低计算量，但显然这种基于prior knowledge的方法的使用范围还是有限的。</p><p>optimization methods受高维数数据影响更少，所以更适合用于这种coordinated motion planning.</p><p>本篇文章的核心目标是：</p><ul><li>更高效。planning 所需时间更短</li><li>可让算法适用于更多的场景。</li></ul><p>算法分为：A high level motion planning module用于解决motion planning问题（求解非凸的优化问题）&amp; a low level safety controller运作在一个高频率下，保证当前的action是安全的。</p><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>主要介绍了这两部分：</p><ul><li>Formulation ofthe motion planning optimization problem 主要说明了一些notation和使用什么样的cost function</li><li>Convex Feasible Set Algorithm（CFS），如何快速求解一个motion planning（non-convex problem）</li><li>以及如何设计constraint（Constraints formulation），保证碰撞安全的约束使用capsules（代表link）之间的距离来计算</li></ul><h2 id="Hierarchical-Receding-Horizon-Control-HRHC"><a href="#Hierarchical-Receding-Horizon-Control-HRHC" class="headerlink" title="Hierarchical Receding Horizon Control (HRHC)"></a>Hierarchical Receding Horizon Control (HRHC)</h2><p>在上层的motion planning部分主要使用CFS，额外设计了Soft constraints。即：引入松弛变量。</p><p>下层额外加入了Low-level safety controller，因为上层生成的无碰撞轨迹的生成时间相比于环境的变化时间较长，所以决策系统上一时刻生成的轨迹可能在此时并不适用，甚至会发生碰撞，所以需要一个更新更快的底层安全控制器来保证系统的安全性。实现起来本质是使用了一个更粗糙的碰撞检测。</p><h2 id="Reference-2"><a href="#Reference-2" class="headerlink" title="Reference"></a>Reference</h2><ol><li>S. M. LaValle, Planning algorithms. Cambridge university press, 2006.</li><li>Search-based planning for manipulation with motion primitives 2010</li><li>“Optimization techniques applied to multiple manipulators for path planning and torque minimization,” 2002</li><li>“Local motion planning for collaborative multi-robot manipulation of deformable<br>objects,” 2015</li><li>“Efficient kinematic planning for mobile manipulators with nonholonomic constraints using optimal control,” 2017 <strong>VITAL</strong></li><li>“Stomp: Stochastic trajectory optimization for motion planning,”  2018</li><li>“Synthesis and stabilization of complex behaviors through online trajectory optimization,” 2012 iterative LQR</li></ol><h1 id="Perceptive-Model-Predictive-Control-for-Continuous-Mobile-Manipulation"><a href="#Perceptive-Model-Predictive-Control-for-Continuous-Mobile-Manipulation" class="headerlink" title="Perceptive Model Predictive Control for Continuous Mobile Manipulation"></a>Perceptive Model Predictive Control for Continuous Mobile Manipulation</h1><p>Author: Johannes Pankert er al</p><p>Journal/Conference: IROS 2020</p><p><strong>本文核心：本质上是一个给定预期end-effector trajectory（基于task space or Cartesian Space），产生每个关节控制量（速度）的controller；can control interaction-forces of a mobile robot without torque-controllable joints，</strong></p><h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><p>主要针对，Continuous manipulation task，例如檫黑板，cleaning这些workspace超出一个固定机械手的workspace.</p><h2 id="Model-Predictive-Control-MPC"><a href="#Model-Predictive-Control-MPC" class="headerlink" title="Model Predictive Control(MPC)"></a>Model Predictive Control(MPC)</h2><p>MPC是一种使用优化方法的controller，其考虑了约束的影响。</p><p><strong>Sequential Linear Quadratic Model Predictive Control(SLQ method)</strong></p><p>简介：本质上还是一个求解optimal Control的方法。对于系统的dynamic进行在当前state和input trajectory进行linearized。之后对于cost function做一个Quadratic approximate。之后求解出一个affine的控制策略</p><p>详细见Reference。</p><p><strong>System Model</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/19/y8Guvp79AKi5BWt.png" alt="image-20210316121642029"></p><p>这里的state就是底盘的位姿（twist中的translation和rotation，rotation用qua）每个arm的转角θ。</p><p><strong>Cost Function</strong></p><p>主要包含了三部分</p><ul><li>Task space tracking error：目标end effector的pose和预期pose间的差距。这个error又分为：translational error（用position vector之间的差距来计算）&amp; orientation error（用四元数计算）</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/19/oirdfCYBuy6gqGb.png" alt="image-20210316121403540"></p><p>end-effector pose的计算使用：Robcogen[Reference 1]，本质上就是求forward Kinematics</p><p>因为使用SLQ想要收敛的话，需要二阶导数项半正定，而一般forward Kinematics的二阶导未必半正定，所以使用Gauss-Newton来近似二阶导</p><ul><li>Soft Constraints:  （不等式约束）使用Relaxed Barrier Functions（RBF）</li><li>Collision Avoidance：本质上也是一种不等式约束，使用RBF函数。在内部函数里使用：球体来近似robot的link，并query Euclidean Signed Distance Field (ESDF)，并且额外引入cache gradients along with the signed distances，来加速计算（结果见paper的实验部分）。最终的约束如下式所示：FK为forward Kinematics。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/19/NxK6jd7mutHWz5I.png" alt="image-20210316124829922"></p><p>生成Euclidean Signed Distance Field (ESDF)使用Voxblox[Reference 2]。</p><p>Querying cached distances and gradients from a euclidean signed distance field(ESDF) 可以有更高的执行效率。</p><p>ESDF的细节见[Refernce 3,4]</p><p><strong>Task Space Admittance Control</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/19/r8UXQzthxnPo95C.png" alt="image-20210316151931284"></p><p>Detail: TODO</p><p>可以实现力的控制！但需要力传感器和给定的力的大小。</p><p><strong>Mechanical Stability</strong></p><p>额外增加了一个SLQ问题的constraint（using Zero Moment Point (ZMP)）</p><p>Detail: ????</p><h2 id="Reference-3"><a href="#Reference-3" class="headerlink" title="Reference"></a>Reference</h2><p>3、4是关于Euclidean Signed Distance Field (ESDF)的</p><ol><li><p>RobCoGen: A code generator for efficient kinematics and dynamics of articulated robots, based on Domain Specific Languages  2016</p></li><li><p>“Voxblox: Incremental 3D Euclidean Signed Distance Fields for onboard MAV planning,” 2017</p></li><li><p>CHOMP: Gradient optimization techniques for efficient motion planning. 2009</p></li><li><p>STOMP: Stochastic trajectory optimization for motion planning. 2011</p></li></ol><h1 id="CHOMP-Gradient-optimization-techniques-for-efficient-motion-planning"><a href="#CHOMP-Gradient-optimization-techniques-for-efficient-motion-planning" class="headerlink" title="CHOMP: Gradient optimization techniques for efficient motion planning"></a>CHOMP: Gradient optimization techniques for efficient motion planning</h1><p>Author: Nathan Ratliff er al</p><p>Journal/Conference: 2009</p><p><strong>本文核心：continuous path refinement  methods &amp; standalone motion planner；使用covariant gradient来优化采样得到的trajectory；基于configuration space</strong></p><p>Q: </p><ol><li>covariant gradient descent</li><li>finite differencing operator/matrix</li><li>functional gradient？（泛函梯度）</li></ol><h2 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h2><p>CHOMP的核心就是利用covariant gradient来优化采样得到的trajectory。基于sample-based methods改良而来。sample-based methods主要包含以下两个步骤：</p><ul><li>first find a feasible path（例如RRT：建树+A*搜索）</li><li>optimize it to remove redundant or jerky motion（RRT*等）</li></ul><p>第二个步骤实际上是一种trajectory optimization，之前比较流行的方法是：</p><ul><li>使用一种 shortcut heuristic [<strong>Reference 1</strong>]</li><li>或者使用elastic bands or elastic strips planning involves modeling paths as mass-spring systems [<strong>Reference 2</strong>]</li></ul><p>CHOMP(Covariant Hamiltonian Optimization for Motion Planning)的特点是：</p><ul><li>不需要提前输入一个collision-free的path（以前的方法需要），可以将一个不可行的trajectory变为一个local optimal的解</li><li>使用covariant gradient update rules（local optimal guarantee）</li></ul><h2 id="THE-CHOMP-ALGORITHM"><a href="#THE-CHOMP-ALGORITHM" class="headerlink" title="THE CHOMP ALGORITHM"></a>THE CHOMP ALGORITHM</h2><p>一个核心思想是：the proper use of geometrical relations, particularly as they apply to<br>inner products. 尤其在：differential geometry？</p><h3 id="Covariant-gradient-descent"><a href="#Covariant-gradient-descent" class="headerlink" title="Covariant gradient descent"></a>Covariant gradient descent</h3><p>Goal：find smooth, collision-free, trajectory through the <strong>configuration</strong> space between two prespecified end points</p><p>Cost Term:</p><p>包含两项：obs包含了障碍物信息（描述了离障碍物的距离）；prior: measures dynamical quantities of the robot such as smoothness and acceleration. $\xi$代表trajectory</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/oSNDJLMpj5RH13s.png" alt="image-20210328175140457"></p><p>目标是在每次iteration都通过最小化一个描述trajectory smoothness的function的local approximation。我们的cost就是这个function。</p><p>其中：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/vhp1GsUTjziCXBF.png" alt="image-20210328180642410">，A是一个对称正定矩阵</p><p>在第k步，用一阶泰勒展开来近似目标函数，即：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/wzmDo6QVK4C5la1.png" alt="image-20210328180139838">，其中<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/R4mqvcxZz8tLgVI.png" alt="image-20210328180149902"></p><p>所以update rule就是：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/uHFcE76vP8kNq4e.png" alt="image-20210328180237799"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/FSz6JhQWxYUl19s.png" alt="image-20210328180403216"></p><p>即：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/8C5J9hlUXinNMI3.png" alt="image-20210328180415645"></p><h3 id="Understanding-the-Updata-Rule"><a href="#Understanding-the-Updata-Rule" class="headerlink" title="Understanding the Updata Rule"></a>Understanding the Updata Rule</h3><p>作者指出这个update rule是covariant gradient descent的一个特殊形式。（covariant gradient descent 见[**Reference 3,4 **]）</p><p>TODO</p><h3 id="Obstacles-and-distance-fields"><a href="#Obstacles-and-distance-fields" class="headerlink" title="Obstacles and distance fields"></a>Obstacles and distance fields</h3><p>首先要说明：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/VbgYudDeNm6jAkx.png" alt="image-20210329110121142"></p><p>这里使用计算距离obstacle的方式是：**signed distance field d(x)**。这种方法对于计算static的环境是很有效的，其本质上就是需要提前计算空间中每一个点到最近障碍物的距离。d(x)  x为空间中一点，如果x在障碍物内部，d(x)为-1，外部d(x)为1，边界为0。计算d(x)的方法为：利用EDT(Euclidean Distance Transform)。</p><p>同时将机器人的身体简化为一些列圆（或简单图形），这样可以方便的计算安全距离。</p><p>有了这些，就可以得到**workspace potential function c(x)**，其用来penalizes points of the robot for being near obstacles.</p><p>例如：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/lieNG17wdTsHqpL.png" alt="image-20210328203712719"></p><p>或更平滑的：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/ApMm8kVDJ3hqT67.png" alt="image-20210328203731905"></p><h3 id="Defining-an-obstacle-potential"><a href="#Defining-an-obstacle-potential" class="headerlink" title="Defining an obstacle potential"></a>Defining an obstacle potential</h3><p>现在已经有了workspace potential function c(x)，这个c(x)是对于robot上一点的描述，而为了对整个机器人产生一个cost，就需要结合机器人各个位置的c(x)。最直接的方式就是对于时间积分，但这样可能会产生让机器人在障碍物区域高速移动的趋势。</p><p>所以选择使用</p><p>最终的loss为：其中u为机器人身上的一点，B为机器人全部位置的点集。本质上来讲是对workspace的arc-length做积分。积分只与速度项和workspace position有关。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/03/29/ck7UV6phyLiWSxm.png" alt="image-20210329104547832"></p><h3 id="Smooth-Projection-for-Joint-Limits"><a href="#Smooth-Projection-for-Joint-Limits" class="headerlink" title="Smooth Projection for Joint Limits"></a>Smooth Projection for Joint Limits</h3><p>一般处理joint limit的时候，有以下两种方法：本篇paper选用后者。</p><ol><li><p>额外加入一个potential term（作为罚项，防止joint value接近limit）</p></li><li><p>当违背极限时，project到安全的区域。</p></li></ol><p>这里的projection是关于A matrix的norm（A matrix定义在Covariant gradient descent这一小节中）。即：如果超过limit则截断至limit，然后乘上A的inverse。这个transform的作用是起到smooth的效果。</p><h2 id="Experiments-on-Robotic-Arm"><a href="#Experiments-on-Robotic-Arm" class="headerlink" title="Experiments on Robotic Arm"></a>Experiments on Robotic Arm</h2><p>稍微修改了Collision heuristic（即稍微修改了workspace potential function c(x)的计算方法），很核心的一个参数就是A矩阵的选取。</p><h2 id="Reference-4"><a href="#Reference-4" class="headerlink" title="Reference"></a>Reference</h2><ol><li>L. Kavraki and J. Latombe. Probabilistic roadmaps for robot path planning. Practical Motion Planning in Robotics: Current Approaches and Future Directions, 53, 1998.</li><li>O. Brock and O. Khatib. Elastic Strips: A Framework for Motion Generation in Human Environments. The International Journal of Robotics Research, 21(12):1031, 2002.</li><li>J. A. Bagnell and J. Schneider. Covariant policy search. In Proceedings ofthe International Joint Conference on Artificial Intelligence (IJCAI), August 2003.</li><li>M. Zlochin and Y. Baram. Manifold stochastic dynamics for bayesian learning. Neural Comput., 13(11):2549–2572, 2001.</li></ol><h1 id="STOMP-Stochastic-trajectory-optimization-for-motion-planning"><a href="#STOMP-Stochastic-trajectory-optimization-for-motion-planning" class="headerlink" title="STOMP: Stochastic trajectory optimization for motion planning"></a>STOMP: Stochastic trajectory optimization for motion planning</h1><h1 id="Sampling-Based-Methods-for-Motion-Planning-with-Constraints-survey"><a href="#Sampling-Based-Methods-for-Motion-Planning-with-Constraints-survey" class="headerlink" title="Sampling-Based Methods for Motion Planning with Constraints (survey)"></a>Sampling-Based Methods for Motion Planning with Constraints (survey)</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>motion planning需要考虑无碰撞的条件下，如何产生从start configuration 到 goal configuration的路径。</p><p>而constraints motion planning则需要在满足约束的情况下（如：保持end-effecort的方向一只朝上）完成motion planning。</p><p>Constrained sampling-based methods are based upon two core primitive operations: </p><ol><li>sampling constraint-satisfying configurations</li><li>generating constraint-satisfying continuous motion</li></ol><p><strong>Motion Planning is a PSAPCE-hard problems</strong>, 复杂程度随空间维数的上升而上升。</p><p>Motion Planning的历史回顾</p><ol><li>potential field methods 缺点：很难跳出local mimum</li><li>另外一类就很接近search based methods。 这类方法很重要的一点在于A careful choice of resolution and heuristics is critical for efficient heuristic search</li><li>基于优化的方法（或者也可也看为对于整个trajectory的一种优化）。存在的问题在于tuning the penalty functions to avoid local minima and avoid paths that go through thin obstacles (as obstacle avoidance is relaxed) is challenging for robots with many degrees offreedom in complex scenes.</li><li>Sampling-based algorithms。Sampling-based methods可以很方便的与Constraints相结合，constraints can be easily incorporated into the core ofa sampling-based algorithmwithout affecting its method for solution finding.</li></ol><p><strong>关于约束：</strong></p><p>常见的就是Crave Tracking。可以用IK来求解，但其问题在于无法保证path continuity。</p><p>整体来说，geometric constraints increased the difficulty of the motion planning problem and required additional consideration for effective planning.</p><h2 id="MOTION-PLANNING-AND-CONSTRAINTS"><a href="#MOTION-PLANNING-AND-CONSTRAINTS" class="headerlink" title="MOTION PLANNING AND CONSTRAINTS"></a>MOTION PLANNING AND CONSTRAINTS</h2><p>假设约束为<strong>完整约束</strong>，这个约束有K的等式，可以降低原来configuration space的维数（降低了m维），原有维度为n（n=m+k）。</p><p>约束形成了一个：The constraint function defines an m-dimensional implicit constrained configuration space within the ambient configuration space</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Mobile-Manipulator-Paper-Reading.assets\image-20210416220043259.png" alt="image-20210416220043259"></p><p>所以motion planning with Constraints的这类问题可以写为：</p><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Mobile-Manipulator-Paper-Reading.assets\image-20210416220335318.png" alt="image-20210416220335318"></p><p><strong>Constraint Expression</strong></p><p>最常见Constraints就是end-effector constraints. 一些表示方法比如：Task-Space Region (tsrs) formulation、Task-Space Region Chains (tsrcs)、和一种更general的情形。</p><p><strong>Constraint Composition</strong></p><p>一种处理Constraints的方法就是Constraint Composition，将多个Constraints变为/编码为一个Constraints（相当于取并集），降低处理的难度。paper中列出了一些方法。即：Howmultiple constraints are encoded。</p><p>另外一种情况是，如果任务也是一种composition，且有多模态的性质。例如：间歇性接触（Intermittent contact）是操纵和有腿运动的重要组成部分，需要不断增加和消除约束。所以需要可以处理Constraints之间取交集的情况。这里也有一些解决方法，例如：利用图来处理different constraints之间的切换；也可以看为一种hierarchical的处理，先用离散度图结构判断处于哪个constraint modality，之后再用上面的geometric constrained planning即可。</p><h2 id="SAMPLING-BASED-MOTION-PLANNING"><a href="#SAMPLING-BASED-MOTION-PLANNING" class="headerlink" title="SAMPLING-BASED MOTION PLANNING"></a>SAMPLING-BASED MOTION PLANNING</h2><p>The general idea behind sampling-based planning is to avoid computing the free space exactly, and to instead sample free configurations and connect them to construct a tree/graph that approximates the connectivity ofthe underlying free space.</p><p>一般是：probabilistic completeness的。即：if a solution exists, the probability of finding a path goes to 1 with the number of samples generated by the algorithm. 最常见的还是两类：PRM（graph、mutil-query）和RRT（tree、single-query），当然还有一些其他的tree-based methods，例如：EST。但这些方法都有一些相同的components，这里列出了部分：</p><ol><li>Samplers。</li><li>Metrics &amp; nearest-neighbor data structures。即：The choice ofdistance measure</li><li>Local planner</li></ol><p><img src= "/img/loading.gif" data-lazy-src="D:\myBlog\hexo\source_posts\Mobile-Manipulator-Paper-Reading.assets\image-20210416224812435.png" alt="image-20210416224812435"></p><p>asymptotically optimal: the solution path will converge to the globally optimal solution over time </p><p>TODO:</p><p>Randomized kinodynamic planning</p><p>Sampling-based algorithms for optimal motion planning</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mobile-Manipulator-Paper-Reading&quot;&gt;&lt;a href=&quot;#Mobile-Manipulator-Paper-Reading&quot; class=&quot;headerlink&quot; title=&quot;Mobile Manipulator Paper Rea
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Mobile Manipulator" scheme="http://canVa4.github.io/tags/Mobile-Manipulator/"/>
    
      <category term="motion planning" scheme="http://canVa4.github.io/tags/motion-planning/"/>
    
  </entry>
  
  <entry>
    <title>RL Pushing</title>
    <link href="http://canva4.github.io/2021/02/07/RL-Pushing/"/>
    <id>http://canva4.github.io/2021/02/07/RL-Pushing/</id>
    <published>2021-02-07T13:37:59.000Z</published>
    <updated>2021-02-08T02:49:11.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RL-Pushing"><a href="#RL-Pushing" class="headerlink" title="RL Pushing"></a>RL Pushing</h1><p>This blog is a note for related papers and some stuff about our traget.</p><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>For now, goal is <strong>Pushing</strong>! More general, that is <strong>General contact-rich manipulation problems</strong>. 这类问题的特点就是对环境非常的敏感，而传统RL方法的一个特点是如果真实场景和训练是差距很大（环境是动态的），是很难得到一个很好地结果的。再者，对于这种与环境高度相关的问题，如何对一个动态的环境做出一个表示或者是一个理解也是至关重要的。目标就是来解决这样一个general的问题。</p><p>这类问题的一个实际例子就是<strong>pushing object</strong>。其中，目标是保持对象的直立姿势，使用机械臂+夹爪作为执行器（gripper），并将其堆到正确的随机的位置。 例如，推一杯水，水会晃，所以其质心是在变化的，同时不能让杯子倒下。而且环境的摩擦力也是不确定的。 当然，被推的物体也是不确定的，即: 被推的物体可能具有不规则的形状和质量分布，并且机器人可能会在物体上的任意点推动，这就导致动力学变得非常复杂。</p><p><strong>Challenges:</strong></p><h2 id="Paper-List"><a href="#Paper-List" class="headerlink" title="Paper List:"></a>Paper List:</h2><ol><li>COCOI: Contact-aware Online Context Inference for Generalizable Non-planar Pushing 2020</li></ol><h1 id="Paper-Notes"><a href="#Paper-Notes" class="headerlink" title="Paper Notes"></a>Paper Notes</h1><h2 id="COCOI"><a href="#COCOI" class="headerlink" title="COCOI"></a>COCOI</h2><p>Author: Zhuo Xu er al 2020</p><h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h3><p>Model as <strong>POMDP</strong>. </p><ul><li>observation: 单目相机捕获到的图像+gripper的高度和夹爪open/close</li><li>action: designated position and orientation of the gripper + the gripper open/close command + termination boolean</li><li>reward: 到达终点（与终点距离的小于阈值）收益为1</li></ul><p>下图为一个示例，对于该任务：被推的物体可能具有不规则的形状和质量分布，并且机器人可能会在物体上的任意点推动，最终目标位置也是不确定的</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20210207231410176.png" alt="image-20210207231410176"></p><h3 id="Model-Algorithm"><a href="#Model-Algorithm" class="headerlink" title="Model/Algorithm"></a>Model/Algorithm</h3><p>本部分将介绍具体COCOI的模型细节。</p><p>整体policy的学习使用Q-Learning的方法。</p><h4 id="Online-Context-Inference-COI"><a href="#Online-Context-Inference-COI" class="headerlink" title="Online Context Inference (COI)"></a>Online Context Inference (COI)</h4><p>Online Context Inference就是一个Online的、结合历史信息的Q-function的一个estimator。下图就是这个Q-function estimator的网络结构。</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20210208094455623.png" alt="image-20210208094455623"></p><p>模型的上半部分是当前时刻的State+action作为输入，下半部分是历史信息（图像+力传感器数据）作为输入。这里的FCN指全连接。下半部分的作用就是让模型能利用历史信息并将其编码为动态的上下文表示（dynamics context representation），从而使policy具有推断对象动态的能力。</p><p>NOTE: 名字相同的块是共享参数的</p><h4 id="The-Contact-aware-Sampling-Strategy"><a href="#The-Contact-aware-Sampling-Strategy" class="headerlink" title="The Contact-aware Sampling Strategy"></a>The Contact-aware Sampling Strategy</h4><p>本部分就是介绍如何从历史数据中来采样。主要包括两个问题：1. 使用多少个历史信息？ 2. 使用什么时刻信息？使用哪些？</p><ol><li>使用多少个历史信息？</li></ol><p>直观来讲，使用更多的历史信息可以减小方差，the policy has more information to infer a potentially less noisy object dynamics representation. 但同时会增加存储空间和时间。实验结果：<strong>使用3个比较好</strong></p><ol start="2"><li>使用什么时刻信息？使用哪些？</li></ol><p>显然完全随机的或者arbitrarily的方法可能会包含数量有限的信息。为了获取足够有意义（对于当前的context而言）的信息，提出了这个Contact-aware Sampling Strategy。实际上就是：它会主动检查安装在机器人夹具上的力扭矩传感器（force torque sensor），并且仅在接触力幅度很大（&gt; 1 N）时收集样本。这样可以保证我们采到的历史数据是gripper和object接触时的数据。此时的模型就是COCOI。</p><p>Q: why 1N ?</p><p>如下图所示：下图的VCOI指：history samples are retrieved with a uniform sampling interval。</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20210208095920331.png" alt="image-20210208095920331"></p><h3 id="Implement-amp-Experiment"><a href="#Implement-amp-Experiment" class="headerlink" title="Implement &amp; Experiment"></a>Implement &amp; Experiment</h3><p>首先是如何克服从仿真到真实世界间的gap？作者使用的是RetinaGAN，目标是generate synthetic images that look realistic with object-detection consistency.</p><p>效果如下：</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20210208100705802.png" alt="image-20210208100705802"></p><p><strong>仿真环境</strong>：<strong>PyBullet</strong>，Q: <em>In PyBullet, the contact physics between the robot gripper and the object is modelled using a point contact model with an elliptic friction cone.</em></p><p><strong>训练framework</strong>：Qt-opt。训练的早期使用：基于规则的策略（rule-based scripted policy），该策略将抓爪沿着连接对象和目标的直线移动，以生成成功的episode并提高探索效率。</p><p>训练结果：COCOI：本文模型。VCOI：使用随机采样策略。Baseline：去掉COCOI的下半部分（即不考虑历史信息）。Oracle：相比于baseline额外输入：两个unobservable dynamics parameters（摩擦系数，物体的质量）</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20210208102138633.png" alt="image-20210208102138633"></p><h3 id="Paper-Reference"><a href="#Paper-Reference" class="headerlink" title="Paper Reference"></a>Paper Reference</h3><ol><li>Qtopt: Scalable deep reinforcement learning for vision-based robotic manipulation 2018</li><li>Quantile qt-opt for risk-aware vision-based robotic grasping 2019 </li><li>Retinagan: An object-aware approach to sim-to-real transfer 2020 （本文中用于克服sim-to-real的gap）</li><li>Pybullet, a python module for physics simulation for games, robotics and machine learning 2016</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RL-Pushing&quot;&gt;&lt;a href=&quot;#RL-Pushing&quot; class=&quot;headerlink&quot; title=&quot;RL Pushing&quot;&gt;&lt;/a&gt;RL Pushing&lt;/h1&gt;&lt;p&gt;This blog is a note for related papers
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="RL" scheme="http://canVa4.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Mobile Manipulator Intro &amp; Resource</title>
    <link href="http://canva4.github.io/2021/01/27/Mobile-Manipulator-Intro-Resource/"/>
    <id>http://canva4.github.io/2021/01/27/Mobile-Manipulator-Intro-Resource/</id>
    <published>2021-01-27T15:15:35.000Z</published>
    <updated>2021-03-04T04:17:56.536Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mobile-Manipulator-Intro-amp-Resource"><a href="#Mobile-Manipulator-Intro-amp-Resource" class="headerlink" title="Mobile Manipulator Intro &amp; Resource"></a>Mobile Manipulator Intro &amp; Resource</h1><p>关于Mobile Manipulator的一些偏向于survey的介绍和一些资源汇总。</p><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><p>主要是survey相关的paper</p><ol><li>Mobile Manipulation Tutorial 2020</li><li>Ros navigation tuning guide 2017123</li></ol><h1 id="Mobile-Manipulator-Intro"><a href="#Mobile-Manipulator-Intro" class="headerlink" title="Mobile Manipulator Intro"></a>Mobile Manipulator Intro</h1><p>组成：robotics arm + mobile robotic platform</p><p>最简单、最常见的具体形式：</p><ul><li>wheeled base 底盘</li><li>6 自由度（DoF）机械臂</li><li>平行二指夹爪</li></ul><p>典型例子：Fetch：7DoF机械臂，躯干可提升，pitch, yaw head with an RGB-D camera &amp; 2D Laser Scanner.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/01/28/V6ANwU5pyzWk4jS.png" alt="image-20210127233150657"></p><p>其他常见传感器</p><ul><li>激光雷达：用于mapping</li><li>RGB-D Camera：用于object recognition and localization</li></ul><p>因为目前短期内的任务是尽快的实现一个在虚拟环境的task，所以这里重点关注一些现有的、已经发行的公开的软件工具包和仿真器。</p><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>最简单也最经典的任务：<strong>Fetch  &amp; Place</strong></p><p>为做简单处理，将path planning, arm planning和task planning分开进行，现在一些更新的方法是将它们结合进行规划，也是我们最终的目标。</p><p>因此，我们可以将问题分为移动机器人部分（负责将机器人驱动到某个位置），操纵部分（寻找要拾取的对象和相应的手臂动作）。当然也要包括简单的人机交互部分。</p><p>对于一个最简单的Fetch &amp; Place的task，常见的流程如下：</p><ol><li>drive to a given location</li><li>identify and grasp a specified object</li><li>drive to a different location</li><li>place the object on a specific QR code（目前使用QR code来定位是最最常见的方案）</li></ol><p>这些流程可能要使用下图中所使用的技术</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/01/28/kFdwYafePO8bp3c.png" alt="image-20210127232713306"></p><h2 id="常见仿真器"><a href="#常见仿真器" class="headerlink" title="常见仿真器"></a>常见仿真器</h2><ol><li>Gazebo–&gt; Most common used, popularity, tight integration of ROS ！</li><li>CoppeliaSim</li><li>Webots</li><li>SAPIEN</li></ol><h2 id="软件包"><a href="#软件包" class="headerlink" title="软件包"></a>软件包</h2><table><thead><tr><th>功能</th><th>Package</th></tr></thead><tbody><tr><td>Localization</td><td>AMCL等</td></tr><tr><td>Local Costmap + Path Planning +Path Flow</td><td>ROS Navigation</td></tr><tr><td>Arm Control（驱动层面的控制）</td><td>kinova_ros, fetch_ros等</td></tr><tr><td>object Detection</td><td>AprilTags，NOCS</td></tr><tr><td>object Place Pose</td><td>AprilTags</td></tr><tr><td>Arm Planning &amp; Inverse Kinematics</td><td>MoveIt</td></tr><tr><td>Grasp Planning（假设使用两指夹爪，相对简单）</td><td>—-</td></tr><tr><td>Decision Making (State machine)</td><td>FlexBE， SMACH</td></tr><tr><td>Simulator</td><td>Gazebo</td></tr><tr><td>SLAM 2D(激光雷达)</td><td>Cartographer，gmapping</td></tr><tr><td>Camera Calibration</td><td>TODO</td></tr><tr><td>Hand eye Calibration</td><td>TODO</td></tr></tbody></table><p>整体的设计逻辑使用<strong>有限状态自动机</strong>。具体行为的设计可以使用FlexBE或SMACH。</p><p>假设：环境是已知的。</p><p>对于环境的建模，就可以使用SLAM（这里给定的task环境是建好的），常见的包如：gmapping和Cartograhper。</p><p>整个路径规划的部分可以市容ROS Navigation。其中包含以下部分：Local Costmap: 为了驱动到地图中指定的2D坐标，ROS导航正在构建成costmap（global和local），将提供的地图与当前扫描得到数据结合在一起。 然后，global path planner规划到坐标的路径，同时考虑机器人的运动学约束（机器人底盘的差动方式和机器人尺寸）。local path planner则主要负责避开在global planner规划的路径上的障碍。</p><p>当机器人达到目标地点，开始执行pick的任务时，这时候一般就需要适用到RGB-D相机了。一般来讲：彩色图像用于识别物体，而深度数据对于计算物体的6D位姿非常重要。</p><p>对于object detection可以使用：AprilTags（有ROS的包），NOCS（2019年新方法，也有ROS的包）<strong>算法细节TODO</strong></p><p>下图为AprilTags</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/01/28/KL1ZCDkIze5jtJX.png" alt="image-20210128111936725"></p><p>下图为：NOCS</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/01/28/mUr4CDvX326QiTP.png" alt="image-20210128105854315"></p><p>一旦检测到物体的方位，就要进行grasp planning了，即找到一个最优的方法来抓取目标物体。因为这里使用的是两指夹爪，最简单方案就是对齐中心，然后抓就可以了。更advanced的算法和包：<strong>TODO</strong>。</p><p>arm planning &amp; control：ROS MoveIt。最主流的实现方法，需要知道机械臂的参数、碰撞信息等。其本质功能就是进行运动学反解（Inverse Kinematics）。细节：<strong>TODO</strong></p><p>一些待调研的问题：</p><ul><li>相机校准</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2021/01/28/z9mySbJh5nkI6jA.png" alt="image-20210128111547824"></p><ul><li>手眼协调</li><li>…</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mobile-Manipulator-Intro-amp-Resource&quot;&gt;&lt;a href=&quot;#Mobile-Manipulator-Intro-amp-Resource&quot; class=&quot;headerlink&quot; title=&quot;Mobile Manipulator
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Mobile Manipulator" scheme="http://canVa4.github.io/tags/Mobile-Manipulator/"/>
    
  </entry>
  
  <entry>
    <title>Note for Dilated Conv</title>
    <link href="http://canva4.github.io/2020/12/03/Note-for-Dilated-Conv/"/>
    <id>http://canva4.github.io/2020/12/03/Note-for-Dilated-Conv/</id>
    <published>2020-12-03T02:23:16.000Z</published>
    <updated>2020-12-11T13:48:01.529Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Note-for-Dilated-Conv"><a href="#Note-for-Dilated-Conv" class="headerlink" title="Note for Dilated Conv"></a>Note for Dilated Conv</h1><p>最近在做edge detection类型的任务，在edge detection领域，也开始使用了很多semantic segmentation的方法；其中一类就是Dilated Con. 这里特此汇总一下相关的学习资源，和自己在阅读和学习时的心得。</p><h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a>Resources:</h1><p>各种资源汇总：</p><h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs:"></a>Blogs:</h2><ol><li>总结-空洞卷积(Dilated/Atrous Convolution)  <a href="https://zhuanlan.zhihu.com/p/50369448">知乎专栏LINK</a> 【深度好文】 包含了Dilated Conv问题的讨论。</li><li>如何理解空洞卷积（dilated convolution）？ <a href="https://www.zhihu.com/question/54149221/answer/323880412">知乎提问LINK</a></li><li>Semantic Segmentation学习流程 <a href="https://zhuanlan.zhihu.com/p/145009250">https://zhuanlan.zhihu.com/p/145009250</a> <a href="https://zhuanlan.zhihu.com/p/76603228">https://zhuanlan.zhihu.com/p/76603228</a>  <a href="https://zhuanlan.zhihu.com/p/27794982">https://zhuanlan.zhihu.com/p/27794982</a></li><li>Object Detection–RCNN,SPPNet,Fast RCNN，FasterRCNN论文详解 <a href="https://blog.csdn.net/u011974639/article/details/78053203#sppnet">https://blog.csdn.net/u011974639/article/details/78053203#sppnet</a></li><li>Witnessing the Progression in Semantic Segmentation: DeepLab Series from V1 to V3+ <a href="https://towardsdatascience.com/witnessing-the-progression-in-semantic-segmentation-deeplab-series-from-v1-to-v3-4f1dd0899e6e?source=rss----7f60cf5620c9---4">LINK</a></li></ol><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h2><ol><li>Multi-Scale Context Aggregation by Dilated Convolutions 2016 <a href="https://arxiv.org/abs/1511.07122">LINK</a></li><li>Dilated Residual Networks 2017 <a href="https://arxiv.org/abs/1705.09914">LINK</a></li><li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 2017 <a href="https://arxiv.org/abs/1606.00915">LINK</a></li><li>Rethinking Atrous Convolution for Semantic Image Segmentation 2017 <a href="https://arxiv.org/abs/1706.05587">LINK</a></li><li>Understanding Convolution for Semantic Segmentation 2018 <a href="https://arxiv.org/abs/1702.08502">LINK</a></li><li>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018</li><li>LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation 2019</li><li>RefineNet</li><li>V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral<br>neural networks. In CVPR, 2016. 一种独特的卷积方式 (bilateral convolution)</li></ol><h2 id="Others"><a href="#Others" class="headerlink" title="Others:"></a>Others:</h2><ol><li>《A guide to convolution arithmetic for deep learning》 and gifs <a href="https://github.com/vdumoulin/conv_arithmetic">github LINK</a></li></ol><h1 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h1><p>本部分是上面资源or paper学习后的note.</p><h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading:"></a>Paper Reading:</h2><p>本部分是一些经典paper阅读后的note.</p><h3 id="Multi-Scale-Context-Aggregation-by-Dilated-Convolutions-2016"><a href="#Multi-Scale-Context-Aggregation-by-Dilated-Convolutions-2016" class="headerlink" title="Multi-Scale Context Aggregation by Dilated Convolutions 2016"></a>Multi-Scale Context Aggregation by Dilated Convolutions 2016</h3><p>Author: Fisher Yu 2016  <a href="https://arxiv.org/abs/1511.07122">LINK</a>  第一篇提出空洞卷积的方法。空洞卷积是下面的那篇BDCN使用到了，这也是我第一次了解这个卷积操作。</p><p>Journal: ICLR 2016</p><p><strong>提出Dilated Conv的paper. 一种专门design for dense prediction（例如：semantic segmentation）的方法</strong></p><p>这种空洞卷积（Dilated Conv）<strong>support exponential expansion of the receptive field without loss of resolution or coverage!</strong></p><p>作者还指出，将这种Dilated Conv可以作为插入模块，来提升这种dense prediction的性能。</p><p><strong>总结：</strong></p><p>Dilated Conv的好处：</p><ul><li><strong>扩大感受野</strong>：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</li><li><strong>捕获多尺度上下文信息：</strong>空洞卷积有一个参数可以设置dilation rate，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度（multi-scale）信息。</li></ul><p>所以我们可以对于一个feature map使用几组不同dilation rate和padding的dilated Conv来让获取multi-scale信息，之后再concatenate并过一个conv聚合并学习这样的multi-scale的信息。</p><p><strong>而语义分割（semantic segmentation）由于需要获得较大的分辨率图，因此经常在网络的最后两个stage，取消降采样操作，之后采用空洞卷积弥补丢失的感受野。</strong></p><p><strong>背景：</strong></p><p>在图像分割领域，图像输入到CNN（典型的网络比如FCN）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像（feature）尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。</p><p><strong>Detail About Dilated Conv：</strong></p><p>对于公式化卷积的过程可以如下所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/x4Upe38VFKAMzLm.png" alt="image-20201202204630447"></p><p>其使用的卷积核与普通CNN一直，只不过对于每一个Dilated Conv存在一个factor l.</p><p>下图就是Dilated Conv的一个直观实例，也显示出了其特点：<strong>The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly.</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/V1mB5KvAc8dSpMr.png" alt="image-20201202210406002"></p><p>dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv。</p><p>下图是传统的卷积：或者说是1-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/03/kUItGT1vOV3oZrd.png" alt="image-20201202211039151"></p><p>而下图是2-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/ojTmcJvQiIMV8ep.png" alt="image-20201202211056595"></p><p><strong>值得注意的是这里的这里对于filter的初始化不能是随机的。</strong></p><h3 id="DeepLab-V2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-2017"><a href="#DeepLab-V2-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-2017" class="headerlink" title="DeepLab V2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 2017"></a>DeepLab V2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 2017</h3><p>Author: Liang-Chieh Chen 2017 <a href="https://arxiv.org/abs/1606.00915">LINK</a>  DeepLab系列方法的v2 是semantic segmentation领域的一个较为经典的方法，其中使用了dilate conv(即：Atrous Conv)。</p><p>Journal\Conference: TPAMI 2017</p><p>Other‘s Blog:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/37645755">https://zhuanlan.zhihu.com/p/37645755</a></li><li><a href="https://blog.csdn.net/u011974639/article/details/79138653#t8">https://blog.csdn.net/u011974639/article/details/79138653#t8</a></li></ol><p>本文主要的3个核心点在于：</p><ol><li>We highlight convolution with upsampled filters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks.（dilate conv的细节见上文）</li><li>提出了atrous spatial pyramid pooling (ASPP)，这个模块（是SSP Spatial Pyramid Pooling的改进）。其可以提取multi-scale的信息。</li><li>使用条件随机场改善localization of object boundarie。可以提升localization performance（虽然这种方法现在并不经常使用了，但也是直观的能够改善结果的trick）DCNN和CRF的组合不是新话题，以前的paper着重于应用局部CRF，这忽略像素间的长期依赖。而DeepLab采用的是全连接的CRF模型，其中高斯核可以捕获长期依赖性，从而得到较好的分割结果。 <strong>CRF的部分还未详细了解，之后会单独做一个Blog</strong></li></ol><p>作者还给出了两种实现空洞卷积的方法（当然现在空洞卷积已经被implement至各大API中了）：</p><ul><li>第一个是通过插入空洞(零)来隐含地对滤波器进行上采样，或等效稀疏地对输入特征图进行采样。通过向im2col函数(从多通道特征图中提取矢量化块)添加稀疏采样底层特征图实现了这一点。</li><li>第二种方法，用一个等于空洞卷积率r等效的因子对输入特征图下采样，对于每一个r×r的移位，都对其进行去交织以产生$r^2$大小的的分辨率映射。然后将标准卷积应用于这些中间特征图，并隔行扫描生成原始图像分辨率。通过将多孔卷积变换为常规卷积，可以使用现成的高度优化的卷积方法。作者已经在TensorFlow框架中实现了第二种方法。</li></ul><p>其整体的模型如下图所示：</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201206221537132.png" alt="image-20201206221537132" style="zoom:80%;" /><p>比较有亮点的就是atrous spatial pyramid pooling (ASPP)了，这个模块在之后的许多模型中都有使用。</p><p>首先要提一下这个方法的灵感来源，即：spatial pyramid pooling（SPP），是在object detection领域提出的方法，作用是来处理region proposal获得的区域不一样，导致之后进入CNN输出的结果不一样，无法使用全连接层，使用SSP就可以让任意size的input输出相同大小的output（使用多组不同大小的卷积）。如下图所示。</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201206224720094.png" alt="image-20201206224720094" style="zoom:80%;" /><p>作者在文中尝试了两种方法来处理语义分割中的尺度变换：</p><ul><li>第一种方法相当于<strong>标准多尺度处理</strong>。（现在已经out了）将原始图像放缩为不同的大小，分别输入到使用相同参数的多个DCNN中，融合score map得到预测结果。为了产生最终的结果，对并行DCNN分支特征图进行双线性插值使其恢复到一定的分辨率，并融合它们，在不同尺度上获取每个位置的最大响应。在训练和测试期间都这样做。多尺度处理显著提高了性能，但代价是需要在输入图像的多个尺度上对所有DCNN层计算特征响应（计算量大）。</li><li>第二种方法受SPPNet中SPP模块的的启发，它指出在任意尺度的区域，可以用从单个尺度图像中进行重采样提取的卷积特征进行准确有效地分类。我们用不同采样率的多个并行的空洞卷积实现了他们的方案的一个变体。并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构。所提出的“多孔空间金字塔池化”(DeepLab-ASPP)方法泛化了DeepLab-LargeFOV变体。</li></ul><p>DeepLab v2的做法与SPPNet类似，并行的采用多个采样率的空洞卷积提取特征，再将特征融合，类似于空间金字塔结构，形象的称为Atrous Spatial Pyramid Pooling (ASPP)。示意图如下：</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201206223200072.png" alt="image-20201206223200072" style="zoom:80%;" /><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201206224914371.png" alt="image-20201206224914371" style="zoom: 80%;" /><p>即：在同一<code>Input Feature Map</code>的基础上，并行的使用4个空洞卷积，空洞卷积配置为r = { 6 , 12 , 18 , 24 }，核大小为3 × 3。最终将不同卷积层得到的结果做pixel level sum到一起。</p><p><strong>Conclusion</strong></p><p>DeepLabv2将空洞卷积应用到密集的特征提取，进一步的提出了空洞卷积金字塔池化结构、并将DCNN和CRF融合用于细化分割结果。实验表明，DeepLabv2在多个数据集上表现优异，有着不错的分割性能。</p><h3 id="Dilated-Residual-Networks-2017"><a href="#Dilated-Residual-Networks-2017" class="headerlink" title="Dilated Residual Networks 2017"></a>Dilated Residual Networks 2017</h3><p>Author: Fisher Yu 2017 <a href="https://arxiv.org/abs/1705.09914">LINK</a></p><p>Journal\Conference: CVPR 2017</p><p>基本来讲就是<strong>Dilated Conv + Residual network（残差网络）</strong>。本篇文章的模型并不复杂，<strong>但是其在文中详细指出了Dilated Conv的存在问题: Gridding!</strong></p><p><strong>模型结构（Dilated Residual Network）</strong></p><p>首先我们看看ResNet，它大致可分为6个阶段，即 conv1~5加上最后的分类层：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/11/ITD8LBNq9whlt1x.jpg" alt="img"></p><p>其中conv2~5为4个分别由相同结构、规模的Residual Block堆砌而成，每个阶段都会进行 stride=2 的下采样(striding)，其带来的作用是：</p><ol><li>feature maps 尺寸在长宽上都减半</li><li>令下一层的receptive field翻倍</li></ol><p>作者认为feature maps尺寸的快速衰减容易造成信息的损失，是不合理，为了解决这个问题，而不改变下一层的receptive field，作者使用dilate conv来代替部分卷积层。</p><p>即如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/11/PeqJCRLAHiglXnb.png" alt="image-20201211204416266"></p><p>具体来说，就是：</p><ol><li>在conv4、conv5阶段不下采样，feature maps 尺寸相较conv3 不变。</li><li>由于原ResNet 的conv4视野相对于conv3是两倍，conv5是conv3的四倍，为了弥补视野行的缺陷，如上图 DRN 在 conv4 设置卷积的 dilation=2，conv5 的dilation=4，卷积核大小依然为3x3。</li></ol><p><strong>Dilated Conv的问题 Gridding and Degridding</strong></p><p>这样设计在不增加模型参数量的情况下，提高了模型对小物体的识别精度，但存在一个的问题，作者称之为 <strong>gridding</strong>。</p><p>Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution.</p><p>下图就是整个网络出现<strong>gridding</strong>实例，DRN-A-18就是上文所提出的模型（未针对gridding问题进行修正）的结果。可以看到其activation map呈现了一种grid pattern.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/11/hjkyfURnqiH7CNS.png" alt="image-20201211212949106"></p><p>一个小问题是，上面这张图是怎么得到出来的？利用的是NIN里的 AvgPooling + conv1x1 替代全连接层输出分类。这样可以使得模型参数量大大减少的同时，提高模型的精度，所以许多模型都采用这种方式输出预测分类，而这样训练出来的模型，将 AvgPooling 取消，对于 h*w 大小的 feature map 上的每一点，即 shape 为 (1,1,c) 的 tensor 使用原有的 conv1x1，</p><p>下图是另一个此现象的示例：In Figure 4(a), the input feature map has a single active pixel. A 2-dilated convolution (Figure 4(b)) induces a corresponding grid pattern in the output (Figure 4(c)).</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/11/ifZ47vJ8FGXKxtM.png" alt="image-20201211213300083"></p><p>作者为了解决这个问题，设计了一种degridding方法，如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/11/7Cq2ABWz1KX5jxH.png" alt="image-20201211213418407"></p><p>绿色的线表示一个downsampling by stride 2. 同一个level的layer包含同样的dilation和空间分辨率。</p><p>主要来讲，从A-&gt;B-&gt;C包含如下操作</p><ol><li>Removing max pooling. 原因：这种最大池化操作会导致出现high-amplitude high-frequency activations，其会传播到后面的层，并加剧这种gridding的现象。</li><li>Adding layers. </li><li>Removing residual connections.</li></ol><h3 id="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-2018"><a href="#Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-2018" class="headerlink" title="Rethinking Atrous Convolution for Semantic Image Segmentation 2018"></a>Rethinking Atrous Convolution for Semantic Image Segmentation 2018</h3><p>Author: Liang-Chieh Chen 2018 <a href="https://arxiv.org/abs/1706.05587">LINK</a>  也就是DeepLab V3</p><p>Journal\Conference: arXiv 2018</p><p>作者首先提到了semantic segmentation这类dense prediction任务的困难之处。</p><p>通常语义分割面临两个挑战：1）由于池化和卷积的降采样操作，导致分辨率的大大降低。atrous convolution的使用可以减缓这个问题。2）样本集中存在不同尺度的目标。作者总结目前（截止至2018年）有以下四类方法来减缓“存在不同尺度的目标”的问题：a）多尺度输入。b）the encoder-decoder structure 。取encoder网络不同分辨率的feature maps，在decoder网络中逐步进行上采样恢复到原来的分辨率。c）在原有的网络后面再串联一个额外的模块，比如串联CRF模块或者额外卷积层。d）spatial pyramid pooling , probes an incoming feature map with filters or pooling operations at multiple rates and multiple effective field-of-views, thus capturing objects at multiple scales.</p><p>如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/10/2oeD7OVqiY1pfbw.png" alt="image-20201210202417469"></p><p>在本文中，使用一组同rate的Dliated Conv和batch normalize（通过实验，发现效果好）。</p><p>并用其作为捕获上下文信息的模块以及建立空间金字塔池化结构的工具。具体来说，复制ResNet中的最后几个block，将它们级联，同时使用包含多个带孔卷积并行的ASPP模块（如图d）。本文的级联模块直接应用于feature map而不是belief map（应该是softmax的最终结果）。对于给定的模块，本文在实验中发现加入batch normalization去训练的效果更好。为了更好地捕获全局上下文信息，本文提出使用图像层次的特征来强化ASPP。</p><p>We discuss an important practical issue when applying a 3 × 3 atrous convolution with an extremely large rate, which fails to capture long range information due to image boundary effects, effectively simply degenerating to 1 × 1 convolution, and propose to incorporate image-level features into the ASPP module.</p><p><strong>Going Deeper with Atrous Convolution：</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/10/yqUHeFKRBdcGAwS.png" alt="image-20201210215634223"></p><p>上面的使用的为resnet网络。这里的output_stride值的是the ratio of input image spatial resolution to final output resolution.    </p><p><strong>这里另一个值得注意的点是（dilate conv的缺点）：</strong></p><p>具有不同dilate rate的ASPP可以有效地捕获多尺度信息。然而，作者发现随着采样率变大，有效滤波器权重的数量（即权重施加到有效特征区域而不是填充零点的区域）变小。如当把一个3×3的具有不同孔的比率的卷积核应用到65×65特征图上时（效果如下图所示）。在比率值接近于特征图大小的极端情况下，3×3滤波器，不再是捕获整个图像上下文，而是退化为简单的1×1滤波器，因为只有中心滤波器的权重才有效。  </p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/10/jqrhuaAs94Z1ONg.png" alt="image-20201210221451130"></p><p>这就是dilate conv 的一个缺点和问题所在。</p><p>作者为了克服上面这个问题并将全局上下文信息整合到模型中，作者采用了image-level features。具体来说，作者在模型的最后一个特征图上采用了global average pooling，将生成的图像级特征输入到256个1×1过滤器（加入batch normalization）中，然后bilinearly upsample the feature to the desired spatial dimension。最后，作者改进ASPP，a)当输出步长等于16，ASPP包括一个1×1卷积和三个3×3卷积，其中3×3卷积的dilate rate为(6,12,18)（所有的滤波器个数为256且加入了batch normalization）。</p><p>b)image-level features. 注意，当输出步长等于8时，rate加倍。在产生最终logits（不知道咋翻）的1×1卷积之前，所有分支的结果特征被连接并通过另一个1×1卷积（也有256个滤波器和加入了batch normalization）。</p><p>如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/10/QWldehRygKMcjE3.png" alt="image-20201210222105773"></p><h2 id="Others-1"><a href="#Others-1" class="headerlink" title="Others:"></a>Others:</h2><h3 id="Coding-For-Dliated-Conv"><a href="#Coding-For-Dliated-Conv" class="headerlink" title="Coding For Dliated Conv"></a>Coding For Dliated Conv</h3><p>若使用pytorch的话，非常简单，在现有的conv2d中已经支持了参数Dilation这个参数，当其为1是（默认值），此时就是正常的卷积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>, dilation=<span class="number">1</span>)  *<span class="comment"># 普通卷积* </span></span><br><span class="line"></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>, dilation=<span class="number">2</span>)  *<span class="comment"># dilation就是空洞率，即间隔*</span></span><br></pre></td></tr></table></figure><p>其本质可以看成是对于filter的一个扩展，将filter中间填入0。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/03/RBUs4ZoV8D5Xynh.png" alt="image-20201203103831461"></p><h3 id="How-to-Compute-the-Output-Size-of-Dilated-Conv？"><a href="#How-to-Compute-the-Output-Size-of-Dilated-Conv？" class="headerlink" title="How to Compute the Output Size of Dilated Conv？"></a>How to Compute the Output Size of Dilated Conv？</h3><p>首先回顾一下标准CNN的计算，设输入feature map尺寸为$(H,W)$，卷积核的大小为: $(K_h,K_w)$, stride为: $(S_h, S_w)$, Padding为 P.</p><p>所以标准CNN的输出size就是:</p><p>$$H_{new} = \frac{H-K_h+2P}{S_h} + 1, W_{new} = \frac{W-K_w+2P}{S_w} + 1$$</p><p>而Dilated Conv可以看成将卷积核填充0. 设 dilatation rate  = r.</p><p>正常情况下$H=W=F, K_h=K_w=K, S_h=S_w=S$</p><p>所以新的kernel的大小为: $K_{new} = K + (K-1)(r-1)$，所以dilated Conv的输出size为:</p><p> $$F_{new} = \frac{F-K_{new}+2P}{S}+1 = \frac{F-(K + (K-1)(r-1))+2P}{S}+1$$</p><h3 id="How-to-Compute-the-Receptive-Field-of-Diltaed-Conv"><a href="#How-to-Compute-the-Receptive-Field-of-Diltaed-Conv" class="headerlink" title="How to Compute the Receptive Field of Diltaed Conv?"></a>How to Compute the Receptive Field of Diltaed Conv?</h3><p>A Blog’s Link <a href="http://shawnleezx.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/">LINK</a></p><p>CNN感受域的的计算如下：从这个公式可以看到，相比前一层，当前层的感受野大小在两层之间增加，这是一个指数级增加。</p><p>$$F_i=(F_{i-1}-1) \cdot stride+K_{size}$$</p><p>$F_i$为第i层的感受野，stride为第i层的stride，$K_{size}$为卷积核或池化核尺寸。</p><p>对于Dilated Conv，其本质可以理解为在卷积核里补充0，即将原始的K变为$K_{new} = K + (K-1)(r-1)$，再带入公式计算即可。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/wqpZ3C2Ktms9JYf.png" alt="image-20201203112630801"></p><h3 id="Problems-of-Dilated-Conv"><a href="#Problems-of-Dilated-Conv" class="headerlink" title="Problems of Dilated Conv"></a>Problems of Dilated Conv</h3><p>是的，空洞卷积是存在理论问题的，论文中称为gridding，其实就是网格效应/棋盘问题。因为空洞卷积得到的某一层的结果中，邻近的像素是从相互独立的子集中卷积得到的，相互之间缺少依赖。</p><ul><li><strong>局部信息丢失</strong>：由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。</li><li><strong>远距离获取的信息没有相关性</strong>：由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。</li></ul><p>一些解决方法：</p><ul><li>Panqu Wang,Pengfei Chen, <em>et al**</em>.Understanding Convolution for Semantic Segmentation.//**WACV 2018</li><li>Fisher Yu, <em>et al</em>. <strong>Dilated Residual Networks.</strong> //CVPR 2017</li><li>Zhengyang Wang,<em>et al</em>.**Smoothed Dilated Convolutions for Improved Dense Prediction.//**KDD 2018.</li><li>Liang-Chieh Chen,<em>et al**</em>.Rethinking Atrous Convolution for Semantic Image Segmentation//2017**</li><li>Sachin Mehta,<em>et al</em>. <strong>ESPNet: Efficient Spatial Pyramid of DilatedConvolutions for Semantic Segmentation.</strong> //ECCV 2018</li><li>Tianyi Wu,et al.<strong>Tree-structured Kronecker Convolutional Networks for Semantic Segmentation.//AAAI2019</strong></li><li>Hyojin Park,et al.<strong>Concentrated-Comprehensive Convolutionsfor lightweight semantic segmentation.//2018</strong></li><li>Efficient Smoothing of Dilated Convolutions for Image Segmentation.//2019</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Note-for-Dilated-Conv&quot;&gt;&lt;a href=&quot;#Note-for-Dilated-Conv&quot; class=&quot;headerlink&quot; title=&quot;Note for Dilated Conv&quot;&gt;&lt;/a&gt;Note for Dilated Conv&lt;/
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Works/Papers/"/>
    
    
      <category term="Segmentation" scheme="http://canVa4.github.io/tags/Segmentation/"/>
    
      <category term="Deep Learning" scheme="http://canVa4.github.io/tags/Deep-Learning/"/>
    
      <category term="CV" scheme="http://canVa4.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Model Compression Paper Reading</title>
    <link href="http://canva4.github.io/2020/11/28/Model-Compression-Paper-Reading/"/>
    <id>http://canva4.github.io/2020/11/28/Model-Compression-Paper-Reading/</id>
    <published>2020-11-28T03:16:38.000Z</published>
    <updated>2020-11-28T16:53:28.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Model-Compression-Paper-Reading"><a href="#Model-Compression-Paper-Reading" class="headerlink" title="Model Compression Paper Reading"></a>Model Compression Paper Reading</h1><p>Last note is a brief overview of model compression and a summary of learning resources.</p><p>This note is focus on some specific papers and methods.</p><h2 id="Total-LIST"><a href="#Total-LIST" class="headerlink" title="Total LIST:"></a>Total LIST:</h2><ol><li>Distilling the Knowledge in a Neural Network 2015 知识蒸馏开山之作</li></ol><h1 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h1><p>本部分主要关注Knowledge Distillation(简记为KD).</p><p><strong>LIST:</strong></p><ol><li>Distilling the Knowledge in a Neural Network 2015</li></ol><h2 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a>Distilling the Knowledge in a Neural Network</h2><p>Author: Geoffrey Hinton er al  <a href="https://arxiv.org/pdf/1503.02531.pdf">LINK</a></p><p>Conference: CVPR 2015</p><p>KEY WORDS: <strong>知识蒸馏(Knowledge Distillation )开山之作</strong></p><p>知识蒸馏的目的就是将大模型原有的知识，让一个小模型也学会。</p><p>正常我们在训练一个图片分类的任务中最后会使用cross entropy这个损失函数，最后学的模型可以的输出一个概率分布（softmax的结果），真实label出现的概率最大，其他的概率相对很小。这个结果，我们成为soft target，他其中包含了比原始label更多的信息，比如在MNIST上可能不大合适。因此，一个网络训练好之后，对于正确的答案会有一个很高的置信度。例如，在MNIST数据中，对于某个2的输入，对于2的预测概率会很高，而对于2类似的数字，例如3和7的预测概率为1e-6 和 1e-9。显然soft target包含了更多的信息。</p><p>但是如果直接学习这个概率分布，其小类别的概率太小了，作者引入了<strong>temperature</strong>来解决这个问题，直观的来讲，就是可以将小类别的概率输出的结果变大，但仍然小于真实label的概率。</p><p>下图就是引入temperature后的softmax. T越大会产生更softer的分布。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/28/QDOZEz7my8S1Jjo.png" alt="image-20201128233002251"></p><p>如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。</p><p>整体的过程就是先训练好一个teacher，之后将teacher的softmax换用带temperature的softmax，并将训练数据输入，记录下来训练数据的结果。再使用训练数据训练student网络，student网络也使用相同的temperature。训练好后，在去除temperature，即将temperature设为1。</p><p>当soft target具有较高的熵时，与硬目标（真实label）相比，它们在每个训练样本中提供的信息更多，并且训练样本之间的梯度差异较小，因此，相比于原始的繁琐模型，小型模型通常可以在少得多的数据上进行训练，并且使用更高的学习率。</p><p><strong>Loss</strong></p><p>如果我们也知道训练数据的真实label，只用这种加权和形式的损失函数会有更好的效果。训练小网络时的loss为：</p><p>$$L = CE(y,p) + \alpha CE(q,p)$$</p><p>在这篇论文中，作者认为可以将模型看成是黑盒子，知识可以看成是输入到输出的映射关系。因此，我们可以先训练好一个teacher网络，然后将teacher的网络的输出结果 <strong>q</strong> 作为student网络的目标，训练student网络，使得student网络的结果 <strong>p</strong>  接近 <strong>q</strong> 。这里CE是交叉熵（Cross Entropy），<strong>y</strong>是真实标签，对于分类任务就是one-hot编码，q是teacher网络的输出结果，p是student网络的输出结果。</p><p><strong>Summary：</strong></p><p>知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近ensemble的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Model-Compression-Paper-Reading&quot;&gt;&lt;a href=&quot;#Model-Compression-Paper-Reading&quot; class=&quot;headerlink&quot; title=&quot;Model Compression Paper Readin
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Model Compression" scheme="http://canVa4.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>Model Compression overview and resources</title>
    <link href="http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/"/>
    <id>http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/</id>
    <published>2020-11-19T12:47:47.000Z</published>
    <updated>2020-11-30T08:05:33.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Model-Compression-Overview-and-Resources"><a href="#Model-Compression-Overview-and-Resources" class="headerlink" title="Model Compression Overview and Resources"></a>Model Compression Overview and Resources</h1><p>此NOTE主要记录一些关于model compression 方面的overview和一些不错的入门资源、survey and papers.</p><ul><li><strong>Model pruning(模型剪枝)</strong>: removes less important parameters</li><li><strong>Weight Quantization</strong>: uses fewer bits to represent the parameters</li><li><strong>Parameter sharing</strong></li><li><strong>Knowledge distillation(知识蒸馏)</strong>: trains a smaller student model that learns from intermediate outputs from the original model.</li><li><strong>Module replacing / Dynamic Computation</strong>: Can network adjust the computation power it need?</li></ul><h2 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h2><ol><li><a href="https://www.bilibili.com/video/BV1tE411F7aC?from=search&seid=16988409402694511485">知识蒸馏简述–起源、改进与研究现状【截至2020年3月22日】</a></li><li><a href="https://www.bilibili.com/video/BV17z411B7Hk?from=search&seid=4245228983927115130">模型压缩Network Compression方法补充</a>    NOTE: 1的延续</li><li><a href="https://www.bilibili.com/video/BV1SC4y1h7HB?from=search&seid=16988409402694511485">网络压缩和知识蒸馏-李宏毅</a></li><li><a href="https://www.bilibili.com/video/BV1Rt4y1m7Fm?from=search&seid=4245228983927115130">【深度学习的模型压缩与加速】台湾交通大学 張添烜教授</a></li></ol><h2 id="Slides"><a href="#Slides" class="headerlink" title="Slides"></a>Slides</h2><p>Overview: <a href="https://slides.com/arvinliu/model-compression">https://slides.com/arvinliu/model-compression</a></p><p>Deep Mutual Learning <a href="https://slides.com/arvinliu/kd_mutual">https://slides.com/arvinliu/kd_mutual</a></p><h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><ol><li>深度学习模型压缩与加速综述 <a href="https://zhuanlan.zhihu.com/p/67871864">LINK</a></li><li>知识蒸馏是什么？一份入门随笔 <a href="https://zhuanlan.zhihu.com/p/90049906">LINK</a></li><li>知识蒸馏（Knowledge Distillation）简述（一）<a href="https://zhuanlan.zhihu.com/p/81467832">LINK</a></li><li>Mutual Mean-Teaching：为无监督学习提供更鲁棒的伪标签 <a href="https://zhuanlan.zhihu.com/p/116074945">LINK</a></li></ol><h2 id="Papers-and-Surveys"><a href="#Papers-and-Surveys" class="headerlink" title="Papers and Surveys"></a>Papers and Surveys</h2><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h3><ol><li>Knowledge Distillation 2015 <a href="https://arxiv.org/pdf/1503.02531.pdf">LINK</a> 知识蒸馏开山之作</li><li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks 2019 <a href="https://arxiv.org/abs/1803.03635">LINK</a></li><li>Rethinking the Value of Network Pruning 2019 <a href="https://arxiv.org/abs/1810.05270">LINK</a></li><li>BinaryConnect: Training Deep Neural Networks with binary weights during propagations 2015 <a href="https://arxiv.org/abs/1511.00363">LINK</a></li><li>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 2016 <a href="https://arxiv.org/abs/1602.02830">LINK</a></li><li>XNOR-NET 2016 <a href="https://arxiv.org/abs/1603.05279">LINK</a></li><li>MobileNets 2017 <a href="https://arxiv.org/abs/1704.04861">LINK</a></li><li>SqueezeNet 2016 <a href="https://arxiv.org/abs/1602.07360">LINK</a></li><li>Multi-Scale Dense Networks for Resource Efficient Image Classification 2018 <a href="https://arxiv.org/abs/1703.09844">LINK</a></li><li>Label Refinery: Improving ImageNet Classification through Label Progression 2018 <a href="https://arxiv.org/abs/1805.02641">LINK</a></li><li>Deep Mutual Learning 2018 <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf">LINK</a></li><li>Born Again Neural Networks 2018 <a href="https://arxiv.org/abs/1805.04770">LINK</a></li><li>Improved Knowledge Distillation via Teacher Assistant 2019 <a href="https://arxiv.org/abs/1902.03393">LINK</a></li><li>FITNETS: HINTS FOR THIN DEEP NETS 2015 <a href="https://arxiv.org/pdf/1412.6550.pdf">LINK</a></li><li>Relational Knowledge Distillation 2019 <a href="https://arxiv.org/pdf/1904.05068.pdf">LINK</a></li><li>Similarity-Preserving Knowledge Distillation 2019 <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf">LINK</a></li><li>Pruning Filters for Efficient ConvNets 2017 <a href="https://arxiv.org/abs/1608.08710">LINK</a></li><li>Learning Efficient Convolutional Networks Through Network Slimming 2017 <a href="https://arxiv.org/abs/1708.06519">LINK</a></li><li>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration 2019 <a href="https://arxiv.org/pdf/1811.00250.pdf">LINK</a></li><li>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures 2016 <a href="https://arxiv.org/abs/1607.03250">LINK</a></li><li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask 2019 <a href="https://arxiv.org/abs/1905.01067">LINK</a></li></ol><h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys:"></a>Surveys:</h3><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>model compression最主要包含以下四类大方法，<strong>每一类并不是独立的，是可以交替和交叉使用的。</strong></p><p>如下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/8bfmXQc5OCFEaj1.png" alt="image-20201126165839580" style="zoom:67%;" /><h2 id="Network-Pruning"><a href="#Network-Pruning" class="headerlink" title="Network Pruning"></a>Network Pruning</h2><p>主要思想为：将Network<strong>不重要</strong>的weight或neuron删除后，再重新训练一次。</p><p>General Reason：尤其在深度学习领域，网路很深，模型参数极多，存在很多冗杂的参数。</p><p>应用：所有有神经网络的地方基本都可以使用。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/y97ufNeRSBvkjDY.png" alt="image-20201126160856008" style="zoom:50%;" /><p>其整体流程大概可以如上图所示。</p><p>NOTE：这里如何计算weight或neuron的重要程度有各种不同的方法，e.g. L2, L1 Norm，the number of times it wasn’t zero on a given data set ……</p><p>可以看到，这是一个iteration的过程。</p><h3 id="Why-Pruning"><a href="#Why-Pruning" class="headerlink" title="Why Pruning"></a>Why Pruning</h3><p>为什么要做network Pruning? 而不是直接在一个小的model上学习?</p><p>经验上来讲：<strong>It is widely known that smaller network is more difficult to learn successfully.</strong> 即：大模型的训练往往比小模型更加简单，即更容易跳过一些local minimum。</p><p>当然也有一些关于这方面的理论，如：</p><ul><li>Lottery Ticket Hypothesis <a href="https://arxiv.org/abs/1803.03635">LINK</a></li><li>Larger network is easier to optimize? [LINK](<a href="https://www.youtube.com/watch?v=_VuWvQU">https://www.youtube.com/watch?v=_VuWvQU</a><br>MQVk)</li><li>Rethinking the Value of Network Pruning <a href="https://arxiv.org/abs/1810.05270">LINK</a>   NOTE: Lottery Ticket Hypothesis的反例</li></ul><p>基本上来讲network Pruning可以分为：</p><ul><li><p>对于weight 做pruning</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/zHNxSCdIkmYylfU.png" alt="image-20201126162158912" style="zoom:67%;" /><p>这种方法的最大问题是导致<strong>不方便implement！！！</strong>因为现在GPU加速都是使用矩阵运算，这样容易使网络结构变的不规则。导致无法Speed Up甚至会出现pruning后速度反而下降的情况。</p><p>在Practice中这类weight pruning常常就简单的将要pruning的weight设置为0，这样显然并没有办法对于模型的体积进行压缩。</p></li><li><p>对于Neuron 做pruning</p></li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/n4LjKY7qrQobh9N.png" alt="image-20201126162424440" style="zoom:67%;" /><p>可以看到，在pruning后，整个网络仍然是regular的，可以继续使用GPU进行加速。实践中比较常用。</p><h3 id="What-is-important-How-to-evaluate-importance"><a href="#What-is-important-How-to-evaluate-importance" class="headerlink" title="What is important? How to evaluate importance?"></a>What is important? How to evaluate importance?</h3><p>如何来衡量一个weight or neuron的importance</p><ul><li>evaluate by weight（看大小）</li><li>evaluate by activation</li><li>evaluate by gradient</li></ul><p>After that?</p><ul><li>Sort by importance and prune by rank</li><li>prune by handcrafted threshold</li><li>prune by generated threshold</li></ul><p>**Threshold or Rank? **</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/4Eci1AqsB67kwVF.png" alt="image-20201126200440010" style="zoom: 67%;" /><h4 id="Evaluate-Importance"><a href="#Evaluate-Importance" class="headerlink" title="Evaluate Importance"></a>Evaluate Importance</h4><p>这部分主要关注evaluate weight.</p><ul><li><p>sum of L1 norm（也可以是其他范数）</p><p>这种直接使用norm的方法一般如下（对于CNN而言）：</p><p>将卷积的filter排列为矩阵，每个filter的channel拼成一行，之后对每一行算norm，根据此norm来选择去掉哪些filter.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SAIQMnsiZF3DPez.png" alt="image-20201126201736434" style="zoom: 80%;" /><p>理想的norm分布应该如下图所示，即有：</p><ul><li>norm非常接近0的部分</li><li>整体是一个比价均匀，而且有较大方差的分布</li></ul><p>我们就想要prune掉norm接近0的部分。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/QYv96ng5r4OtIMH.png" alt="image-20201126201949988" style="zoom:80%;" /><p>而真实的分布往往并不尽如人意。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/qb2RPiuFAlSgY85.png" alt="image-20201126202145833" style="zoom:80%;" /><ol><li>方差很小，此时很难选取一个合适的threshold。</li><li>没有接近0的部分，不接近0，很难从norm的角度说明一个filter他trivial。</li></ol></li><li><p>FPGM(Filiter Pruning via Gemetirc Media 2019): 大的norm一定important？小的norm的一定trivial？用Gemetirc Media来解决这个问题。（解决hazard of pruning by norm）如下图所示。</p></li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/hly5eWXK4TaPvq9.png" alt="image-20201126202506572" style="zoom:80%;" /><ul><li><p>Evaluate By BN(Batch Norm) Network Slimming</p><ul><li>根据BN的γ来判断是否pruning</li><li><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SOvbfiQ73FJ84BM.png" alt="image-20201126203026861"></li><li>而往往γ的分布并不好，我们需要做一些penalty。让这个分布更容易被筛选。</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/KabxdY4LPWyjSn7.png" alt="image-20201126203224808" style="zoom:67%;" /></li><li><p>Eval by 0s after ReLU - APoZ(Average Percentage of Zeros)</p></li></ul><h3 id="Some-theory-about-Network-Pruning"><a href="#Some-theory-about-Network-Pruning" class="headerlink" title="Some theory about Network Pruning"></a>Some theory about Network Pruning</h3><h4 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/MeY3PogsmfuARxt.png" alt="image-20201126203825313" style="zoom:67%;" /><h4 id="Rethinking-the-Value-of-Network-Pruning"><a href="#Rethinking-the-Value-of-Network-Pruning" class="headerlink" title="Rethinking the Value of Network Pruning"></a>Rethinking the Value of Network Pruning</h4><h4 id="Rethinking-VS-Lottery-Ticket"><a href="#Rethinking-VS-Lottery-Ticket" class="headerlink" title="Rethinking VS Lottery Ticket"></a>Rethinking VS Lottery Ticket</h4><p>Rethinking: 一种neuron pruning or structural pruning</p><p>Lottery Ticket: 一种 weight pruning，且要求learning rate要小。</p><h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p>主要思想为：利用一个已经训练好的大model做teacher，来训练小model(student).</p><p>最核心的思路为下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/RHYLdCQWbqlySxc.png" alt="image-20201126163001439" style="zoom:67%;" /><p>对于Knowledge Distillation(下文简称为KD)的分类，基本上可以按照Distill What来分类。</p><ol><li><strong>Logits</strong> 即：网络的输出值，一个label的概率分布<ul><li>直接一对一匹配logits</li><li>以batch为学习单位来学习其中的logits distillation</li><li>……</li></ul></li><li><strong>Feature</strong> 即：网络每层中的中间值<ul><li>直接一对一匹配每层的中间值</li><li>学习teacher网络中feature是如何转换的</li></ul></li></ol><h3 id="The-Power-of-Soften-Label"><a href="#The-Power-of-Soften-Label" class="headerlink" title="The Power of Soften Label"></a>The Power of Soften Label</h3><p>对于分类任务，我们模型的输出并不是想真实的label中一样，是一个one-hot encoding，而是一组在许多label上都用几率的一组概率分布。可以直观的看到，这个模型的输出，相比于真实的label包含了更多的信息，甚至包含了类别间的relationship. 现在有一类研究方向就是在训练时不适用这种one-hot encoding，而是研究如何产生包含更多信息的Soften Label。</p><p>例如这篇Label Refinery: Improving ImageNet Classification through Label Progression <a href="https://arxiv.org/abs/1805.02641">LINK</a> 之后写这篇文章的总结和介绍。</p><h3 id="Logits-Distillation"><a href="#Logits-Distillation" class="headerlink" title="Logits Distillation"></a>Logits Distillation</h3><p>本质就是：通过soft target让小model可以学到class之间的关系。</p><p>一些比较有趣的Work:</p><ul><li>Deep Mutual Learning </li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkrAz1DXclxyK8P.png" alt="image-20201126171644149" style="zoom:67%;" /><ul><li>Born Again Neural Networks</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ERveOL2rFQAo6H5.png" alt="image-20201126171821469" style="zoom:67%;" /><p>显然，这其中存在着一些问题，可能因为teacher模型的模型能力过强，而小模型的能力不足，导致无法很好地直接学习。</p><p>其中的一种很有趣的解决方法就是，向我们上课一样，引入TA。TA模型的能力介于teacher和student之间。这样可以避免缩小模型间的差距。下图即为：Improved Knowledge Distillation via Teacher Assistant 2020这篇paper的想法。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/Lc9MU1257Gw4jXh.png" alt="image-20201126172800289" style="zoom:67%;" /><h3 id="Feature-Distillation"><a href="#Feature-Distillation" class="headerlink" title="Feature Distillation"></a>Feature Distillation</h3><p>不再是直接根据logits来学习，而是学习网络中的中间features。</p><p>其代表方法有：</p><ul><li>FitNet: 先让学生学习如何产生teacher的中间feature，以后再使用标准的KD。NOTE：框架越相似，效果越好。</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/EWcQDowM2CZNlzv.png" alt="image-20201126173316849" style="zoom:67%;" /><p>该方法存在两大问题：</p><ul><li>model capacity is different. 显然如果大模型很复杂，可能小模型的中间部分无法学习到大模型的复杂映射。</li><li>redundance in teacher feature. 这个是很直观的，对于复杂的模型，这个feature中并不是所有的部分都是起作用的，这些对于小模型来讲是一个学习的负担。</li></ul><p>解决上述问题的方法可以是对于大模型的每一个feature map做一个知识蒸馏，目的就是在压缩feature的同时也降低了redundance. </p><p>另外一种的解决方法就是使用<strong>Attention</strong>，告诉student model中的feature map(主要指CNN)中那些part是最重要的。</p><p>下图是一种简单计算attention的方法，就是将filter对应产生的out dim做一个压缩。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/JnTAcqSV2r1UE75.png" alt="image-20201126192625424" style="zoom:67%;" /><h3 id="Relational-Distillation-Learn-from-Batch"><a href="#Relational-Distillation-Learn-from-Batch" class="headerlink" title="Relational Distillation: Learn from Batch"></a>Relational Distillation: Learn from Batch</h3><p>前面的不论是logit KD还是feature KD，都是对于每一个sample来学习的（即：individual KD），这类Relational Distillation关注的则通过一个batch来distillation batch之间sample的关系。</p><p>下图是这individual KD与Relational KD的paradigm的对比。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkfRwnCVBd4WLaT.png" alt="image-20201126193016466" style="zoom:67%;" /><p>而衡量sample之间的relation 可以用以下的两种描述角度：</p><ul><li>Distance-wise KD: 使用L2 distance来描述。</li><li>Angle-wise KD: 使用cosin similarity来描述。</li></ul><p>该方法是使用logits来做KD的，自然也可以使用features来做KD。这就有了：Similarity-Preserving Knowledge Distillation这篇文章。</p><h2 id="Parameter-Quantization"><a href="#Parameter-Quantization" class="headerlink" title="Parameter Quantization"></a>Parameter Quantization</h2><p>将原本神经网络中数据的存储单位float32/64压缩为更小的单位，例如8位。</p><p>应用：对于所欲已经train好的模型使用，或者边train边让模型去做quantize。</p><p>基本来讲大致分为以下方法：</p><ol><li>Using less bits to represent a value</li><li>Weight clustering</li></ol><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ECuVJeA4ZiOKNsR.png" alt="image-20201126163333485" style="zoom:67%;" /><ol start="3"><li>Represent frequent clusters by less bits, represent rare clusters by more bits. e.g. Huffman encoding</li></ol><p>其中一类是使用Binary Weight。(从某种角度上，也是一种正则化的方法)</p><ul><li>Binary Connect <a href="https://arxiv.org/abs/1511.00363">LINK</a></li><li>Binary Network <a href="https://arxiv.org/abs/1602.02830">LINK</a></li><li>XNOR-Net <a href="https://arxiv.org/abs/1603.05279">LINK</a></li></ul><h2 id="Architecture-Design"><a href="#Architecture-Design" class="headerlink" title="Architecture Design"></a>Architecture Design</h2><p>方法：利用更少的参数来实现原本某些layer效果。</p><p>例如对于全连接层：</p><p>我们可以将原本的 N到M维的映射 变为 N-&gt;K-&gt;M的映射。从矩阵乘法的角度来看，这可以看作一种Low rank approximate. 这种方法也可以极大的减少全连接层的参数数量。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/U8oKGqR9xNWDlV3.png" alt="image-20201126164212464" style="zoom:67%;" /><p>对于卷积层：可以使用Depthwise Separable Convolution（这也是大名鼎鼎的mobile net使用的方法）</p><p>即将原本一步的卷积运算变为两个卷积运算，如下图所示。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/mqGlihdKrTAOX9I.png" alt="image-20201126164522446" style="zoom:67%;" /><p>其他的一些经典的这类design有：</p><ul><li>MobileNet</li><li>SqueezeNet</li><li>Xception</li><li>ShuffleNet</li></ul><h2 id="Dynamic-Computation"><a href="#Dynamic-Computation" class="headerlink" title="Dynamic Computation"></a>Dynamic Computation</h2><p>核心思想：Can network adjust the computation power it need?</p><p>即：若此时计算资源充分，使用大模型；若不足，使用较小的模型。</p><p>可能的解决方法：</p><ol><li><p>Train multiple classifiers</p></li><li><p>Classifiers at the intermedia layer 例如 Multi-Scale Dense Networks <a href="https://arxiv.org/abs/1703.09844">LINK</a></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/zJ5US8EW79Ftkv6.png" alt="image-20201130154939206" style="zoom:80%;" /></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Model-Compression-Overview-and-Resources&quot;&gt;&lt;a href=&quot;#Model-Compression-Overview-and-Resources&quot; class=&quot;headerlink&quot; title=&quot;Model Compre
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Model Compression" scheme="http://canVa4.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>CS224w HomeWork 2</title>
    <link href="http://canva4.github.io/2020/11/10/CS224w-HomeWork-2/"/>
    <id>http://canva4.github.io/2020/11/10/CS224w-HomeWork-2/</id>
    <published>2020-11-10T12:52:03.000Z</published>
    <updated>2020-11-19T03:26:27.254Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224w-HomeWork-2"><a href="#CS224w-HomeWork-2" class="headerlink" title="CS224w HomeWork 2"></a>CS224w HomeWork 2</h1><p>本文旨在记录CS224w Machine Learning With Graphs 2019完成作业中遇到的问题和作业的结果。</p><p>我的github仓库 <a href="https://github.com/canVa4/CS224w">LINK</a>。</p><p>本部分共包含：</p><ul><li>Node Classication: correlation with <strong>Collective Classification and message passing</strong></li><li>Node Embeddings with TransE: correlation with <strong>Graph Representation Learning</strong></li></ul><h2 id="Part-1-Node-Classification"><a href="#Part-1-Node-Classification" class="headerlink" title="Part 1 Node Classification"></a>Part 1 Node Classification</h2><p>Node Classication主要是使用Collective Classification的方法，其主要流程和组成部分如下图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/hboyclIR93dNCar.png" alt="image-20201110211233087" style="zoom:80%;" /><p>其中approximate inference的方法主要为（这三者sort by how advanced these methods are）：</p><ul><li>Relational Classification</li><li>Iterative Classification</li><li>Belief Classification</li></ul><p>这三种方法都是基于Markov Assumption，即：</p><p> the labely $Y_i$ of one node depends on the labels of its neighbors, which can be mathematically written as:</p><p>$$P(Y_i|i)=P(Y_i|N_i)$$</p><p>其中$N_i$为节点i的neighbors。</p><h3 id="Relational-Classification"><a href="#Relational-Classification" class="headerlink" title="Relational Classification"></a>Relational Classification</h3><p>非常简单的方法，只使用了label和网络拓扑结构的信息，没有使用每个节点的features。对于每个无label节点的预测只是简单的取一个邻居的label的平均。</p><p>使用下式的公式来预测无label的节点。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/t4QmbGOg5zWsr7I.png" alt="image-20201114201117560" style="zoom:67%;" /><p>其缺点也很明显，即</p><ul><li>The convergence not guaranteed.</li><li>Cannot use node feature information, only use the graph information.</li></ul><p>这部分对应的作业也比较简单，即在给定的简单的图上实现这个算法。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/3NqRoHriQpXLITj.png" alt="image-20201114205829364" style="zoom: 67%;" /><p>简易的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> snap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span>():</span></span><br><span class="line">    g = snap.TUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">        g.AddNode(i)</span><br><span class="line">    g.AddEdge(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    g.AddEdge(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    g.AddEdge(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    g.AddEdge(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">    g.AddEdge(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    g.AddEdge(<span class="number">4</span>, <span class="number">7</span>)</span><br><span class="line">    g.AddEdge(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">    g.AddEdge(<span class="number">5</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">6</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">6</span>, <span class="number">10</span>)</span><br><span class="line">    g.AddEdge(<span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">    g.AddEdge(<span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">    g.AddEdge(<span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">G = create_graph()</span><br><span class="line">node_dict = &#123;&#125;</span><br><span class="line">positive = [<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">negative = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">node_dict[<span class="number">3</span>] = positive</span><br><span class="line">node_dict[<span class="number">5</span>] = positive</span><br><span class="line">node_dict[<span class="number">8</span>] = negative</span><br><span class="line">node_dict[<span class="number">10</span>] = negative</span><br><span class="line">label_id = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">node_num = G.GetNodes()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, node_num + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_id:</span><br><span class="line">        node_dict[i] = [<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line">loop_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> flag <span class="keyword">is</span> <span class="keyword">not</span> <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 当不在变化时，停止迭代</span></span><br><span class="line">    flag = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, node_num + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> label_id:</span><br><span class="line">            neighbors = []</span><br><span class="line">            cur_node = G.GetNI(i)</span><br><span class="line">            degree = cur_node.GetDeg()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> nbr <span class="keyword">in</span> range(degree):</span><br><span class="line">                neighbors.append(cur_node.GetNbrNId(nbr))</span><br><span class="line">            origin = node_dict[i]</span><br><span class="line">            sum_p = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> mid <span class="keyword">in</span> neighbors:</span><br><span class="line">                sum_p[<span class="number">0</span>] += node_dict[mid][<span class="number">0</span>] / degree</span><br><span class="line">                sum_p[<span class="number">1</span>] += node_dict[mid][<span class="number">1</span>] / degree</span><br><span class="line">            node_dict[i] = sum_p</span><br><span class="line">            <span class="keyword">if</span> abs(origin[<span class="number">0</span>] - sum_p[<span class="number">0</span>]) &gt; <span class="number">0.001</span>:</span><br><span class="line">                <span class="comment"># 当每次变化小于0.001时，认为仍在变化</span></span><br><span class="line">                flag += <span class="number">1</span></span><br><span class="line">            print(<span class="string">&#x27;id:&#123;&#125;\t pro:&#123;&#125;&#x27;</span>.format(i, sum_p))</span><br><span class="line">    loop_cnt += <span class="number">1</span></span><br><span class="line">    print(<span class="string">&#x27;Loop &#123;&#125; finish!!!&#x27;</span>.format(loop_cnt))</span><br></pre></td></tr></table></figure><p>运行代码即可得到Q1.1的答案。</p><h3 id="Belief-Propagation"><a href="#Belief-Propagation" class="headerlink" title="Belief Propagation"></a>Belief Propagation</h3><p>Belief Propagation is a dynamic programming approach to answering conditional probability queries in a graphical model. 本质上是模拟网络上信息的传递，根据相邻节点传递的信息来确定自己的状态。在迭代过程中，每个节点都会跟相邻节点通信，即传递信息。详细的公式和细节见slide。</p><p>答案：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/14/tHWLRPhnfJr5d6S.png" alt="image-20201114215247475"></p><h2 id="Part-2-Node-Embeddings-with-TransE"><a href="#Part-2-Node-Embeddings-with-TransE" class="headerlink" title="Part 2 Node Embeddings with TransE"></a>Part 2 Node Embeddings with TransE</h2><p>这里主要讨论一种经典的应用于Multi-relational graphs的embedding方法—-TransE。Multi-relational graphs是指：graphs with multiple types of edges. 这种图的一个典型例子就是知识图谱（Knowledge Graph）。而TransE也是知识图谱上面embedding的经典方法。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/16/gU8xO4V9tQdSMLD.png" alt="image-20201116215835875"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/16/K2DA7Ec8xw9BWNX.png" alt="image-20201116215808214"></p><h2 id="Part-3-GNN-Expressiveness"><a href="#Part-3-GNN-Expressiveness" class="headerlink" title="Part 3 GNN Expressiveness"></a>Part 3 GNN Expressiveness</h2><p>NOTE：在进行训练的时候，如果节点没有features，其features vector设为全1向量或者该节点的度。</p><p>本部分主要在研究，GNN的层数与其表示能力（Expressiveness）的关系。expressiveness refers to the set of functions (usually the loss function for classication or regression tasks) a neural network is able to compute, which depends on the structural properties of a neural network architecture.</p><h3 id="3-1-Effect-of-Depth-on-Expressiveness"><a href="#3-1-Effect-of-Depth-on-Expressiveness" class="headerlink" title="3.1 Effect of Depth on Expressiveness"></a>3.1 Effect of Depth on Expressiveness</h3><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/QesCVAjP6HDgxw5.png" alt="image-20201119103926420" style="zoom:50%;" /><h3 id="3-2-Relation-to-Random-Walk"><a href="#3-2-Relation-to-Random-Walk" class="headerlink" title="3.2 Relation to Random Walk"></a>3.2 Relation to Random Walk</h3><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/P2Dyvsf7AUlN1C3.png" alt="image-20201119105125162" style="zoom: 50%;" /><h3 id="3-3-Over-Smoothing-Effect"><a href="#3-3-Over-Smoothing-Effect" class="headerlink" title="3.3 Over-Smoothing Effect"></a>3.3 Over-Smoothing Effect</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/19/KEFlauwm1Io5W7i.png" alt="image-20201119111833303"></p><h3 id="3-4-Learning-BFS-with-GNN"><a href="#3-4-Learning-BFS-with-GNN" class="headerlink" title="3.4 Learning BFS with GNN"></a>3.4 Learning BFS with GNN</h3><p>很简单的average aggregation，这里就不再写出。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS224w-HomeWork-2&quot;&gt;&lt;a href=&quot;#CS224w-HomeWork-2&quot; class=&quot;headerlink&quot; title=&quot;CS224w HomeWork 2&quot;&gt;&lt;/a&gt;CS224w HomeWork 2&lt;/h1&gt;&lt;p&gt;本文旨在记录CS22
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS224w" scheme="http://canVa4.github.io/tags/CS224w/"/>
    
      <category term="GNN" scheme="http://canVa4.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Embedding Literature Review</title>
    <link href="http://canva4.github.io/2020/10/30/Embedding-Literature-Review/"/>
    <id>http://canva4.github.io/2020/10/30/Embedding-Literature-Review/</id>
    <published>2020-10-30T03:14:49.000Z</published>
    <updated>2020-11-19T03:00:21.832Z</updated>
    
    <content type="html"><![CDATA[<p>Embeddings主要思路分类：</p><ul><li>NLP类方法 使用LSTM等对时序数据做表示</li><li>Graph Embedding 对图做嵌入（引入图的原因之一是：用图可以表示复杂关系的长时间时间序列）</li><li>类似CNN的方法也可以看为Embedding</li><li>对比学习，学到更好的表示 or Embedding</li></ul><p>知乎LINKS</p><ol><li>Embedding的原因 <a href="https://zhuanlan.zhihu.com/p/164502624">https://zhuanlan.zhihu.com/p/164502624</a> </li><li>Embedding的简单发展史（主要为Word2vec -&gt; Item2Vec）<a href="https://zhuanlan.zhihu.com/p/164502624">https://zhuanlan.zhihu.com/p/164502624</a></li><li>万物皆可Embedding <a href="https://zhuanlan.zhihu.com/p/109935332">https://zhuanlan.zhihu.com/p/109935332</a></li><li>Embedding在深度学习中的3大方向 <a href="https://zhuanlan.zhihu.com/p/67218758">https://zhuanlan.zhihu.com/p/67218758</a></li></ol><p>Video：</p><ol><li>Pre-trained Models for Natural Language Processing: A Survey <a href="https://www.bilibili.com/video/BV16K4y1475Z?from=search&seid=10138468149493332259">LINK</a></li><li>自然语言处理中的表示学习 <a href="https://www.bilibili.com/video/BV1va4y1Y7BE/?spm_id_from=333.788.videocard.2">LINK</a></li></ol><h1 id="Resource-List："><a href="#Resource-List：" class="headerlink" title="Resource List："></a>Resource List：</h1><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h2><ol><li>A Tutorial on Network Embeddings 2018 <strong>Finished</strong></li><li>Tutorial on NLP-Inspired Network Embedding 2019 <strong>Finished</strong></li><li>DeepWalk   <strong>INCLUDE</strong></li><li>LINE   <strong>INCLUDE</strong></li><li>Node2Vec   <strong>INCLUDE</strong></li><li>GraphAttention   <strong>INCLUDE</strong></li><li>SDNE</li><li>HOPE</li><li>Learning edge representations via low-rank asymmetric projections  2017</li><li>Deep graph kernels 2015</li></ol><h2 id="Videos"><a href="#Videos" class="headerlink" title="Videos:"></a>Videos:</h2><ol><li>Pre-trained Models for Natural Language Processing: A Survey <a href="https://www.bilibili.com/video/BV16K4y1475Z?from=search&seid=10138468149493332259">LINK</a> </li><li>自然语言处理中的表示学习 <a href="https://www.bilibili.com/video/BV1va4y1Y7BE/?spm_id_from=333.788.videocard.2">LINK</a> <strong>Finished</strong></li></ol><h2 id="Specturm-Methods"><a href="#Specturm-Methods" class="headerlink" title="Specturm Methods:"></a>Specturm Methods:</h2><p><a href="https://www.cnblogs.com/pinard/p/6221564.html">https://www.cnblogs.com/pinard/p/6221564.html</a></p><h1 id="A-Tutorial-on-Network-Embeddings"><a href="#A-Tutorial-on-Network-Embeddings" class="headerlink" title="A Tutorial on Network Embeddings"></a>A Tutorial on Network Embeddings</h1><p>Author: Haochen Chen er al <a href="https://arxiv.org/abs/1808.02590">LINK</a></p><p>Journal: Social and Information Networks 2018</p><p><strong>Goal of Network Embeddings：</strong></p><p>Network embedding methods aim at learning low-dimensional latent representation of nodes in a network.</p><p>而获得的embeddings应该有如下的性质：</p><ul><li><strong>适应性（Adaptability）</strong>- 现实的网络在不断发展；新的应用算法不应该要求不断地重复学习过程。</li><li><strong>可扩展性（Scalability）</strong>- 真实网络本质上通常很大，因此网络嵌入算法应该能够在短时间内处理大规模网络。</li><li><strong>社区感知（Community aware）</strong>- 潜在表示之间的距离应表示用于评估网络的相应成员之间的相似性的度量。这就要求同质网络能够泛化。</li><li><strong>低维（Low dimensional）</strong>- 当标记数据稀缺时，低维模型更好地推广并加速收敛和推理。</li><li><strong>连续的（Continuous）</strong></li></ul><h2 id="从传统ML降维的角度看Graph-Embedding"><a href="#从传统ML降维的角度看Graph-Embedding" class="headerlink" title="从传统ML降维的角度看Graph Embedding"></a>从传统ML降维的角度看Graph Embedding</h2><p>这其中使用了例如：PCA和MDS等方法，这类方法都可以看为使用一个n by k矩阵来代表原始数据的n by m矩阵，其中k&lt;m。之后又提出了一些新的降维方法比如：IsoMap，LLE等方法。总体来讲这类方法在小的网络上显示了不错的性能，但由于其复杂度往往随矩阵规模而指数增长，导致这种方法无法应用于大型的图。</p><p>另外一类就是图的谱方法了（如LE: Laplacian eigenmaps），基本上来讲使用拉普拉斯矩阵or标准化的拉普拉斯矩阵的特征值和特征向量的信息来对于每个节点进行聚类和划分以实现嵌入的效果。这类方法的主要问题是：对矩阵的特征值分解是与矩阵规模呈指数形式的，所以在large network中也较难应用，且这类基于spectrum的方法的性能往往逊色于基于neural network的方法。</p><h2 id="The-Age-of-Deep-Learning"><a href="#The-Age-of-Deep-Learning" class="headerlink" title="The Age of Deep Learning"></a>The Age of Deep Learning</h2><p><strong>DeepWalk在图上使用表示学习（或深度学习）的方法，是一个非常经典的方法</strong>。DeepWalk 通过将节点视为单词并生成短随机游走（random walk）作为句子来弥补网络嵌入和单词嵌入之间的差距。然后，可以使用 Skip-gram 等，应用于这些随机游走以获得网络嵌入。</p><p>优点：</p><ul><li>Online algorithm</li><li>Scalable</li></ul><p>也引出了一种在图上使用Deep Learning的paradigm</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/ySGZqvIswTxdeLD.png" alt="image-20201105105601325" style="zoom:80%;" /><h2 id="Unsupervised-Network-Embeddings"><a href="#Unsupervised-Network-Embeddings" class="headerlink" title="Unsupervised Network Embeddings"></a>Unsupervised Network Embeddings</h2><p>最经典的还是DeepWalk，其有如下两个地方可以改进：</p><ul><li><p>Source of Context Node（如何产生序列）</p></li><li><p>Embedding Learning Methods(deepwalk中使用skip-gram，当然也可以使用其他学习序列表示的方法)</p></li></ul><p>针对于Deep Walk的可改良点，有这些改进的方法。<img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201105113538975.png" alt="image-20201105113538975"></p><p>其中比较出名的有的：LINE，Node2Vec，GraphAttention等。后面的SDNE和DNGR则引入了deep learning中较深的encoder类方法。</p><p>值得注意的是，这些方法主要是用于undirected graph中的。</p><h3 id="Directed-Graph-Embedding"><a href="#Directed-Graph-Embedding" class="headerlink" title="Directed Graph Embedding"></a>Directed Graph Embedding</h3><p>基本上全部的基于无向图的方法都可以很自然的推广到无向图中。</p><p>其中常见和经典的方法为：HOPE</p><h3 id="Edge-Embedding"><a href="#Edge-Embedding" class="headerlink" title="Edge Embedding"></a>Edge Embedding</h3><p>这类embedding可以用于link prediction等task中。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/W1Hxt5GgIqs6TDU.png" alt="image-20201105150551272"></p><h3 id="Signed-Graph-Embeddings"><a href="#Signed-Graph-Embeddings" class="headerlink" title="Signed Graph Embeddings"></a>Signed Graph Embeddings</h3><p>Signed Graph指：边的权值只为-1or1，主要方法报过：SiNE和SNE。</p><h3 id="Subgraph-Embeddings"><a href="#Subgraph-Embeddings" class="headerlink" title="Subgraph Embeddings"></a>Subgraph Embeddings</h3><p>经典方法为Deep Kernel也是这类方法的常见范式。</p><h3 id="Meta-strategies-for-Improving-Network-Embeddings"><a href="#Meta-strategies-for-Improving-Network-Embeddings" class="headerlink" title="Meta-strategies for Improving Network Embeddings"></a>Meta-strategies for Improving Network Embeddings</h3><p>主要方法HARP。HARP is a general meta-strategy to improve all of the state-of-the-art neural algorithms for embedding graphs, including DeepWalk, LINE, and Node2vec.</p><h2 id="Attributed-Network-Embeddings"><a href="#Attributed-Network-Embeddings" class="headerlink" title="Attributed Network Embeddings"></a>Attributed Network Embeddings</h2><p>前面的讨论针对的主要是没有attribute的图，而对于有attribute的图，其attribute一般是附加在节点上的，一般主要研究两类attribute</p><ul><li>high-level features such as text or images</li><li>node labels</li></ul><p>针对第一点（high-level features）：常见方法包括：TADW，CENE</p><p>针对第二点（node labels 常见于 引用网络和社交网络）：GENE</p><p>当然在许多真正的网络中，并不是所有节点都是有label的，所以也有一些Semi-supervised的方法设计出来。例如：Planetoid，Max-margin Deep Walk, </p><h2 id="Heterogeneous-Network-Embeddings"><a href="#Heterogeneous-Network-Embeddings" class="headerlink" title="Heterogeneous Network Embeddings"></a>Heterogeneous Network Embeddings</h2><p>Heterogeneous Network/Graph 异质图，即have multiple classes of nodes or edges。大部分异构网络嵌入方法通过联合最小化每种modality的损失来学习节点嵌入。这些方法要么直接在相同的潜在空间中学习所有节点嵌入，要么事先为每个模态构建嵌入，然后将它们映射到相同的潜在空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此paper主要是总结了一下目前常见的和经典的一些图嵌入的方法（连介绍都算不上），也对于在不同应用场景上的方法做了一个分类，很适合在之后想在某个场景使用图嵌入方法时来进行查阅。这部分note也主要是这个目的。作者也简单总结了未来图嵌入可能的发展方向，不过本文是2018年的，之后会跟进一些2018年之后更新的方法。</p><h1 id="Tutorial-on-NLP-Inspired-Network-Embedding"><a href="#Tutorial-on-NLP-Inspired-Network-Embedding" class="headerlink" title="Tutorial on NLP-Inspired Network Embedding"></a>Tutorial on NLP-Inspired Network Embedding</h1><p>主要介绍使用word embedding的方法到graph embedding中。主要包括以下几个经典的方法：</p><ul><li>DeepWalk: Online Learning of Social Representations 2014</li><li>LINE: Large-scale Information Network Embedding 2015</li><li>node2vec: Scalable Feature Learning for Networks 2016</li><li>struct2vec: Learning Node Representations from Structural Identity 2017</li><li>metapath2vec: Scalable representation learning for heterogeneous networks 2017</li></ul><p>LINE和node2vec是对于deepwalk的改进，公认后者更加稳定一些。</p><h2 id="word2vec（skip-gram）"><a href="#word2vec（skip-gram）" class="headerlink" title="word2vec（skip-gram）"></a>word2vec（skip-gram）</h2><p>word2vec基于的假设是：the principle of co-occurrence: the assumption that words with related semantic meanings appear close to each other in texts.</p><p>word2vec模型示意图，一种无监督方法。需要一个大小为2*k的slide window来生成输入序列。给定一个词w，其上下文词$c\in C(w)$由该slide window获得，我们优化的目标是：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/PUfhDNZBjCFczie.png" alt="image-20201112150224584"></p><p>即给定当前词，令上下文词出现的概率最大，找到合适的参数θ。</p><p>对于整个预料库，我们有：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ogSdlFwfcLmAtNG.png" alt="image-20201112150338856"></p><p>计算这个概率一般使用softmax，即：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/dGkvFhIPXKC6ySW.png" alt="image-20201112150434138"></p><p>$V_c, V_w$为生成的embedding。由于在计算分母部分时，也非常的耗费计算时间，所以经常使用negative sampling（本质是预测总体类别的一个子集）或hierarchy softmax（本质是把 N 分类问题变成 log(N)次二分类）来取一个approximate。整体模型如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/qRknro8QuvaA5Et.png" alt="image-20201112145802004"></p><h2 id="Deep-Walk"><a href="#Deep-Walk" class="headerlink" title="Deep  Walk"></a>Deep  Walk</h2><p>一个比较简答扩展，将word2vec可以应用于网络之中！对应关系如下图所示，具体细节可以见cs224w  slides or 原始paper</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/oCdrmGn3ayKiwJs.png" alt="image-20201112152105745"></p><p>The assumption is that sampling from multiple random walks captures the structure of the graph.</p><p><strong>缺点：</strong></p><p>生成序列使用的random walk是unbiased的，其与DFS比较相似。</p><h2 id="LINE"><a href="#LINE" class="headerlink" title="LINE"></a>LINE</h2><p>LINE的目标就是优化random walk的unbiased特性，solve this issue by preserving first-order and second-order node proximities（保留一阶和二阶节点的亲近性）。其没有使用random walk，而是使用1-hop和2-hop的节点来构造供给NLP word embedding的sequence序列。</p><ul><li>first-order proximity：节点的一阶proximity被定义为：其链接边的权值。</li></ul><p>其数学定义为：v是其生产的embedding</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/lOGSHkYgbuyvIaq.png" alt="image-20201112163126930" style="zoom: 67%;" /><p>其理想情况应该与基于节点权值定义的一阶相似度比较接近。即与下式接近。其中W为正则化因子。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/dAUixgQrvF6yIke.png" alt="image-20201112163256655" style="zoom:67%;" /><p>embedding的优化目标就是尽可能缩小二者分布的“距离”，即最小化二者的KL散度。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ibcMVprNhLkJs14.png" alt="image-20201112163428368" style="zoom:67%;" /><ul><li>second-order proximity：节点的二阶proximity被定义为：the common neighborhood of two nodes</li></ul><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/k6nZMCovr2PUG7w.png" alt="image-20201112162920623" style="zoom:67%;" /><p>其二阶相似度的优化目标为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ajqCp6lYOUMvxiB.png" alt="image-20201112163646201" style="zoom:67%;" /><p>其中P2为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/w1uDkBVZvI475Ws.png" alt="image-20201112163703959" style="zoom:67%;" /><p>下面是对于一阶和二阶相似度的一个直观解释。如上图所示，可见节点6与7有很高的一阶相似度（相连的边的权值大），所以他们在embedding space应该比较接近。</p><p>节点5与节点6虽然没有直接连接，但二者有高度相似的邻居节点，所以二者的二阶相似度很高，他们在embedding space中也应该比较接近。</p><p>在LINE中，the embedding for the first-order and second-order proximities (i.e. maximizing for O1 and O2) is done separately。最终从这两个模型得到的embedding在concatenate成为最终的embedding。</p><p><strong>总结</strong>：</p><p>不同于DeepWalk的DFS，LINE更倾向于BFS；而且其最大的特点是可以利用在带权图中，利用了权重的信息。</p><h2 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h2><p>使用biasing the random walks。本质上就是改变了random walk生成序列的方式。作者因为了两个因子，来控制random walk是倾向于DFS还是BFS。在实现时要记录是从哪一个节点来的。</p><p>文中还提出了基于node2vec如何表示边的信息。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/khASl6HZfpFVroX.png" alt="image-20201112170716300" style="zoom:67%;" /><h2 id="struct2vec"><a href="#struct2vec" class="headerlink" title="struct2vec"></a>struct2vec</h2><p>Focuses on the <strong>role</strong> of nodes in a network. Struc2vec is a framework for representations based on structural similarity. </p><p>GOAL: The goal of struc2vec is to preserve the identity of the nodes’ structure when projecting them into Euclidean space.</p><p>这与node2vec 和 deepwalk 很不一样，这俩是捕获neighbor的信息，而许多节点在struct很相似，但在图上的距离却很远。</p><p>为了计算这个表示，struct2vec构建了一个特殊的graph：<strong>the context graph</strong> 其代表了structural similarities between nodes.有了context graph之后，只需在该图上使用deep walk or node2vec即可。</p><p>设原始的图的diameter（直径为K），则the context graph M is a multi-layer graph with K + 1 layers. Each layer includes all the nodes in G.在每一个layer中，带权的权值表示了两个节点的结构相似度。</p><h3 id="Step-1-Computing-Structural-Similarity"><a href="#Step-1-Computing-Structural-Similarity" class="headerlink" title="Step 1: Computing Structural Similarity"></a>Step 1: Computing Structural Similarity</h3><p>第一步主要是寻找节点之间的structural Similarity。For each node v, we look at the $N_k(u)$, which is the set of nodes which are k-distant from node u. 对于每一组$N_k(u)$看成一个ring，我们找他们的ordered degree squence，即把每个节点的度组成序列（按照降序）。k最大为K（图的diameter(直径)）。measure两个节点的structural Similarity就是使用每个节点计算的ordered degree squence。用于这个ordered degree squence显然未必是等长度的，所以作者使用了Dynamic Time Warping (DTW)，一种match element between 2 squence with different length.</p><p>下图即为整体的Structural Similarity的计算公式，可以看到时递归计算的。DS()即为ordered degree squence，g()即为前面提到的DTW，用于计算两个长度不一样序列的距离。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/iWlMAjBEmdy3cwx.png" alt="image-20201112191817212" style="zoom:67%;" /><h3 id="Step-2-Constructing-the-Context-Graph"><a href="#Step-2-Constructing-the-Context-Graph" class="headerlink" title="Step 2: Constructing the Context Graph"></a>Step 2: Constructing the Context Graph</h3><p>The context graph M is a multilayer graph with K layers. Each layer is a complete graph consisting of all the nodes u ∈ V in the original graph G. 所以图G中的每个节点可以用M中K+1层(k=0, … , K)个对应的节点来表示。</p><p>下图就是一个根据左边G建立的M。一共包含3层。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/ovb5lOJXpqUQMED.png" alt="image-20201112201859657" style="zoom:67%;" /><p>在每一层都是一个全连接的无向带权图，其权重的计算公式为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/vI1WDsHVRcwUYSf.png" alt="image-20201112202040048"></p><p>式中的f就是上文计算不同长度序列的函数。直观的来讲，两个节点的的structural similarity越相似，即该计算的距离越小，这两个节点的间的权值就越大。</p><p>从图中也可以看出，不同layer间的同一个节点间也存在着双向的带权边，权值的计算方式如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/cRdreDCqtKy47Tw.png" alt="image-20201112202329435" style="zoom:67%;" /><p>其中$\Gamma_k(u)$为连接u的边中权重大于该层平均权重的数量。直观的来讲：$\Gamma_k(u)$就是有多少节点是与节点u相似的。</p><h3 id="Step-3-Generate-Context"><a href="#Step-3-Generate-Context" class="headerlink" title="Step 3: Generate Context"></a>Step 3: Generate Context</h3><p>现在我们已经建立了Context Graph，下一步就是与Deep Walk非常相似了。不过在这里显然一个节点可以往上面的层走，也可以向下面的层走，也可以在同层间游动。其中：留在本层继续游走的概率为q，自然跳层的概率就是1-q，该q为一个超参数。</p><p>切换layer的概率为1-q，向上和向下移动的概率为：</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201112203231908.png" alt="image-20201112203231908" style="zoom: 80%;" /><p>停留在当前layer的概率为q，移动的概率也根据边的权值决定。其中$Z_k(u)$为一个归一化因子。直观的来讲，结构越相似的节点越容易出现在一次random walk之中。</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201112203320875.png" alt="image-20201112203320875" style="zoom:80%;" /><h3 id="Step-4-Learn-Representations"><a href="#Step-4-Learn-Representations" class="headerlink" title="Step 4: Learn Representations"></a>Step 4: Learn Representations</h3><p>最后就是使用生成的序列来进行word2vec即可（skip-gram + negative sampling）</p><h2 id="metapath2vec"><a href="#metapath2vec" class="headerlink" title="metapath2vec"></a>metapath2vec</h2><p>主要针对heterogeneous networks（异质图）。在异质图中，节点可以属于不同的类别。这个与之前random walk类方法使用的同质图非常不同，作者主要使用random walks will be biased by using meta-paths。一个mate-path是一个predefined composite relation between nodes. 本质就是random walk只能发生在定义好的meta-path上，the random walks must follow the semantics dictated by the various prescribed meta-paths。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Embeddings主要思路分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP类方法 使用LSTM等对时序数据做表示&lt;/li&gt;
&lt;li&gt;Graph Embedding 对图做嵌入（引入图的原因之一是：用图可以表示复杂关系的长时间时间序列）&lt;/li&gt;
&lt;li&gt;类似CNN的方法也可以看为
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="Embedding" scheme="http://canVa4.github.io/tags/Embedding/"/>
    
      <category term="Literature Review" scheme="http://canVa4.github.io/tags/Literature-Review/"/>
    
  </entry>
  
  <entry>
    <title>CS224w HomeWork 1</title>
    <link href="http://canva4.github.io/2020/10/28/CS224w-HomeWork-1/"/>
    <id>http://canva4.github.io/2020/10/28/CS224w-HomeWork-1/</id>
    <published>2020-10-28T08:21:31.000Z</published>
    <updated>2020-11-10T12:56:33.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224w-HomeWork-1"><a href="#CS224w-HomeWork-1" class="headerlink" title="CS224w HomeWork 1"></a>CS224w HomeWork 1</h1><p>本文旨在记录CS224w Machine Learning With Graphs 2019完成作业中遇到的问题和作业的结果。</p><p>我的github仓库 <a href="https://github.com/canVa4/CS224w">LINK</a>。</p><h2 id="Part-1-Network-Characteristics"><a href="#Part-1-Network-Characteristics" class="headerlink" title="Part 1 Network Characteristics"></a>Part 1 Network Characteristics</h2><p>课程中反复强调的一个非常重要的观点就是：想要比较一个网络的属性，我们需要一个criterion或者一个null network。</p><p>所以这部分的核心就是null network or criterion的生成。我们需要生成Erdös-Renyi Random Graphs和Small-World Random Network。</p><h3 id="Erdos-Renyi-Random-Graphs"><a href="#Erdos-Renyi-Random-Graphs" class="headerlink" title="Erdös-Renyi Random Graphs"></a>Erdös-Renyi Random Graphs</h3><p>非常的简单，即：undirected graph with n nodes, and m edges picked uniformly at random. 需要的参数就是节点数和边数了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genErdosRenyi</span>(<span class="params">N=<span class="number">5242</span>, E=<span class="number">14484</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string">    :param - E: number of edges</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Erdos-Renyi graph with N nodes and E edges</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    Graph = snap.PUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        Graph.AddNode(i)</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> cnt &lt; E:</span><br><span class="line">        src = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        dst = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Graph.IsEdge(src, dst):</span><br><span class="line">            Graph.AddEdge(src, dst)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br></pre></td></tr></table></figure><h3 id="Small-World-Random-Network"><a href="#Small-World-Random-Network" class="headerlink" title="Small-World Random Network"></a>Small-World Random Network</h3><p>生成Small-World Random Network需要三步。分别为：</p><ol><li>Begin with N nodes arranged as a ring, i.e. imagine the nodes form a circle and each node is connected to its two direct neighbors.</li><li>Connect each node to the neighbors of its neighbors.</li><li>Randomly select some given number pairs of nodes not yet connected and add an edge between them.</li></ol><p>实现起来并不困难，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCircle</span>(<span class="params">N=<span class="number">5242</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Circle graph with N nodes and N edges. Imagine the nodes form a</span></span><br><span class="line"><span class="string">        circle and each node is connected to its two direct neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    Graph = snap.PUNGraph.New()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        Graph.AddNode(i)</span><br><span class="line">    Graph.AddEdge(N - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N - <span class="number">1</span>):</span><br><span class="line">        Graph.AddEdge(i, i + <span class="number">1</span>)</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connectNbrOfNbr</span>(<span class="params">Graph, N=<span class="number">5242</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing a circle graph on N nodes</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Graph object with additional N edges added by connecting each node</span></span><br><span class="line"><span class="string">        to the neighbors of its neighbors</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        n1 = (i + <span class="number">1</span> + N) % N</span><br><span class="line">        n2 = (i - <span class="number">1</span> + N) % N</span><br><span class="line">        Graph.AddEdge(n1, n2)</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connectRandomNodes</span>(<span class="params">Graph, M=<span class="number">4000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string">    :param - M: number of edges to be added</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Graph object with additional M edges added by connecting M randomly</span></span><br><span class="line"><span class="string">        selected pairs of nodes not already connected.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    N = Graph.GetNodes()</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> cnt &lt; M:</span><br><span class="line">        src = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        dst = np.random.randint(<span class="number">0</span>, N)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Graph.IsEdge(src, dst):</span><br><span class="line">            Graph.AddEdge(src, dst)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> Graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genSmallWorld</span>(<span class="params">N=<span class="number">5242</span>, E=<span class="number">14484</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - N: number of nodes</span></span><br><span class="line"><span class="string">    :param - E: number of edges</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: snap.PUNGraph</span></span><br><span class="line"><span class="string">    return: Small-World graph with N nodes and E edges</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Graph = genCircle(N)</span><br><span class="line">    Graph = connectNbrOfNbr(Graph, N)</span><br><span class="line">    Graph = connectRandomNodes(Graph, <span class="number">4000</span>)</span><br><span class="line">    <span class="keyword">return</span> Graph</span><br></pre></td></tr></table></figure><h3 id="Question-1-1-Degree-Distribution"><a href="#Question-1-1-Degree-Distribution" class="headerlink" title="Question 1.1 Degree Distribution"></a>Question 1.1 Degree Distribution</h3><p>计算Degree Distribution。Degree Distribution就是：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/XTKUEY1jAedIkOM.png" alt="image-20201028163717245" style="zoom:50%;" /><p>这里并不需要计算出分数，而是直接将度为k的节点数以log-log图画出来。</p><p>这里画图使用matplotlib，首先要生成x,y坐标上对应的数。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDataPointsToPlot</span>(<span class="params">Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return values:</span></span><br><span class="line"><span class="string">    X: list of degrees</span></span><br><span class="line"><span class="string">    Y: list of frequencies: Y[i] = fraction of nodes with degree X[i]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    X, Y = [], []</span><br><span class="line">    DegToCntV = snap.TIntPrV()</span><br><span class="line">    snap.GetDegCnt(Graph, DegToCntV)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> DegToCntV:</span><br><span class="line">        X.append(item.GetVal1())</span><br><span class="line">        Y.append(item.GetVal2())</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>画图部分代码已给出，这里不再列出。值得注意的是画log-log图使用<code>plt.loglog</code>即可。</p><h4 id="Answers"><a href="#Answers" class="headerlink" title="Answers"></a>Answers</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/OLJ8VwY4PeaIBxM.png" alt="image-20201028164113889" style="zoom:67%;" /><h3 id="Question-1-2-Clustering-Coefficient"><a href="#Question-1-2-Clustering-Coefficient" class="headerlink" title="Question 1.2 Clustering Coefficient"></a>Question 1.2 Clustering Coefficient</h3><p>计算Clustering Coefficient。公式如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/h2bqeKRVTHPc3uA.png" alt="image-20201028164303055" style="zoom:50%;" /><p>代码如下，可能稍微负责的就是找该节点邻居间边的数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcClusteringCoefficientSingleNode</span>(<span class="params">Node, Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Node: node from snap.PUNGraph object. Graph.Nodes() will give an</span></span><br><span class="line"><span class="string">                   iterable of nodes in a graph</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: float</span></span><br><span class="line"><span class="string">    returns: local clustering coeffient of Node</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here!</span></span><br><span class="line">    C = <span class="number">0.0</span></span><br><span class="line">    neigbors = []</span><br><span class="line">    deg = Node.GetDeg()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(deg):</span><br><span class="line">        neigbors.append(Graph.GetNI(Node.GetNbrNId(i)))</span><br><span class="line">    cnt_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(deg):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            cnt_nbr += neigbors[i].IsInNId(neigbors[j].GetId())</span><br><span class="line">    <span class="keyword">if</span> deg &gt;= <span class="number">2</span>:</span><br><span class="line">        C = <span class="number">2</span> * cnt_nbr / (deg * (deg - <span class="number">1.0</span>))</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcClusteringCoefficient</span>(<span class="params">Graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param - Graph: snap.PUNGraph object representing an undirected graph</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return type: float</span></span><br><span class="line"><span class="string">    returns: clustering coeffient of Graph</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Your code here! If you filled out calcClusteringCoefficientSingleNode,</span></span><br><span class="line">    <span class="comment">#       you&#x27;ll probably want to call it in a loop here</span></span><br><span class="line">    C = <span class="number">0.0</span></span><br><span class="line">    V = Graph.GetNodes()</span><br><span class="line">    <span class="keyword">for</span> NI <span class="keyword">in</span> Graph.Nodes():</span><br><span class="line">        Ci = calcClusteringCoefficientSingleNode(NI, Graph)</span><br><span class="line">        C = C + Ci</span><br><span class="line">    C = C / V</span><br><span class="line"></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> C</span><br></pre></td></tr></table></figure><h4 id="Answers-1"><a href="#Answers-1" class="headerlink" title="Answers"></a>Answers</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/CitYa4IGqexDrbW.png" alt="image-20201028165111378"  /><h2 id="Part-2-Structural-Roles-Rolx-and-ReFex"><a href="#Part-2-Structural-Roles-Rolx-and-ReFex" class="headerlink" title="Part 2 Structural Roles: Rolx and ReFex"></a>Part 2 Structural Roles: Rolx and ReFex</h2><p>本部分主要是implement计算每个节点Structural Role的方法（实现Rolx 和 ReFex方法），可以看做一种对于每个节点拓扑信息的提取方法，将每个节点的拓扑信息变为一个向量，即实现一个feature extraction的方法。这里的feature extraction主要包含两步：</p><ol><li>Basic Features: First extract basic local features from every node.</li><li>Recursive Features: Then aggregate the basic features along graph edges so that global features are also obtained.</li></ol><h3 id="Basic-Features"><a href="#Basic-Features" class="headerlink" title="Basic Features"></a>Basic Features</h3><p>首先是计算basic feature，主要指的是节点本身的local features。这里计算的basic feature为一个三维向量。</p><p>其三个维度的意义是：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/EdNXguTZCts9Bnj.png" alt="image-20201029171924469"></p><p>这里简单的记录一下计算2和3的方法。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/24UTaJ1bmz5vQWE.png" alt="image-20201029195853008"></p><p>有了上图，这几个关系就很明确了，将代码完善即可。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_basic_features</span>(<span class="params">node, graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    提取basic features</span></span><br><span class="line"><span class="string">    :param node: 目标节点，SNAP中的node类，而非ID，可以用GetNI()获得</span></span><br><span class="line"><span class="string">    :param graph: 目标图（无向图）</span></span><br><span class="line"><span class="string">    :return: 长度为3的array 分别为：该节点deg，egonet内部deg，进出egonet的边数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    degree = node.GetDeg()</span><br><span class="line"></span><br><span class="line">    neighbors = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算全部邻居的deg的和</span></span><br><span class="line">    total_deg_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(degree):</span><br><span class="line">        <span class="comment"># 获取全部的邻居，目标构建egonet</span></span><br><span class="line">        neighbor = graph.GetNI(node.GetNbrNId(i))</span><br><span class="line">        neighbors.append(neighbor)</span><br><span class="line">        total_deg_nbr += neighbor.GetDeg()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算邻居之间边的数量</span></span><br><span class="line">    edge_between_nbr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(neighbors)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            edge_between_nbr += neighbors[i].IsNbrNId(neighbors[j].GetId())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array((degree, edge_between_nbr + degree, total_deg_nbr - <span class="number">2</span> * edge_between_nbr - degree))</span><br></pre></td></tr></table></figure><p>我的结果为：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/CRQPthH7lSfYaWc.png" alt="image-20201029200007798" style="zoom:50%;" /><h3 id="Recursive-Features"><a href="#Recursive-Features" class="headerlink" title="Recursive Features"></a>Recursive Features</h3><p>也相对简单，使用下图的公式即可。具体细节，请参照官方作业的pdf。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/4ND6q9Hz8xIWJap.png" alt="image-20201029200049787" style="zoom: 67%;" /><p>核心函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recursive_features</span>(<span class="params">v_mat, graph</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    迭代一次</span></span><br><span class="line"><span class="string">    :param v_mat: np array</span></span><br><span class="line"><span class="string">    :param graph: 目标图</span></span><br><span class="line"><span class="string">    :return: 新的v_mat，向量长度变为原来的3倍</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cnt_node, ori_len = v_mat.shape</span><br><span class="line">    new_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    mean_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    sum_mat = np.zeros((cnt_node, ori_len))</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph.Nodes():</span><br><span class="line">        nodeId = node.GetId()</span><br><span class="line">        cnt_nbrs = node.GetDeg()</span><br><span class="line">        new_mat[nodeId] = v_mat[nodeId]</span><br><span class="line">        <span class="keyword">if</span> cnt_nbrs == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(cnt_nbrs):</span><br><span class="line">            nbr_id = node.GetNbrNId(i)</span><br><span class="line">            mean_mat[nodeId] += v_mat[nbr_id]</span><br><span class="line">            sum_mat[nodeId] += v_mat[nbr_id]</span><br><span class="line">        mean_mat[nodeId] = mean_mat[nodeId] / cnt_nbrs</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((new_mat, mean_mat, sum_mat), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>结果如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/kfYKOELZ7HrcUAy.png" alt="image-20201029200224081" style="zoom:50%;" /><h3 id="Role-Discovery"><a href="#Role-Discovery" class="headerlink" title="Role Discovery"></a>Role Discovery</h3><p>主要工作即为：根据计算出的迭代3次的cosine similarity，绘出其分布直方图；根据直方图判别有几种role（看凸起即可），并随机选一个role中的一个节点，根据其local(basic) feature绘制其2-hop子图。唯一难点可能就是2-hop子图的绘制，这里我使用了networkx来绘制。我使用了python中的set()来滤去重复节点，这种方法时间复杂度显然很高。不过对于小图而言还是能接受的。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Draw_SubGraph</span>(<span class="params">Graph, Node_id</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    绘图函数 Node 节点2-hop子图</span></span><br><span class="line"><span class="string">    :param Graph: 目标图</span></span><br><span class="line"><span class="string">    :param NI_id: 目标节点</span></span><br><span class="line"><span class="string">    :return: 无</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Nodes = []</span><br><span class="line">    Edges = []</span><br><span class="line">    center_node = Graph.GetNI(Node_id)</span><br><span class="line">    Nodes.append(Node_id)</span><br><span class="line">    colors = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有1-hop节点放进去</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(center_node.GetDeg()):</span><br><span class="line">        nbr_id = center_node.GetNbrNId(i)</span><br><span class="line">        Nodes.append(nbr_id)</span><br><span class="line">    <span class="comment"># 所有2-hop节点放进去</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Nodes)):</span><br><span class="line">        mid_node = Graph.GetNI(Nodes[i])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(mid_node.GetDeg()):</span><br><span class="line">            nbr_id = mid_node.GetNbrNId(j)</span><br><span class="line">            Nodes.append(nbr_id)</span><br><span class="line">    Nodes = list(set(Nodes))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> mid_id <span class="keyword">in</span> Nodes:</span><br><span class="line">        <span class="keyword">if</span> mid_id == Node_id:</span><br><span class="line">            colors.append(<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colors.append(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    <span class="comment"># print(Nodes)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Nodes)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">            <span class="keyword">if</span> Graph.IsEdge(Nodes[i], Nodes[j]):</span><br><span class="line">                Edges.append((Nodes[i], Nodes[j]))</span><br><span class="line">    <span class="comment"># print(len(Edges))</span></span><br><span class="line">    G = nx.Graph()</span><br><span class="line">    G.add_nodes_from(Nodes)</span><br><span class="line">    G.add_edges_from(Edges)</span><br><span class="line">    nx.draw_networkx(G, node_color=colors)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure><p>运行结果如下：分别为直方图和2-hop子图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/86VsXSJYwZ3DnNR.png" alt="image-20201029200326758" style="zoom:50%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/jgJoFmLKwrCZ2QD.png" alt="image-20201029200838918" style="zoom:50%;" /></p><h3 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h3><p>最大的感觉就是光听一遍和写完一遍代码对于整个流程完全有着不同程度的理解，还是得多练啊。。。</p><p>中间也卡了不少次，花费了不少的时间T.T</p><h2 id="Part-3-Community-detection-using-the-Louvain-algorithm"><a href="#Part-3-Community-detection-using-the-Louvain-algorithm" class="headerlink" title="Part 3 Community detection using the Louvain algorithm"></a>Part 3 Community detection using the Louvain algorithm</h2><p><strong>Community is just sets of tightly connected nodes.</strong></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/KLAc6P95G8W4dxv.png" alt="image-20201103104601046" style="zoom:67%;" /><p>如上图所示，这个就是Community Detection后的结果。这类Community detection的算法一般将：原本问题变为一个优化问题。</p><p>首先是定义一个优化目标，其应该是A measure of how well a network is partitioned into communities，该值正比于划分的好坏.  即对于每一个可能的partition都给出一个分数，目标就是找到使该分数最大的一个划分。这里使用的指标为：<strong>Modularity</strong>。定义如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/KxdW8jNcHizRuSg.png" alt="image-20201103105343050" style="zoom:80%;" />公式化之后变为：注：这里的Null model使用的是<strong>configuration model</strong>。即：有同样的degree distribution但node之间的连接是uniformly random的。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/W1ZkKhylRXFou7Q.png" alt="image-20201103104747043" style="zoom: 80%;" /><p>其取值在[-1,1]之间，It is positive if the number of edges within groups exceeds the expected number. </p><p>然而直接优化这个问题是一个NP hard问题。所以提出了<strong>Louvain algorithm</strong>。这是一种greedy algorithm。并且可以提供一种<strong>Hierarchical communities</strong>. 其整体流程如下：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/wY4eihbvVKT65F3.png" alt="image-20201103110548639" style="zoom:80%;" /><p>值得注意的是这种方法假设：community是disjoint的。</p><h3 id="Q3-1-Proof-for-Modularity-gain-when-an-isolated-node-moves-into-a-community"><a href="#Q3-1-Proof-for-Modularity-gain-when-an-isolated-node-moves-into-a-community" class="headerlink" title="Q3-1 Proof for Modularity gain when an isolated node moves into a community"></a>Q3-1 Proof for Modularity gain when an isolated node moves into a community</h3><p>证明如图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/03/65UYz17gLoMFIQq.png" alt="image-20201103150122229"></p><h3 id="Q3-2-Louvain-algorithm-on-a-16-node-network"><a href="#Q3-2-Louvain-algorithm-on-a-16-node-network" class="headerlink" title="Q3-2 Louvain algorithm on a 16 node network"></a>Q3-2 Louvain algorithm on a 16 node network</h3><p>Answers for Graph H:</p><ol><li>1</li><li>12 NOTE: 6*2</li><li>$4 \cdot Q_c = 4(\frac{12}{2m}-\frac{14^2}{4m^2})$,where $m=12\cdot4+1\cdot4=52$, Q=0.158</li></ol><p>Answers for Graph J:</p><ol><li>2</li><li>26</li><li>Q=2(26/104-(28/104)^2)=0.355</li></ol><h3 id="Q3-3-Louvain-algorithm-on-a-128-node-network"><a href="#Q3-3-Louvain-algorithm-on-a-128-node-network" class="headerlink" title="Q3-3 Louvain algorithm on a 128 node network"></a>Q3-3 Louvain algorithm on a 128 node network</h3><p>与Q3-2是相同的计算方式，比较简单，不再计算一遍了。</p><h2 id="Part-4-Spectral-clustering"><a href="#Part-4-Spectral-clustering" class="headerlink" title="Part 4 Spectral clustering"></a>Part 4 Spectral clustering</h2><p>经典的谱聚类的方法。讲得还是很精彩和清晰的。我在这里简单的总结一下整体的思路框架。</p><p>首先是谱聚类的目标，对于图做一个good partition分成不同的部分，即不同的clustering。</p><p>所以我们要有一个指标来衡量一个good partition。引出了cut。</p><p>+</p><p>这个指标的最大问题是没有将每个cluster内部的连接考虑进去，在优化该指标时，即最小化cut指标时，我们总可以找到如下的“最优指标”，但该指标显然不是我们期望的。原因就是没有考虑每个cluster内部的连接。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/BWUP9Rpac2CGjdl.png" alt="image-20201104163807540" style="zoom:67%;" /><p>为了解决这个问题，引入了指标<strong>Conductance</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/aFuU1NYBLbfhvie.png" alt="image-20201104163936635"></p><p>相当于对于cluster的连接做了一个正则，这样优化该指标，我们可以找到一个更好的partition。</p><p>下面作者引入了邻接矩阵的谱，拉普拉斯矩阵的谱（拉普拉斯矩阵有很好的性质，半正定矩阵，必定有全1向量为eigenvector，其对应了eigenvalue为0），最终将寻找第二小的特征值和特征向量与一个partition挂钩（直观解释），并且从理论推导出小的特征值为最优Conductance的一个下界。这样就寻找将拉普拉斯矩阵第二小的特征值和特征向量和找到最优partition联系了起来。简单回顾Spectral Clustering后，开始完成这部分的作业。</p><h3 id="Q4-1-A-Spectral-Algorithm-for-Normalized-Cut-Minimization-Foundations"><a href="#Q4-1-A-Spectral-Algorithm-for-Normalized-Cut-Minimization-Foundations" class="headerlink" title="Q4-1 A Spectral Algorithm for Normalized Cut Minimization: Foundations"></a>Q4-1 A Spectral Algorithm for Normalized Cut Minimization: Foundations</h3><p>一些公式推导的题目，具体题目见官网源文件。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/AUeCo94uExJrwn1.png" alt="image-20201104174652449" style="zoom:80%;" /><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/OgNUo1s7FDqtGkX.png" alt="image-20201104174758916"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/dgx7PFJvmIj5Qzu.png" alt="image-20201104174838628"></p><h3 id="Q4-2-Normalized-Cut-Minimization-Solving-for-the-Minimizer（Uncertain）"><a href="#Q4-2-Normalized-Cut-Minimization-Solving-for-the-Minimizer（Uncertain）" class="headerlink" title="Q4-2 Normalized Cut Minimization: Solving for the Minimizer（Uncertain）"></a>Q4-2 Normalized Cut Minimization: Solving for the Minimizer（Uncertain）</h3><p>本部分的证明不太确定，最后部分使用的Rayleigh定理不太确定。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/CYFUQRcvtD4VZMO.png" alt="image-20201104210042302" style="zoom:67%;" /><h3 id="Q4-3-Relating-Modularity-to-Cuts-and-Volumes"><a href="#Q4-3-Relating-Modularity-to-Cuts-and-Volumes" class="headerlink" title="Q4-3 Relating Modularity to Cuts and Volumes"></a>Q4-3 Relating Modularity to Cuts and Volumes</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/04/FhUZy3amOexgTQi.png" alt="image-20201104205957243"></p><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>至此homework1的内容全部结束了，整体而言的难度不是很高，在编程上有snap库极大的简化了编程上的难度。整体来讲homework1涉及了很多分析图的方法和非常多的基本却关键的概念。比如：如何衡量一个图的特点（通过一些全局的指标比如：degree distribution，Clustering Coefficient，diameter），这是要和random graph做对比的。包括后面的涉及的如何衡量一个节点的特征（Role），motifs，graphlet的概念，community/group的概念，这些都是分析复杂图的基石。</p><p>整体而言，目前这部分课程也让我对于图论产生了很大的兴趣，由于本人之前没有离散数学or图论的基础，对于一些例如induced graph的概念并不是清楚，之后计划会额外自学一些关于图论方面的知识。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS224w-HomeWork-1&quot;&gt;&lt;a href=&quot;#CS224w-HomeWork-1&quot; class=&quot;headerlink&quot; title=&quot;CS224w HomeWork 1&quot;&gt;&lt;/a&gt;CS224w HomeWork 1&lt;/h1&gt;&lt;p&gt;本文旨在记录CS22
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS224w" scheme="http://canVa4.github.io/tags/CS224w/"/>
    
      <category term="GNN" scheme="http://canVa4.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>FFT idea for Displacement Measurement</title>
    <link href="http://canva4.github.io/2020/10/27/FFT-idea-for-Displacement-Measurement/"/>
    <id>http://canva4.github.io/2020/10/27/FFT-idea-for-Displacement-Measurement/</id>
    <published>2020-10-27T12:00:33.000Z</published>
    <updated>2020-10-27T13:12:02.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FFT-idea-for-Displacement-Measurement"><a href="#FFT-idea-for-Displacement-Measurement" class="headerlink" title="FFT idea for Displacement Measurement"></a>FFT idea for Displacement Measurement</h1><p>Displacement Measurement任务核心：利用加速度计、陀螺仪（加速度和陀螺仪数据）来计算位移。</p><h2 id="Origin-Paper-Reading"><a href="#Origin-Paper-Reading" class="headerlink" title="Origin Paper Reading"></a>Origin Paper Reading</h2><p>原始思路：核心：计算倾角θ。假设：底部固定，只发生上部的形变。</p><p>计算倾角使用：加速度和陀螺仪来计算，二者的计算结果结合，通过参数α来调节。</p><p>在获取加速度数据前，先通过低通滤波器滤除震动噪声；获取角加速度前，先通过高通滤波器滤除漂移。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/W3rebtwV5Ohp1Yz.png" alt="image-20201027201049406" style="zoom:50%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/x5nZbIdwqLCJVfj.png" alt="image-20201027201103750" style="zoom: 50%;" /><h2 id="Idae-Using-FFT-to-calculate-Displacement"><a href="#Idae-Using-FFT-to-calculate-Displacement" class="headerlink" title="Idae: Using FFT to calculate Displacement"></a>Idae: Using FFT to calculate Displacement</h2><p>一时域信号x(t)，设其样本长度为T。则其傅里叶变换为$X(f)=\int_{0}^{T}x(t)e^{-j2\pi ft}dt$。</p><p>信号x(t)被采集（即采样）后，变为离散形式$x(nt_s)$，$t_s$为采样周期。设在T时间内采样了N个数据，就有：</p><ul><li>$$X(k)=\sum_{n=0}^{N-1}x(n)e^{-j(\frac{2\pi}{N})nk}$$</li></ul><p>其对应的傅里叶反变换为：</p><ul><li>$$x(n)=\frac{1}{N}\sum_{n=0}^{N-1}X(k)e^{j(\frac{2\pi}{N})nk}$$</li></ul><p>傅里叶变换后得到的X(k)是一个长度为N的离散复数序列。其第k个数据为：</p><p>$$X(k)=X(k/T)=a_k+jb_k$$</p><p>其代表了x(t)中频率为k/T的简谐运动分量，该分量用$x_k$表示。应有：</p><p>$$x_k=A_k\cos{(2\pi kt/T + \phi_k)}$$</p><p>其中$A_k=\sqrt{a_k^2+b_k^2}$为其幅值，$\phi_k=arctan(b_k/a_k)$为其相角。</p><p>运动台产生的应为一系列简谐运动的叠加，假设通过建筑物系统后仍为一系列简谐运动的叠加（看成时不变系统，如果有这个性质最好了，滤波就很方便，需要一些结构方面知识来确认），那么可以方便的滤除非明显频点的幅值，来降低噪声。若没有此性质，目前初步计划是使用一个低通滤波来滤除噪声（同原始论文）。</p><p>假设我们收到了加速度信号a(n)，对其使用傅里叶变换，得到：</p><p>$$A(k)=\sum_{n=0}^{N-1}a(n)e^{-j(\frac{2\pi}{N})nk}=u_k+jv_k$$</p><p>设真实的位移（Displacement）序列为d(n)，傅里叶变换后为D(k)。D(k)自然也能也成简谐运动的形式。根据加速度和位移是积分的关系。</p><p>若加速度为$a = Acos(\omega t+\phi)$，则位移$x=\int\int adt=\frac{A}{\omega^2}cos(\omega t+\phi-\pi)$。</p><p>直接可得</p><p>$$d_{1k}=\frac{A_k}{\omega_k^2}cos(\phi_k-\pi)$$</p><p>$$d_{2k}=\frac{A_k}{\omega_k^2}sin(\phi_k-\pi)$$</p><p>其中$A_k=\sqrt{u_k^2+v_k^2}$，$\phi_k=arctan(v_k/u_k)$，$w_k=2\pi k/T$。</p><p>而$D(k)=d_{1k}+jd_{2k}$。这样我们就得到了D(k)，再将D(k)做傅里叶反变换即可得到d(n)。即一段时间内的位移情况。</p><p>TODO：</p><ol><li>测试此方法。</li><li>思考如何引入角加速度的信息。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FFT-idea-for-Displacement-Measurement&quot;&gt;&lt;a href=&quot;#FFT-idea-for-Displacement-Measurement&quot; class=&quot;headerlink&quot; title=&quot;FFT idea for Displ
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="FFT" scheme="http://canVa4.github.io/tags/FFT/"/>
    
      <category term="Displacement Measurement" scheme="http://canVa4.github.io/tags/Displacement-Measurement/"/>
    
  </entry>
  
  <entry>
    <title>Crack Detection Paper Reading</title>
    <link href="http://canva4.github.io/2020/10/23/Crack-Detection-Paper-Reading/"/>
    <id>http://canva4.github.io/2020/10/23/Crack-Detection-Paper-Reading/</id>
    <published>2020-10-23T11:51:09.000Z</published>
    <updated>2020-12-07T04:05:44.634Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Crack-Detection-Paper-Reading"><a href="#Crack-Detection-Paper-Reading" class="headerlink" title="Crack Detection Paper Reading"></a>Crack Detection Paper Reading</h1><h2 id="Paper-LIST"><a href="#Paper-LIST" class="headerlink" title="Paper LIST:"></a>Paper LIST:</h2><ol><li><p>Autonomous concrete crack detection using deep fully convolutional neural network 2019</p></li><li><p>DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection 2018</p></li><li><p>Holistically-Nested Edge Detection(经典边缘检测算法HED) 2015</p></li><li><p>Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection 2019</p></li><li><p>A review on computer vision based defect detection and condition assessment of concrete and asphalt civil infrastructure 2015 <strong>TODO: 精读</strong></p></li><li><p>CrackGAN 2020</p></li><li><p>SegNet 2016</p></li><li><p>Feature Pyramid Networks for Object Detection 2017</p></li><li><p>RCF</p></li><li><p>Bi-Directional Cascade Network for Perceptual Edge Detection 2019</p><p><strong>THE FOLLOWINGS ARE MAINLY CLASSIFICATION TASK</strong></p></li><li><p>Image based techniques for crack detection, classification and quantification in asphalt pavement: a review 2017 <strong>CLF</strong> TODO</p></li><li><p>Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning 2018 <strong>Structural Health Monitoring</strong></p></li><li><p>Review and Analysis of Crack Detection and Classification Techniques based on Crack Types 2019 TODO</p></li><li><p>Concrete Cracks Detection Based on Deep Learning Image Classification <a href="https://www.mdpi.com/2504-3900/2/8/489">LINK</a> 2019</p></li><li><p>Multi-scale classification network for road crack detection 2018 <strong>CLF</strong> TODO</p></li><li><p>Learning relaxed deep supervision for better edge detection 2016 （improvement for HED）</p></li></ol><p><strong>Semantic Segmentation:</strong></p><ol><li>Dilated Residual Networks 2017 <a href="https://arxiv.org/abs/1705.09914">LINK</a></li><li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 2017 <a href="https://arxiv.org/abs/1606.00915">LINK</a></li><li>Multi-Scale Context Aggregation by Dilated Convolutions 2016 <a href="https://arxiv.org/abs/1511.07122">LINK</a></li><li>Understanding Convolution for Semantic Segmentation 2018 <a href="https://arxiv.org/abs/1702.08502">LINK</a></li><li>Rethinking Atrous Convolution for Semantic Image Segmentation 2017 <a href="https://arxiv.org/abs/1706.05587">LINK</a></li></ol><h2 id="Reading-More："><a href="#Reading-More：" class="headerlink" title="Reading More："></a>Reading More：</h2><p>From Paper number 16：</p><ol><li>Deepedge 2015</li><li>High-for-low and low-for-high: Efficient Boundary detection from deep object features 2015</li><li>Canny A computational approach to edge detection 1986</li><li>Deeply supervised Nets 2015</li></ol><h2 id="In-General"><a href="#In-General" class="headerlink" title="In General"></a>In General</h2><p>This part is for answering some general questions for this topic.</p><h3 id="Q1-Why-using-Deep-Learning-methods-instead-of-traditional-CV-algorithm"><a href="#Q1-Why-using-Deep-Learning-methods-instead-of-traditional-CV-algorithm" class="headerlink" title="Q1: Why using Deep Learning  methods instead of traditional CV algorithm:"></a>Q1: Why using Deep Learning  methods instead of traditional CV algorithm:</h3><p>Cracks may constantly suffer from noise in the background, leading to poor continuity and low contrast. That lead to the traditional CV algorithms have bad performance on these cases.</p><h3 id="Q2-Which-kind-of-tasks-in-CV-are-corresponding-to-Crack-Detection"><a href="#Q2-Which-kind-of-tasks-in-CV-are-corresponding-to-Crack-Detection" class="headerlink" title="Q2: Which kind of tasks in CV are corresponding to Crack Detection?"></a>Q2: Which kind of tasks in CV are corresponding to Crack Detection?</h3><p>Meanly 3 task:</p><ol><li>Semantic Segmentation</li><li>Edge Detection</li><li>Classification</li></ol><h3 id="Q3-Main-DataSets"><a href="#Q3-Main-DataSets" class="headerlink" title="Q3: Main DataSets:"></a>Q3: Main DataSets:</h3><p><strong>Concrete:</strong></p><ol><li>CRACK500</li><li>GAPs384</li><li>Cracktree200</li><li>sCFD</li><li>Aigle-RN &amp; ESAR &amp; LCMS</li></ol><p><strong>Pavement:</strong></p><ol><li>CFD</li><li>CGD</li></ol><h3 id="Q4-Metric-For-Crack-Detection-Task"><a href="#Q4-Metric-For-Crack-Detection-Task" class="headerlink" title="Q4: Metric For Crack Detection Task"></a>Q4: Metric For Crack Detection Task</h3><ol><li>最常见的指标之一就是PR（precision and recall）了，将二者统一，即变为F1-measure，这个在各个论文中评价crack detection 模型的性能中经常出现。</li></ol><ul><li>$$F_1=\frac{2*PR}{P+R}$$</li></ul><p>该值位于0~1之间，约接近1认为模型性能越好。</p><p>其缺点为：无法很好的表明检测到的裂缝和地面真实情况之间的重叠程度，尤其是在裂缝较大时。</p><ol start="2"><li>ODS &amp; OIS</li></ol><p>这两个是edge detection中的指标。边缘检测领域的标准准则是固定scale or threshhold(ODS)数据集上的最佳F值，每个图像中最佳scale or threshhold数据集上的平均F值(OIS)。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/FoQfzMcXZ89ngas.png" alt="image-20201029215820897" style="zoom:80%;" /><ol start="3"><li><p>AP(准确率)</p></li><li><p>FPHBN中提出的average intersection over union(AIU)。</p></li></ol><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/jbvE5lNcuyCkQKO.png" alt="image-20201029220020769" style="zoom:80%;" /><p>其中$N_t$表示阈值t的总个数,t在区间[0.01,0.99]内，间隔是0.01;对于给定的阈值t, $N_{pg}^t{}$为预测区域与真实裂缝区域相交（重叠）区域的像素个数，$N_p^t$和$N_g^t$分别表示预测裂缝区域和真值裂缝区域的像素数。因此，AIU在0到1之间，值越大，性能越好。数据集的AIU是数据集中所有图像的AIU的平均值。</p><p>提出的主要目的就是弥补PR指标的缺点，作为一个complementary measurement。AIU takes the width information into consideration to evaluate detections and illustrates the overall overlap extent between detections and ground truth.</p><h1 id="Paper’s-Detail"><a href="#Paper’s-Detail" class="headerlink" title="Paper’s Detail:"></a>Paper’s Detail:</h1><p>This part included the detail of each paper that I have perused.</p><h2 id="Autonomous-concrete-crack-detection-using-deep-fully-convolutional-neural-network"><a href="#Autonomous-concrete-crack-detection-using-deep-fully-convolutional-neural-network" class="headerlink" title="Autonomous concrete crack detection using deep fully convolutional neural network"></a>Autonomous concrete crack detection using deep fully convolutional neural network</h2><p>Author: Cao Vu Dung, Le Duc Anh     <a href="https://www.sciencedirect.com/science/article/pii/S0926580518306745">Paper Link</a></p><p>Journal: Automation in Construction 2019</p><p>Key Point: <strong>Concrete Crack Detection,  FCN(Fully CNN), Semantic Segmentation(Use FCN), Evaluate crack density, Test the Model on a real video</strong></p><p>数据集：<a href="https://data.mendeley.com/datasets/5y9wdsg2zt/1">https://data.mendeley.com/datasets/5y9wdsg2zt/1</a></p><p>模型很简单：Classification + Semantic Segmentation(FCN)</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/23/FEscvZiNOMCJPYK.png" alt="image-20201023202412382"></p><p>Semantic Segmentation Result：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/23/jpxeIOHzhwlG67m.png" alt="image-20201023202652414"></p><p>总结：特点不多，主要将CV中的Semantic Segmentation的经典方法（FCN）引入至Crack Detection领域，并且使用了较大的预训练( VGG16, ResNet, Inception)模型，获得结论使用预训练模型可以获得更好的预测结果，在实验部分做了不少工作。最后用了一个现实中的视频来验证一下model的效果。</p><h2 id="DeepCrack-Learning-Hierarchical-Convolutional-Features-for-Crack-Detection"><a href="#DeepCrack-Learning-Hierarchical-Convolutional-Features-for-Crack-Detection" class="headerlink" title="DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection"></a>DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection</h2><p>Author: Qin Zou er al  <a href="https://ieeexplore.ieee.org/document/8517148/similar">Paper LINK</a></p><p>Journal: IEEE TRANSACTIONS ON IMAGE PROCESSING(Published at 2018 Oct)</p><p>KEY WORDS: <strong>基于SegNet，fuse the feature map from encoder and decoder at different scales，crack/line/edge detection</strong></p><p>整体模型：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/24/FZEbNWKCVmY38Ap.png" alt="image-20201024200409648"></p><p><strong>模型核心：</strong></p><p>In DeepCrack, they first pairwisely fuse the convolutional features of the encoder network and decoder network at each scale, which produces the single-scale fused feature map, and then combine the fused feature maps at all scales into a multi-scale fusion map for crack detection. 这也是模型相比于SegNet最大的改进。</p><p>下图为SegNet的模型结构：</p><p>​    </p><p>下图为encoder和decoder的feature map是如何聚合的（即论文中的skip-layer）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/24/wT7AYeSmjWBNgus.png" alt="image-20201024200734542"></p><p><strong>Loss Function</strong></p><p>使用的是cross entropy（并没有对于不平衡类别 即：imbalance classification做处理）</p><p>最终的loss是由每层聚合的feature map和多层聚合的feature map的损失相加。</p><p><strong>Experiments And Results</strong></p><p>Modeling by <strong>caffee</strong></p><p><strong>Datasets</strong>: </p><ol><li>Training: CrackTree260</li><li>Testing: CRKWH100, CrackLS315, Stone331</li></ol><p><strong>Compare With Other Models</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/cg2CBjQtMNGwLx7.png" alt="image-20201027090600436"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/l6FXZJpDygiqBWf.png" alt="image-20201027090703558"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/twXOKVsmTEg6Pvu.png" alt="image-20201027090730577"></p><p><strong>Other Experiments</strong></p><ol><li>作者也比较了每一个scale对于最终结果的影响，即：在最终的loss中对于每一层的loss增加一个权重。</li><li>是否使用预训练模型的比较。有趣的是预训练的效果并不好，作者给出解释是：because that the pretrained model is well fit for nature image segmentation and is impossible or very difficult to be fine-tuned for crack detection.</li><li>作者讨论了在真实label中加入噪声对于模型的影响。结论是：对于噪声并不敏感。</li><li>作者还比较了不同的up-sampling方法的影响。即：传统的在max pooling时记录index的方法和bilinear interpolation的方法。结论：传统的在max pooling时记录index的up-sampling方法更好。</li><li>Different Weights on the Crack and Non-Crack Background，即在计算loss时对于不同的类别（即 crack or non-crack）赋予不同的权重。结论：会对性能提升，降低了类别不平衡的影响。</li><li>作者还比较了<strong>Running Efficiency</strong>, 上图给出的FPS为对于512×512大小的图片的实时处理速度。DeepCrack为：0.153 second per image。不过其使用的设备是GeForce GTX TITAN-X GPU和2.3GHz主频的E5-2630 CPU.</li></ol><h2 id="Holistically-Nested-Edge-Detection-HED"><a href="#Holistically-Nested-Edge-Detection-HED" class="headerlink" title="Holistically-Nested Edge Detection(HED)"></a>Holistically-Nested Edge Detection(HED)</h2><p>Author: Saining Xie er al  <a href="https://arxiv.org/abs/1504.06375">Paper LINK</a></p><p>Conference: CVPR 2015</p><p>KEY WORDS: <strong>基于VGGNET，mutil-scale fuse，edge detection</strong></p><p>此paper为使用CNN(VGG)的经典边缘检测模型，后期出现在多篇Crack Detection模型的baseline中。下一篇Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection得部分思路也是来源于HED的。这里主要总结和介绍一下模型部分。</p><p>其中Holistically表示该算法试图训练一个image-to-image的网络；Nested则强调在生成的输出过程中通过不断的集成和学习得到更精确的边缘预测图的过程。HED使用VGG改造的网络，针对整张图片提取特征信息，使用multi-scale fusion，multi-loss的方法。</p><p>值得注意的是，该方法的速度还是比较快的，可以达到0.4s per image(on the NYU Depth dataset)</p><p><strong>Model Detail</strong></p><p>下图为整个HED的模型结构。特征提取使用的是pre-trained的VGG网络。</p><p>作者给出performance提升的原因是：</p><ol><li>patch-based CNN edge detection methods</li><li>FCN-like image-to-image training allows models to simultaneously train on a significantly larger amount of samples</li><li>deep supervision in model guides the learning of more transparent features</li><li>interpolating the side outputs in the end-to-end learning encourages coherent contributions from each layer(used in fuse)</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/Oh8aFMxQpDrIdje.png" alt="image-20201027155719185"></p><p>本文的一大亮点就是使用了与以往不同的multi-scale fuse的方法。如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/F2GtTpIRcxJqyDu.png" alt="image-20201027163733313"></p><p>(a)Multi-stream learning 示意图，可以看到图中的平行的网络下（不同的网络有不同的参数和receptive field），输入是同时输进去，经过不同路的network之后，再连接到一个global out layer得到输出。</p><p>(b)Skip-layer network learning 示意图，该方法主要连接各个单的初始网络流得到特征图，并将图结合在一起输出。</p><p>这里（a）和（b）都是使用一个输出的loss函数进行单一的回归预测，而边缘检测可能通过多个回归预测得到结合的边缘图效果更好。</p><p>(c)Single model on multiple inputs 示意图，单一网络，图像resize方法得到多尺度进行输入。当用于test端时，有点类似于集成学习的感觉。</p><p>(d)Training independent networks ，通过多个独立网络分别对不同深度和输出loss进行多尺度预测，该方法下训练样本量较大。</p><p>(e)Holistically-nested networks，本文提出的结构，从（d）演化来，类似地是一个相互独立多网络多尺度预测系统，但是将multiple side outputs组合成一个单一深度网络。</p><p>这种方法的好处在于：基于hidden layer的supervision更有利于performance的提升；也相对的减小了模型的复杂度；在fuse时可以自由的调配每个scale的权重，或者通过学习来学到一个更好的参数。</p><p>作者主要使用了VGG16，并对其进行小幅度修改。</p><p><strong>Loss Function</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/QbWvcajnZixkYL7.png" alt="image-20201027170404471"></p><p>总的side-output loss为每一个side-output的加权和，权重可以通过学习来获得。由于edge detection任务的特性，大部分pixel为非edge的，所以存在明显的类别不平衡，每个side-output的loss使用下图的公式来解决这个问题。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/F3lSQWU8Ndg2J5b.png" alt="image-20201027170511603"></p><p>另外一部分的loss是fuse loss。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/8yiP3qKzN2dwOrS.png" alt="image-20201027170911639"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/BJA6Lk8pPaTo1dN.png" alt="image-20201027170938292"></p><p>h为fuse的一组参数。最终的loss和学习目标为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/Wa4JQD8bkIrnVhN.png" alt="image-20201027171037647"></p><p><strong>Experiment Results:</strong></p><p>Results on BSDS500 datasets.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/27/HoQmpjAEGfNy5LC.png" alt="image-20201027195428396"></p><h2 id="Feature-Pyramid-and-Hierarchical-Boosting-Network-for-Pavement-Crack-Detection"><a href="#Feature-Pyramid-and-Hierarchical-Boosting-Network-for-Pavement-Crack-Detection" class="headerlink" title="Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection"></a>Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection</h2><p>Author: Fan Yang er al  <a href="https://arxiv.org/abs/1901.06340">Paper LINK</a></p><p>Conference: CVPR 2019</p><p>KEY WORDS: <strong>Pavement Crack Detection，主要将Feature Pyramid Network(FPN)应用于Crack Detection task，提出新的 measurement for crack detection: average intersection over union (AIU)</strong></p><p><strong>模型细节：</strong></p><p>FPHBN使用了HED（本篇文档中也有HED的介绍与总结）作为其backbone net，在其之上引入了FPN的思想（FPN在本文档中也有介绍与总结）。</p><p>下图就是HED与FPHBN的模型对比图，可以很明显的看到，在HED后面加入了类似于FPN的思想（top-down + skip connection）和Hierarchy Boostings。Hierarchy Boostings可以起到平衡top layer与bottom layer的权重，可以让模型pay attention to hard examples.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/tQwqR5f9MONEAGk.png" alt="image-20201029210048041" style="zoom: 80%;" /><p>整个模型分为四个部分：</p><ol><li>A bottom-up architecture for hierarchical feature extraction</li><li>A feature pyramid for merging context information to lower layers using a top-down architecture</li><li>Side networks for deep supervision learning. The side network at each level performs crack prediction individually.</li><li>A hierarchical boosting module to adjust sample weights in a nested way</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/J6YuzEiAtFc49Le.png" alt="image-20201029212056151"></p><p>这里主要介绍一下Hierarchical Boosting，1~3部分即为HED+FPN，这二者在本文中都有介绍。</p><p>使用Hierarchical Boosting是为了解决HED的一个缺点，即：the network cannot effectively learn parameters from misclassified samples during training phase. 这个是因为edge detection task 本质的类别不平衡导致的，虽然在HED中引入了系数β来损失函数对不同类别的权重，但这种方法并不能区分easy and hard samples，因为类别不平衡很容易导致损失函数主要被不平衡类别占据。</p><p>作者针对这个问题（还有一些其他解决方法），提出了Hierarchical Boosting来reweigh samples。</p><p>想法来源于，回看feature pyramid层，可以看到，上层的信息会传递到下层，这种信息会告诉下层那些样本是hard的，这样这些下层的网络就可以pay attention to这些hard样本了。于是作者就想到来facilitating communication between adjacent side networks.</p><p>实现起来就是重写每一个side network的损失函数：$d_i^{m+1}$ 为第m+1个side network的预测值与真实值在第i个像素的差异。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/BtihxYVnDHAX2yJ.png" alt="image-20201029215013161" style="zoom:80%;" /><p><strong>Experiments And Results</strong></p><p>可以看到在inference时的速度也比HED有了提升。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/Jgt6xp2SoLkrzAG.png" alt="image-20201029215415393" style="zoom:80%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/29/q5Y9OGf8UuWPetN.png" alt="image-20201029215633706" style="zoom:80%;" /><p>Related Paper:</p><p>Y. Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai, “Richer convolutional features for edge detection,”</p><p>Deeply-supervised nets</p><h2 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h2><p>Author: Lin er al   <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">Paper Link</a></p><p>Journal: CVPR 2017</p><p>Key Point: <strong>Adding Pyramid representation into Deep Learning Methods, Multi-scale and pyramidal hierarchy</strong></p><p>Other’s Note: <a href="https://zhuanlan.zhihu.com/p/78160468">ZhiHu Link</a></p><p><strong>总结：</strong> FPN本质是另一种特征提取的结构，获得的representation可以用于很多不同的task，例如object detection 或者 semantic segmentation(crack detection就可以看为是这种任务)。其最大的好处是：在保持原有性能的情况下，极大的降低了模型的复杂度，让模型更feasible。现在FPN已经经常作为各种Detecton和segmentation算法的标准组件。</p><p>下图给出了几种常见的pyramid形式的特征提取方法。例如b方法就是常见的CNN所使用的结构。</p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201028201708812.png" alt="image-20201028201708812" style="zoom:67%;" /><p>值得注意的是，在图中边框更粗的代表了更high-level的语义的特征（很符合直观）。个人理解：许多常见的multi-scale方法就是类似c的方法，将每层CNN的结果聚合作为预测结果，如SSD(Single Shot Detector)，SegNet和上面的HED。作者给出的(c)方法的缺点为：This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p><p>(a) 中的 Featurized image pyramid结构在ImageNet 或 COCO上取得了很好的表现，因为这种方法产生了multi-scale的representation而且每层特征提取都semantically strong。这种方法的主要问题就是：inference time过长，导致实际难以应用。</p><p>下图为FPN网络的具体图示，作者也引入了类似ResNet的skip connection。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/brWwTLKR79zapf2.png" alt="image-20201028204220471" style="zoom: 67%;" /><p>整体结构主要包含三个部分：</p><ol><li>Bottom-up pathway 正常使用backbone网络，如Resnet</li><li>Top-down pathway 使用nearest neighbor upsampling</li><li>Lateral connections 如下图所示。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/28/1QGbFcIAe9O67vE.png" alt="image-20201028210642456"></p><h2 id="SegNet-SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation"><a href="#SegNet-SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation" class="headerlink" title="SegNet: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"></a>SegNet: SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</h2><p>Author: Vijay Badrinarayanan er al  </p><p>Journal: TPAMI 2015</p><p>Key Point: <strong>Semantic Segmentation，compare different upsampling methods</strong></p><ul><li>本文提出的SegNet和FCN、DeepLab-LargeFOV、DeconvNet做了比较，这种比较揭示了在实现良好分割性能的前提下内存使用情况与分割准确性的权衡。</li><li>SegNet的主要动机是场景理解的应用。因此它在设计的时候考虑了要在预测期间保证内存和计算时间上的效率。</li><li>定量的评估表明，SegNet在和其他架构的比较上，时间和内存的使用都比较高效。</li></ul><p><strong>模型细节</strong></p><p>Encoder: VGG16，移除全连接层(基本操作)</p><p>Decoder: Use pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. </p><p>特点：相比于最经典的FCN等，有更少的参数，相对更快的inference time。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/LyvwAGCUunghDzl.png" alt="image-20201101113757989"></p><p>实际上，SegNet与FCN最大的差别就在于upsampling的不同。作者在文章中还探讨了不同upsampling对性能的影响和每个方法可能的优劣。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/RI9yD4W1tjLhTk6.png" alt="image-20201101121932268"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/02/iVmDakgBtvPA9rG.png" alt="image-20201101121952246"></p><p>通过上表分析，可以得到如下分析结果：</p><ul><li>bilinear interpolation 表现最差，说明了在进行segmentation时，decoder是可学习的还是非常重要的。</li><li>SegNet-Basic与FCN-Basic对比，均具有较好的精度，不同点在于SegNet存储空间消耗小，FCN-Basic由于feature map进行了降维，所以时间更短。</li><li>SegNet-Basic与FCN-Basic-NoAddition对比，两者的decoder有很大相似之处，SegNet-Basic的精度更高，一方面是由于SegNet-Basic具有较大的decoder,同时说明了encoder过程中低层次feature map的重要性。</li><li>FCN-Basic-NoAddition与SegNet-Basic-SingleChannelDecoder：证明了当面临存储消耗，精度和inference时间的妥协的时候，我们可以选择SegNet，当内存和inference时间不受限的时候，模型越大，表现越好。</li></ul><p>作者总结到：</p><ul><li>encoder特征图全部存储时，性能最好。 这最明显地反映在语义轮廓描绘度量（BF）中。</li><li>当限制存储时，可以使用适当的decoder（例如SegNet类型）来存储和使用encoder特征图（维数降低，max-pooling indices）的压缩形式来提高性能。</li><li>更大更复杂的decoder提高了网络的性能。</li></ul><h2 id="Concrete-Cracks-Detection-Based-on-Deep-Learning-Image-Classification"><a href="#Concrete-Cracks-Detection-Based-on-Deep-Learning-Image-Classification" class="headerlink" title="Concrete Cracks Detection Based on Deep Learning Image Classification"></a>Concrete Cracks Detection Based on Deep Learning Image Classification</h2><p>Author: Wilson Ricardo Leal da Silva 1,OrcID andDiogo Schwerz de Lucena 2  <a href="https://www.mdpi.com/2504-3900/2/8/489">LINK</a></p><p>Journal: The 18th International Conference on Experimental Mechanics (ICEM 2018) 材料领域的会议</p><p>总结：模型真的非常简单。使用VGG16作为Backbone Net，在其上面做transfer Learning（pre-train model）。</p><p>整体感觉，没想到2018年的会议上也有这么水的文章，不过引用了一些可以扩展阅读的文献。</p><p>模型训练使用NVIDIA Tesla K80（约12G显存）</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/05/diWuIegAlKoED4L.png" alt="image-20201105220013434"></p><h3 id="Related-Paper"><a href="#Related-Paper" class="headerlink" title="Related Paper"></a>Related Paper</h3><p>使用无人机（UAV）做Crack detection</p><ol><li>Robust crack detection for unmanned aerial vehicles inspection in an acontrario decision framework 2015</li><li>Development of Crack Detection System with Unmanned Aerial Vehicles and Digital Image Processing 2015</li></ol><p>Deep Learning System for Automated Cracking</p><ol start="3"><li>Basis and Design of a Deep Learning System for Automated Cracking Survey 2017</li><li>Automatic Pavement Crack Detection Based on Structured Prediction with the Convolutional Neural Network. 2018</li></ol><h2 id="Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning"><a href="#Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning" class="headerlink" title="Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning"></a>Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning</h2><p>Author: Hyunjun Kim er al  <a href="https://www.researchgate.net/profile/Myoungsu_Shin2/publication/324707011_Crack_and_Noncrack_Classification_from_Concrete_Surface_Images_Using_Machine_Learning/links/5bec1eec92851c6b27bde0c2/Crack-and-Noncrack-Classification-from-Concrete-Surface-Images-Using-Machine-Learning.pdf">LINK</a></p><p>Journal: Structural Health Monitoring 2018</p><p>Key Point: <strong>Concrete crack identification/classification，feature preprocessing(use speeded-up robust features) VS CNN, Detect existence and location of cracks</strong></p><p>作者给出的contribution为：</p><ul><li>an efficient classification framework based on a <strong>crack candidate region (CCR)</strong> is proposed to effectively categorize cracks and noncracks</li><li>comparative analysis between SURF-based and CNN-based methods is conducted to evaluate the classification performances</li><li>a comprehensive crack identification in the presence of crack-like noncracks is conducted for practical applications</li></ul><p>作者给出的SURF（speeded-up robust feature）是一种计算机视觉领域中的<strong>特征点检测</strong>的传统算法（无学习能力）。可以获得distinctive features。其主要包含以下两个步骤：</p><ol><li>interest point detection</li><li>interest point description</li></ol><h3 id="SURF-based-classification"><a href="#SURF-based-classification" class="headerlink" title="SURF-based classification"></a>SURF-based classification</h3><p>这部分与CV中的Bag-of-Words(Visual categorization with bags of keypoints 2004)的流程很相似，共分为三个阶段：</p><ol><li><p>Feature Extraction: 使用SURF对每个4*4的子区域生成一个64维feature vector。</p></li><li><p>Visual vocabulary construction(K-meas Clustering): 所有interest point的feature vector都用于visual word，该词用作代表性的小图像段，用来描述诸如颜色，形状和表面纹理之类的特征。之后在使用K-means来进行聚类，聚类的结果被分为许多组，这些组就是visual vocabulary or the bag of features.</p></li><li><p>Classification(SVM): 训练时，先试用feature extraction+k-means生成词汇表，然后BoW将每一个图片在每一个组内的数目统计下来，生成一个feature histograms来输入给SVM训练。</p></li></ol><p>下图为SURF-based classification和CNN-based classification(使用Fast RCNN)的示意图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/09/8DeSxBLqHNQuTr3.png" alt="image-20201109222216350"></p><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><p>主要由两部分组成：</p><ol><li>generation of CCRs</li><li>SURF-based or CNN-based classifications</li></ol><p>具体的流程为：</p><p>先将图片binarization，这样可以找到crack和crack-like noncrack，并对得到的结果的每个部分赋予类别。然后，使用CNN or SURF来提取特征，之后在进行分类。</p><h4 id="CCR（crack-candidate-region）"><a href="#CCR（crack-candidate-region）" class="headerlink" title="CCR（crack candidate region）"></a>CCR（crack candidate region）</h4><p>下图为生成CCR的流程图。 其中image binarization: 将所有pixel的取值二值化，0为黑色，1为白色计算时基于一个阈值，使用的方法为Sauvola’s binarization（该方法在noisy和high-contrast的图像上性能出色）来计算该阈值。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/Ll2toTmuNYOd1fp.png" alt="image-20201110123707934"></p><p>作者给出这种方法的好处为：</p><ol><li>获得的CCR只关注cracks和crack-liked noncrack.</li><li>计算时更加高效。因为只有被选出的CCR才会进入train and test stages.</li></ol><h4 id="SURF-based-and-CNN-based-classification-models"><a href="#SURF-based-and-CNN-based-classification-models" class="headerlink" title="SURF-based and CNN-based classification models"></a>SURF-based and CNN-based classification models</h4><p>大体流程和对比如下图所示：</p><p>CNN框架使用AlexNet。生成feature后，进入各自不同的分类模型，分类模型的介绍在上一节有写。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/g6vuGe5lzKSOREW.png" alt="image-20201110125058886"></p><p>最终模型的流程图为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/dkMLwv1hEfZ5TD7.png" alt="image-20201110125734790"></p><h3 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h3><p>居然是使用MATLAB来implement的 ！训练i7-7700 GTX1080，最终的分类结果。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/kvsPg3I82JhSyRc.png" alt="image-20201110134830501"></p><p>作者在多种超参数组合下，计算了各种性能指标，包括：P、R，F1、Acc，计算时间等。</p><p>结论是CNN-based methods的性能更好的。不过在训练速度上难以对比，因为CNN based method使用的是GPU，而SURF使用的是CPU。训练速度对比图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/LS1G8NUK6knc3tY.png" alt="image-20201110160403602"></p><p>只不过在有些样例上，使用CNN的方法效果不如SURF，作者给出的原因是：the local features extracted using the SURF can in some instances correctly classify the CCRs  hat were incorrectly categorized using the CNN-based method.</p><p>作者猜测使用：使用DeepLearning提取的feature和local feature的结合会进一步提高魔性的性能。</p><p>这里作者给出了一个很有趣、也很合理的结论，作者使用CCR后，相当于在训练时的样本基本上只包括crack 和 crack-like noncrack，这样相比于使用crack和intact surface来训练，极大的提升了模型性能。正如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/10/FYqUpxRWsHr2X3z.png" alt="image-20201110161951740"></p><h2 id="Richer-Convolutional-Features-for-Edge-Detection-RCF"><a href="#Richer-Convolutional-Features-for-Edge-Detection-RCF" class="headerlink" title="Richer Convolutional Features for Edge Detection(RCF)"></a>Richer Convolutional Features for Edge Detection(RCF)</h2><p>Author: Yun Liu er al  </p><p>Journal: CVPR 2017</p><p>另一篇经典的edge detection的算法。算法的特点：性能优于HED，速度基本与HED持平。其中也提出的Fast RCF：**在BSDS500数据集上，ODS指标可以达到0.806 with(30 FPS)**。人类在这个benchmark上的水平为：0.803.</p><p>相比于HED的改进就在于以下三点：</p><ol><li>使用了每次卷积后的结果，而不是同HED只使用每个stage的最后一个输出</li><li>改变了loss，引入了对于类别不平衡的修正</li><li>引入Multiscale Hierarchical Edge Detection，会从一定程度上提升准确度，显然会导致运算速度的下降。</li></ol><p>本质：exploits multi-scale and multi-level information</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>同HED，也是使用VGG16network，将VGG16的5个conv阶段分开，分别产生一个side-output.（这部分与HED很相似）。</p><p>下图为RCF的网络结构：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/9Uh32sRWfJScHju.png" alt="image-20201122205002664"></p><p>整体就是一个VGG16，与HED的不同之处在于HED是在每一个conv stage的最后取最后的输出做multi-scale fuse，拼接后再过一个1*1的卷积得到最终的结果。</p><p>而RCF进一步的使用了网络中每次卷积的结果，在每个stage的conv的结果都被输出而聚合。这也正是paper的标题：Richer Convolutional Features的由来。</p><p>NOTE：每个stage的划分标准是根据max pooling layer来分割的。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>使用的每一个loss如下图：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/xVEfwpaOdFC9zUv.png" alt="image-20201122211255637"></p><p>可以看到本质就是一个二分类的交叉熵，训练数据本身的正例和反例的不均匀性（即edge在图中的pixel的数量是远远小于非edge的），所以α和β来起到一个类别平衡的作用。而超参数λ用来balance positive and negative samples.</p><p>最终的loss是各个loss的和，如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/VKw1AUyZ8CYexHR.png" alt="image-20201122211636629"></p><h3 id="Multiscale-Hierarchical-Edge-Detection"><a href="#Multiscale-Hierarchical-Edge-Detection" class="headerlink" title="Multiscale Hierarchical Edge Detection"></a>Multiscale Hierarchical Edge Detection</h3><p>在test time时，为了提高模型的准确性，使用了image pyramids，有点类似于使用集成学习的方法。具体来讲：we resize an image to construct an image pyramid, and each of these images is input to our single-scale detector separately. Then, all resulting edge probability maps are resized to original image size using bilinear interpolation. At last, these maps are averaged to get a final prediction map. </p><p>即如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/22/Um81vzqQBEafOW9.png" alt="image-20201122212050249"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>训练时也使用了同HED一样的数据增强。在NYUD数据集上，为了使用深度信息，作者使用HHA，来进一步做对比。</p><h2 id="CrackGAN"><a href="#CrackGAN" class="headerlink" title="CrackGAN"></a>CrackGAN</h2><p>Author: Kaige Zhang, Yingtao Zhang, and H. D. Cheng  <a href="https://arxiv.org/abs/1909.08216">LINK</a></p><p>Journal: CVPR 2020</p><p>Key Point: <strong>Pavement crack detection，generative adversarial learning, partially accurate ground truths(GTs)</strong></p><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul><li><p>Why label is <strong>partially accurate GTs</strong>?</p><p>在我们获取数据集的时候（这里尤其是pavement数据），往往是通过在一辆车上固定一个相机来拍摄，之后再由人工手工标注而获得的。由于是通过这种方式来获取label的，crack往往都非常的细，相对于background而言是很少的，而且他们的边界也相对模糊。这也导致了对于pixel-level的ground truth标记困难。所以在实践中，往往只将crack标记为1-pixel的曲线，这种GT也被称为labor-light GT（减小了人工标注的成本）。所以很明显，这种GT并不是完全准确的（pixel-level），所以被称为<strong>partially accurate ground truths(GTs)</strong></p></li><li><p>Then What Happened when using FCN-based methods？</p><p>显然，这种情况下，这是一个明显的类别不平衡问题，前人的解决方法往往是使用对于loss做一个修正。作者认为这种方法是无法解决这个问题的，因为<strong>partially accurate GTs</strong>的存在。所以就导致了the network will simply converge to the status that treats the entire crack image as BG (labeled with zero), and still can achieve a good detection accuracy or loss (BG-samples dominate the accuracy calculation). 这就是<strong>ALL BLACK</strong> problem。此问题也经常出现在其他的pixel-level pavement crack detection之中。</p></li></ul><p>So, CrackGAN is solving:</p><ol><li>solving ALL BLACK issue</li><li>proposes the crack-patch-only (CPO) supervised adversarial learning and the asymmetric U-Net architecture to perform<br>the end-to-end training</li><li>the network can be trained with partially accurate GTs generated by labor-light method which can reduce the workload of preparing GTs significantly</li><li>solve data imbalance problem which is the byproduct of the proposed approach</li></ol><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/qHBsKmReYXWkt2V.png" alt="image-20201127102112858"></p><p>D is a pretrained discriminator obtained directly from a pre-trained DC-GAN using crack-GT patches only. 这类的discriminator会令网络一直产生包含crack GT的image，作者认为这个是解决ALL BLACK问题的关键。</p><p>作者为了解决这个问题，在loss中加入了额外的generative adversarial loss.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/JYL5G6yNo7kd2zj.png" alt="image-20201127110512690" style="zoom: 67%;" /><p>实际上就是使用一个预训练好的discriminator（训练一个只使用crack  ground-truth的data，这样如果出现没有crack就认为其是fake，这样的discriminator作者称为one-class）来防止出现将整个图片都预测为ground truth的情况。</p><p>下图就是对于discriminator预训练的过程（训练一个DC-GAN），其中CPO是由人工产生的。文中多次提到的COP-supervision指的是：the training data are prepared with crack patches only, without involving any non-crack patches and “all black” patches.</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/tu3zsFVK2onCfvq.png" alt="image-20201127213218128" style="zoom:80%;" /><p>训练好这个discriminator后，这个discriminator和模型的核心 U-net一起训练。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/WvV6nFyAfQJENlr.png" alt="image-20201127215836597" style="zoom: 80%;" /><p>至于为什么只使用CPO-supervision就可以让整个模型也处理不包含crack的图像，作者在C. Asymmetric U-Net for BG-image translation给出了解释。目前这部分还不是很明白，涉及到了pix2pix GAN的Receptive field. 而我本人对于GAN的了解不是很多，所以部分之后会在了解了GAN之后继续更新的。</p><p>最后还有一部分是加入了一个pixel-level的loss. y是经过处理的partially accurate GT（将1-pixel width的crack变为3-pixel宽）。直观来讲：就是U-net生成结果的crack在扩展后的真实label内。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/27/tprUJ3eMCKsLqFW.png" alt="image-20201127223942711" style="zoom:80%;" /><p>作者还指出，这个asymmetric U-net可以处理任意尺寸的input image.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li>K. Zhang, H. D. Cheng, and B. Zhang, “Unified approach to pavement crack and sealed crack detection using pre-classification based on transfer learning,” J. Comput. Civil Eng., vol. 32, no. 2 (04018001), 2018.</li><li>K. Zhang, H. D. Cheng, and S. Gai, “Efficient Dense-Dilation Network for Pavement Crack Detection with Large Input Image Size,” in Proc. IEEE ITSC 2018, Maui.</li><li>F. C. Chen, and M. R. Jahanshahi, “NB-CNN: Deep learning-based crack detection using convolutional neural network and Nave Bayes data fusion,” IEEE Transactions on Industrial Electronics, vol. 65, no. 5, pp.4392-4400, 2018.</li><li>S. Park, S. Bang, and H. Kim, “Patch-Based Crack Detection in Black Box Images Using Convolutional Neural Networks,” Journal of Computing in Civil Engineering, vol. 33, no. 3 (040190170), 2019.</li><li>N. D. Hoang, Q. L. Nguyen, and V. D. Tran, “Automatic recognition of asphalt pavement cracks using metaheuristic optimized edge detection algorithms and convolution neural network,” Automation in Construction, vol. 94, pp. 203-213, 2018.</li><li>K. Gopalakrishnan, S. K. Khaitan, A. Choudhary, and A. Agrawal, “Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection,” Construction and Building Materials, vol. 157, pp. 322-330, 2017.</li><li>Q. Zou, Z. Zhang, Q. Li, X. Qi, Q. Wang, S. Wang, “DeepCrack: Learning hierarchical convolutional features for crack detection,” IEEE Transactions on Image Processing, vol. 28, no. 3, pp. 1498-512, 2019.</li><li>F. Yang, L. Zhang, S. Yu, D. Prokhorov, X. Mei, H. Ling, “Feature pyramid and hierarchical boosting network for pavement crack detection,” arXiv preprint arXiv:1901.06340. 2019.</li><li>M. Mirza, and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014.</li><li>P. Isola, J. Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proc. IEEE CVPR, 2017.</li><li>A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” in Proc. ICLR, 2016.</li></ol><h2 id="Learning-relaxed-deep-supervision-for-better-edge-detection"><a href="#Learning-relaxed-deep-supervision-for-better-edge-detection" class="headerlink" title="Learning relaxed deep supervision for better edge detection"></a>Learning relaxed deep supervision for better edge detection</h2><p>Author: Yu Liu er al  <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Learning_Relaxed_Deep_CVPR_2016_paper.pdf">LINK</a></p><p>Journal: CVPR 2016</p><p>文中很有特点的一点就是使用：relaxed supervision，即：使用额外的relaxed label，这些relaxed label是使用simple detector来获取的（比如：Canny算子）之后再与真实的ground truth 相结合。</p><p>本质来讲就是RDS(本篇文章提出的模型)使用relaxed labels 和额外的训练数据来 retrain HED，这样可以获得更好的性能的提升。</p><p><strong>问题引出：</strong></p><p>作者认为HED的模型对于所有的intermediate layer只使用了一个general supervision（有真实标记的）。作者认为这样让网络的diversities消失了，即hierarchical layer(CNN)不同层所获取特征不再具有多样性。所以作者就想引入：diversities supervision 来重新赋予网络的CNN layer重新获得coarse to fine的表示。这种diversities supervision就是作者本文提出的relaxed deep supervision(RDS). </p><p>RDS 加入了额外的label（原有的label是edge为positive，non-edge为negative）。额外加入的label使用simple detector来获取的（比如：Canny算子）。之后将这些relaxed label加入的原始的ground truth来训练。</p><p><strong>Contribution：</strong></p><ol><li>提出了RDS这种relaxed label来提升HED算法的性能</li><li>因为在许多edge detection任务中，数据标注的成本很高，导致训练数据很少，例如BSDS500只有200个训练数据。作者还证明了使用使用一些在大型数据集上预选练的模型，之后fine-tune，可以提升模型的性能。</li></ol><p><strong>Related Work：</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/oPQBKhdMUfVkzLy.png" alt="image-20201130120942864"></p><ul><li>pixel-level的work有: DeepEdge和其的一篇改进。extract deep features peer pixel and classify it to edge and non-edge class.</li><li>patch-level的work有：DeepContour。这类方法：estimate edge maps for the input patches and then integrate them for the whole edge map.</li><li>image-level主要就是：HED了。一类end-to-end的方法。</li></ul><p>作者主要工作就是将HED与DSN（deeply supervised net）相结合</p><p><strong>Model‘s Architecture：</strong></p><p>与HED极其的相似，不同点在于：</p><ul><li>HED：每个side output和fuse output都用真实label</li><li>RDS：side output用自己生成的relaxed label+positive label；最终的fuse output的label为真实label</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/YSa9TElGLCigH5Z.png" alt="image-20201130122707987"></p><p><strong>实际上就是作者手工让hierarchical layer强行学到不一样的东西。</strong> 结果显示这样可以提升性能。</p><p>作者在生成relaxed label时有两种选择，分别为<strong>Canny算子</strong>和<strong>SE Detector</strong>。</p><p>下图为这些方法在不同参数下提取到的特征的结果。通常来讲，SE detector可以获得更稀疏的relaxed label.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/ltocrCOV1w4xkgq.png" alt="image-20201130131508627"></p><p><strong>Loss：</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/FrHNJYcyz5XVBxp.png" alt="image-20201130133627195"></p><p>其中$L_{side}$如下图所示，这里Relaxed label为2，GT为1，non-edge为0。 α和β是用来平衡类别不平衡带来的影响</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/qEomyO8Rp6ftIK3.png" alt="image-20201130132143508"></p><p><strong>Pretrain with CEA:</strong></p><p>CEA（coarse edge annotation）指的是在大型的例如semantic segmentation数据集PSACAL VOC上，是有一些edge的label的，但其并不精细，如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/6oGWkhFStfT7IgR.png" alt="image-20201130133516278"></p><p> 使用这些大型的数据集来预训练RDS模型，之后在BSDS500使用RDS的方法进行fine tune，可以达到更好的效果。</p><p>整体流程如下图所示：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/Rgs1t9GDuCOvfEx.png" alt="image-20201130133653994"></p><h2 id="Multi-Scale-Context-Aggregation-by-Dilated-Convolutions"><a href="#Multi-Scale-Context-Aggregation-by-Dilated-Convolutions" class="headerlink" title="Multi-Scale Context Aggregation by Dilated Convolutions"></a>Multi-Scale Context Aggregation by Dilated Convolutions</h2><p>Author: Fisher Yu 2016  <a href="https://arxiv.org/abs/1511.07122">LINK</a>  第一篇提出空洞卷积的方法。空洞卷积是下面的那篇BDCN使用到了，这也是我第一次了解这个卷积操作。</p><p>Journal: ICLR 2016</p><p><strong>提出Dilated Conv的paper. 一种专门design for dense prediction（例如：semantic segmentation）的方法</strong></p><p>这种空洞卷积（Dilated Conv）<strong>support exponential expansion of the receptive field without loss of resolution or coverage!</strong></p><p>作者还指出，将这种Dilated Conv可以作为插入模块，来提升这种dense prediction的性能。</p><p><strong>总结：</strong></p><p>Dilated Conv的好处：</p><ul><li><strong>扩大感受野</strong>：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</li><li><strong>捕获多尺度上下文信息：</strong>空洞卷积有一个参数可以设置dilation rate，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度（multi-scale）信息。</li></ul><p>所以我们可以对于一个feature map使用几组不同dilation rate和padding的dilated Conv来让获取multi-scale信息，之后再concatenate并过一个conv聚合并学习这样的multi-scale的信息。</p><p><strong>而语义分割（semantic segmentation）由于需要获得较大的分辨率图，因此经常在网络的最后两个stage，取消降采样操作，之后采用空洞卷积弥补丢失的感受野。</strong></p><p><strong>背景：</strong></p><p>在图像分割领域，图像输入到CNN（典型的网络比如FCN）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像（feature）尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。</p><p><strong>Detail About Dilated Conv：</strong></p><p>对于公式化卷积的过程可以如下所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/x4Upe38VFKAMzLm.png" alt="image-20201202204630447"></p><p>其使用的卷积核与普通CNN一直，只不过对于每一个Dilated Conv存在一个factor l.</p><p>下图就是Dilated Conv的一个直观实例，也显示出了其特点：<strong>The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly.</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/V1mB5KvAc8dSpMr.png" alt="image-20201202210406002"></p><p>dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv。</p><p>下图是传统的卷积：或者说是1-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/03/kUItGT1vOV3oZrd.png" alt="image-20201202211039151"></p><p>而下图是2-dilated Conv</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/02/ojTmcJvQiIMV8ep.png" alt="image-20201202211056595"></p><p><strong>值得注意的是这里的这里对于filter的初始化不能是随机的。</strong></p><h2 id="Bi-Directional-Cascade-Network-for-Perceptual-Edge-Detection"><a href="#Bi-Directional-Cascade-Network-for-Perceptual-Edge-Detection" class="headerlink" title="Bi-Directional Cascade Network for Perceptual Edge Detection"></a>Bi-Directional Cascade Network for Perceptual Edge Detection</h2><p>Author: Jianzhong He er al  <a href="https://arxiv.org/pdf/1902.10903.pdf">LINK</a>  一篇比较新的2019年的edge detection的方法。</p><p>Journal: CVPR 2019</p><p><strong>此方法基本上是目前edge detection的state-of-art的方法！</strong></p><p>作者给出的改进在于：</p><ol><li>Bi-Directional Cascade Network：增强不同scale可以学到的不同的东西。即让模型focus on meaningful details and deep layers should depict object-level boundaries. 即CNN的每一层应该有其单独的supervision，而不是像之前主流的HED和RCF使用统一的supervision。这种每层不同的supervision的方法可以让网络的不同层关注不同的信息，例如浅层关注于一些detail，而深层关注于表述整个物体的edge. <strong>实现这个的本质是利用相邻卷积层的预测和真实的supervision共同作用，作为每层的supervision。可以看为是一种对于loss的修改，这种修改赋予了层与层之间交流信息的能力。</strong></li><li>加入额外的Scale Enhancement Module（SEM）：目的是增强每个scale的学习能力。<strong>SEM本质就是一组有不同dilated rate的dilated Conv.</strong></li></ol><p>关于<strong>空洞卷积</strong>的一些细节和思考可以见我的另一篇note.</p><p>整体来讲，空洞卷积可以<strong>effectively increases the size of receptive fields of network neurons.</strong> 对于这种pixel level的任务，很大的receptive field是非常重要的。</p><p>整体的模型大致如下图所示。</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/MfBAIUkXy4trVsD.png" alt="image-20201201221204410" style="zoom:67%;" /><p>其中的ID Block是Incremental Detection Block，使用VGG网络的每个卷积部分加上SEMs组成的。This bi-directional cascade structure enforces each layer to focus on a specific scale, allowing for a more rational training procedure.</p><p>在我看来，模型相比于HED，RCF这种经典的multi-scale方法的改进之处在于：</p><ol><li><strong>改进了loss function，引入了不同scale layer的communicate。使用的是每层之间的supervise是相互关联的</strong></li><li><strong>增加每个scale的表示能力（每个scale额外使用 dilated convolution）</strong></li></ol><p>（i）首先介绍作者是如何增强不同scale可以学到的不同的东西而并不依赖额外的监督信息（增加额外的监督信息见：Relaxed Deep Supervision这篇paper，此blog中也有涉及），<strong>实现这个的本质是利用相邻卷积层的预测和真实的supervision共同作用，作为每层的supervision。可以看为是一种对于loss的修改，这种修改赋予了层与层之间交流信息的能力。</strong></p><p>作者将真实的label拆解为:</p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201206175302650.png" alt="image-20201206175302650"></p><p>其中Y是真实的label，$Y_s$指的是在scale s上的label. 目标是学到一个detector D, that capable of detecting edges at different scales，D本质上就是一个神经网络N，其有S个conv层，The pooling layers make adjacent convolutional layers depict image patterns at different scales.</p><p>优化目标可以写为：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/LV9IciSv4Fr6Oxl.png" alt="image-20201206175735676"></p><p>其中$P_s=D_s(N_s(x))$为在scale s的edge prediction。最终的D可以看为$D_s$的ensemble.</p><p>显然，下面我们需要定义出每一个$Y_s$，如果每一个$Y_s$都合理，那么显然我们可以让模型的不同卷积学到不同的scale，也是这种改进的核心所在。</p><p>然而直接手工的分解$Y$显然是非常困难的。作者使用的方法就是根据真实label Y和其他层的prediction来估计$Y_s$.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/ruaAKY7CVZPwiM3.png" alt="image-20201206180258502"></p><p>但这种估计并不是一个很好地layer-specific supervision. （原因见原文，本质就是这种方法对于每个scale产生的loss的梯度是一致的，dose not necessarily differentiate the scales depicted by different layers）。</p><p>所以作者用了两个complementary supervisions来估计$Y_s$，一个是来自smaller scale，另一个是来自larger scale. 这两个不同的supervision会训练两个detector at each scale.</p><p>即：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/8nxTkUvqONGLr16.png" alt="image-20201206180932590"></p><p>最终就是利用这两个$P_s^{s2d}+P_s^{d2s}$ to interpolate edge detection at scale s.</p><p>(ii) 接着介绍第二个改进，即加入额外的Scale Enhancement Module（SEM）：目的是增强每个scale的学习能力。<strong>SEM本质就是一组有不同dilated rate的dilated Conv.</strong></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/95IByYifhxtD4C1.png" alt="image-20201201233106975"></p><p>最终的输出与HED和RCF一致，都是使用每个中间层结果的fuse再过一个1*1的conv.</p><p>每个SEM的结构如图所示，for each SEM we apply K dilated convolutions with different dilation rates.</p><p>(iii)最终的loss:</p><p>可以看到每个ID Blog由两个layer-specific side supervision来训练，最终也用了fuse 了所有中间层产生的final predict来与真实label产生loss.</p><p>最终的loss如下图所示.</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/IYOutz2Fn3oad6y.png" alt="image-20201206182031788"></p><p>L()是一个balanced Cross entropy. Because of the inconsistency of annotations among different annotators, we also introduce a threshold γ for loss computation.即：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/A7QGEPVJeFvTlDf.png" alt="image-20201206182156780"></p><p>这是其产生的一个结果。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/12/06/MCpoAWxHuLnBcmI.png" alt="image-20201206182243105"></p><p>Reference: 本部分对应的paper在pdf中用波浪线给出</p><ol><li>L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915, 2016.</li><li>Learning deep structured multi-scale features using attention-gated crfs for contour prediction. In NIPS, pages 3964–3973, 2017.  About Edge detection</li><li>Attention to scale: Scale-aware semantic image segmentation. In CVPR, pages 3640–3649, 2016.</li><li>dilated convolution</li><li>Network Cascade</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Crack-Detection-Paper-Reading&quot;&gt;&lt;a href=&quot;#Crack-Detection-Paper-Reading&quot; class=&quot;headerlink&quot; title=&quot;Crack Detection Paper Reading&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
      <category term="Papers" scheme="http://canVa4.github.io/categories/Works/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://canVa4.github.io/tags/Deep-Learning/"/>
    
      <category term="CV" scheme="http://canVa4.github.io/tags/CV/"/>
    
      <category term="Crack Detection" scheme="http://canVa4.github.io/tags/Crack-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Running Neural Networks On Embedding Systems</title>
    <link href="http://canva4.github.io/2020/10/04/Running-Nuerual-Networks-On-Embedding-Systems/"/>
    <id>http://canva4.github.io/2020/10/04/Running-Nuerual-Networks-On-Embedding-Systems/</id>
    <published>2020-10-04T09:11:39.000Z</published>
    <updated>2020-11-02T10:25:24.017Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Running-Neural-Networks-On-Embedding-Systems"><a href="#Running-Neural-Networks-On-Embedding-Systems" class="headerlink" title="Running Neural Networks On Embedding Systems"></a>Running Neural Networks On Embedding Systems</h1><p>我检索资料时，核心的关键词是：<strong>云平台+边缘计算、IoT、AI加速</strong>。所以主要看的是有GPU或者有ASIC（专用集成电路）的方案。</p><p>经过资料的查找之后，我发现许多做硬件的大公司比如：高通、NVIDIA，Google、海思都有自己的解决方案。</p><p>以下为我找到的一些资料。资料来源：各个公司的官网；淘宝，论坛、博客。</p><p>同时给出一些我个人的简易分析，这些分析可能不是很准确，因为只是根据网上资料所得的结果。</p><p>TODO:</p><ul><li>各个芯片的加速程度对比</li></ul><h2 id="Qualcomm-高通"><a href="#Qualcomm-高通" class="headerlink" title="Qualcomm 高通"></a>Qualcomm 高通</h2><p>高通的优势之一就是其提供了一整套完整的硬件和软件的解决方案。<a href="https://www.qualcomm.com/products/artificial-intelligence">高通AI LINK</a> </p><p>其中Qualcomm NPE(Neural Processing Engine) 使用Caffe or tensorflow写好模型后，用Qualcomm NPE SDK添加到晓龙CPU/GPU上运行模型。整个开发使用流程如下所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/jar7ecdq9KwPXGT.png" alt="image-20201004205847255"></p><p>其主要应用领域就用手机AI、IoT领域等。 </p><p><strong>优势：</strong></p><ul><li>有完整的软件+硬件方案</li><li>模型建立后部署方便</li><li>支持AI的晓龙芯片性能强大，因为是SOC，一个芯片实现了非常多的功能</li></ul><p><strong>不足：</strong></p><ul><li>Data Center支持有限</li><li>目前国际形势可能导致不方便购买其解决方案（还未详细了解）<h2 id="Cambricon-寒武纪"><a href="#Cambricon-寒武纪" class="headerlink" title="Cambricon 寒武纪"></a>Cambricon 寒武纪</h2></li></ul><p>寒武纪是国内的一家目前在智能芯片市场上很出名的企业。是一家国内提供智能芯片，满足：有终端硬件+云平台的需求。<a href="http://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=71">寒武纪 LINK</a> </p><p>寒武纪同样也有几款主打的AI加速芯片。同时他也有一个云平台——寒武纪人工智能开发平台（Cambricon NeuWare）是寒武纪专门针对其云、边、端的智能处理器产品打造的软件开发平台， Neuware采用端云一体的架构，可同时支持寒武纪云、边、端的全系列产品。寒武纪终端IP、边缘端芯片及云端芯片共享同样的软件接口和完备生态，可以方便地进行智能应用的开发，迁移和调优。</p><p>开发者可以借助云端丰富的计算资源进行算法模型的解析与调试，利用Neuware生成离线模型，并能够在任意搭载寒武纪智能终端IP的设备运行，解决了终端调试手段受硬件资源限制的问题。</p><p>下图为其端云一体业务部署流程。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/Xxi7QG9Cale16OP.png" alt="image-20201004211854078"></p><p>下图为其软件栈。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/04/YnSeXP6Cs2UGE4f.png" alt="image-20201004211515460"></p><p><strong>优势：</strong></p><ul><li>国内的方案，购买、实施、使用相对方便</li><li>有专门的云平台</li></ul><p><strong>不足：</strong></p><ul><li>真实性能和效果未知</li><li>其云端的重点好像是主要用于调试和部署</li></ul><h2 id="NVIDIA-Jetson-系列"><a href="#NVIDIA-Jetson-系列" class="headerlink" title="NVIDIA Jetson 系列"></a>NVIDIA Jetson 系列</h2><p>NVIDIA Jetson 系列。GPU大厂的解决方法，据我所知，在机器人领域使用很多。其Jetson系列都有GPU，且可运行Linux操作系统。比如：我原来所在北邮机器人队，需要摄像头进行一些较大运算量算法时，就使用的是Jetson系列的TX2。</p><p>其主打：适用于新一代自主机器的嵌入式系统；NVIDIA Jetson：适用于一切自主机器的 AI 平台。Jetson系统所提供的性能和能效可提高自主机器软件的运行速度，而且功耗更低。每个系统都是一个完备的模块化系统 (SOM)，具备 CPU、GPU、PMIC、DRAM 和闪存，可节省开发时间。自然也具有可扩展性，比如：支持USB，串口，HDMI等接口（不同的型号不太一样）。</p><p>其主要有四个如下的产品，从左至右性能依次提升。（当然价格和功耗也会提升）</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/JX2FyS5eZEABtPT.png" alt="image-20201005210658302"></p><p>例如：广泛用于智能小车、比较需要计算量的智能产品中的Jetson Nano。</p><p>Jetson Nano 模块是一款低成本的 AI 计算机，具备超高的性能和能效，可以运行现代 AI 工作负载，并行运行多个神经网络，以及同时处理来自多个高清传感器的数据。这使其成为向嵌入式产品中添加高级 AI 的理想的入门级选项。</p><p>下图为Jetson Nano的照片，可以看到其支持多种外设接口。<a href="https://detail.tmall.com/item.htm?id=608609593274&ali_refid=a3_430582_1006:1268380158:N:sH2jsRQiqncfQma5KNTJH3QCsGA2TDnC:33fec7c4be41b2615b4987bf0b673408&ali_trackid=1_33fec7c4be41b2615b4987bf0b673408&spm=a230r.1.14.13&skuId=4314923092276">淘宝链接</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/slfVR6Y3WiXoSAZ.png" alt="image-20201005210954346"></p><p>而且其另一大优势时都是NVIDIA一家的，其GPU支持CUDA，这样例如：pytorch等可以很方便的部署。</p><p>Jetson 平台由 Jetpack SDK 提供支持，包括板级支持包 (BSP)、Linux 操作系统、NVIDIA CUDA(R)，并且兼容第三方平台。开发者可以利用 DeepStream SDK 在 Jetson 上快速构建和部署高效的视频分析管线。</p><p>下面是其软件栈：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/RCc2L8bsJ4uS3YK.png" alt="image-20201005211204653"></p><p><strong>优势：</strong></p><ul><li>产品种类多，可选择空间大</li><li>模型部署方便</li><li>购买比较方便（淘宝即可）</li><li>教程和应用实例比较多</li></ul><p><strong>不足：</strong></p><ul><li>加速性能由于不同的GPU架构可能差距很大</li></ul><p>TODO: Data Center 调研</p><h2 id="Google-TPU"><a href="#Google-TPU" class="headerlink" title="Google TPU"></a>Google TPU</h2><h3 id="Cloud-TPU"><a href="#Cloud-TPU" class="headerlink" title="Cloud TPU"></a>Cloud TPU</h3><p>张量处理单元 (TPU) 是专门设计用于处理机器学习应用计算需求的 ASIC 设备。有着很完善的文档，教程等。<a href="https://cloud.google.com/tpu/docs/quickstart?hl=zh-cn">链接</a></p><h3 id="Edge-TPU"><a href="#Edge-TPU" class="headerlink" title="Edge TPU"></a>Edge TPU</h3><p>作为Cloud TPU的补充，目前Edge TPU主要作用于推理，专为在边缘运行TensorFlow Lite ML模型而设计。</p><p>AIY Edge TPU 加速器是一个适用于现有系统的神经网络协处理器，一个加速棒。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/pm8INj3TZcfRiME.png" alt="image-20201005214027249"></p><p>AIY Edge TPU Dev开发板 是一个带搭载Edge TPU的单板计算机。类似于NVIDIA Jetson Nano。下图左边为树莓派，中间为Google Edge TPU Dev，右边为NVIDIA Jetson Nano</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/05/c5OB3J7nwXb6WFP.png" alt="image-20201005214053059"></p><p>其Edge TPU是对于TensorFlow Lite有专门优化的，在计算速度上强于Jetson Nano。</p><p><strong>优势：</strong></p><ul><li>云端非常强大</li><li>文档支持良好</li><li>对于TensorFlow有专门优化</li></ul><p><strong>不足：</strong></p><ul><li>国内可能不支持</li></ul><h2 id="瑞芯微电子"><a href="#瑞芯微电子" class="headerlink" title="瑞芯微电子"></a>瑞芯微电子</h2><p>国内的一家提供相关解决方案的厂家。主要为为高端智能硬件、手机周边、平板电脑、电视机顶盒、工控等多个领域提供专业芯片解决方案。</p><p>目前许多门禁系统的人脸识别部分，有许多使用的是瑞芯的RK3288或者RK3399（性能更强）系列。均使用arm内核。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/10/20/eqaQcPkpIGWxgLX.png" alt="image-20201020114444744"></p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>AI加速计算，如：Intel Movidius神经计算棒（可以结合树莓派使用，即支持Linux操作系统）</p><p>华为旗下的高端芯片企业。主打AI处理器。也提供了许多领域的解决方案：如：IoT，Face Cam，门禁系统等的解决方法。主要偏向于高端市场。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Running-Neural-Networks-On-Embedding-Systems&quot;&gt;&lt;a href=&quot;#Running-Neural-Networks-On-Embedding-Systems&quot; class=&quot;headerlink&quot; title=&quot;Runn
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="嵌入式系统" scheme="http://canVa4.github.io/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="TPU" scheme="http://canVa4.github.io/tags/TPU/"/>
    
      <category term="智能终端" scheme="http://canVa4.github.io/tags/%E6%99%BA%E8%83%BD%E7%BB%88%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>单片机解决方案调研（2）</title>
    <link href="http://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/"/>
    <id>http://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/</id>
    <published>2020-09-14T12:36:48.000Z</published>
    <updated>2020-11-26T11:41:35.503Z</updated>
    
    <content type="html"><![CDATA[<h1 id="解决方案调研（2）"><a href="#解决方案调研（2）" class="headerlink" title="解决方案调研（2）"></a>解决方案调研（2）</h1><p>GOAL：针对上一次老师提出的点进行补充调研。<a href="https://ieeexplore.ieee.org/document/8812785">https://ieeexplore.ieee.org/document/8812785</a></p><p>KEY WORDS: <strong>具体通信细节、耗电量估算、MSP系列、Google edge TPU（另写一篇）</strong></p><p>在认真读完SnowFort这篇文章后，我更进一步了解了我们设计的预期，想相比于SnowFort有更进一步的提升，可能可以在如下方面做一些升级：</p><ul><li>提升Mote的运算性能，以便可以在mote上进行更多的数据处理</li><li>提升Mote的可扩展性，可以让每个mote接入更多种类的传感器</li><li>令Mote有更长or维持当前的功率（使用时间）</li><li>提升Base Station的运算性能，可以不用考虑其功率情况</li><li>组网能力提升、通信距离提升</li><li>云端功能、数据处理算法提升</li></ul><p>综上，除了对于老师上次提出的点进行补充，我也尝试着从这些方面来尝试改进。</p><h2 id="具体通信细节"><a href="#具体通信细节" class="headerlink" title="具体通信细节"></a>具体通信细节</h2><p>在上一篇调研的最后有一个简易的实现方案。<a href="https://canva4.github.io/2020/09/14/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%EF%BC%882%EF%BC%89/">LINK</a></p><h3 id="STM32-PART"><a href="#STM32-PART" class="headerlink" title="STM32 PART"></a>STM32 PART</h3><p>无限通信部分STM32系列方案使用的是 <a href="https://detail.tmall.com/item.htm?id=609757779633&ali_refid=a3_430582_1006:1267360122:N:9/mfWI1BJMLzXLT4ATlUnA==:de4e276b258975c722c4a03cf64e8c17&ali_trackid=1_de4e276b258975c722c4a03cf64e8c17&spm=a230r.1.14.8">ATK-ESP8266</a> 。</p><p>该模块的核心性能指标如图所示：</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/14/VsDwkx5XSnAEjLQ.png" alt="image-20200914204633006" style="zoom: 67%;" /><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/14/RWDBFwVb9e6svLY.png" alt="image-20200914204839819" style="zoom:67%;" /><h3 id="M5STACK-PART"><a href="#M5STACK-PART" class="headerlink" title="M5STACK PART"></a>M5STACK PART</h3><p>其核心是esp32系列芯片。esp32是esp8266的升级版，其WIFI支持：802.11 b/n/g，与ATK-ESP8266相似。其相比于ATK-ESP8266还支持蓝牙。</p><ul><li>传统蓝牙支持 L2CAP，SDP，GAP，SMP，AVDTP，AVCTP，A2DP (SNK) 和 AVRCP (CT) 协议</li><li>低功耗蓝牙 (Bluetooth LE) 支持 L2CAP，GAP，GATT，SMP，和 GATT 之上的 BluFi，SPP-like 协议等</li></ul><h2 id="STM32L-低功耗系列"><a href="#STM32L-低功耗系列" class="headerlink" title="STM32L 低功耗系列"></a>STM32L 低功耗系列</h2><p><strong>产品栈</strong></p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201014114115216.png" alt="image-20201014114115216"></p><p><strong>超低功耗模式中的不同产品系列</strong></p><p>有M0+内核的STM32L0，有Cortex-M3内核的L1以及Cortex-M4内核的L4和L4+，其中L0和L1都有5种低功耗模式，这5种低功耗模式分别是低功耗的运行、睡眠、低功耗睡眠、停止和待机。对于L4和L4+，它们在5种低功耗模式基础上又添加了停止模式下的stop 1、stop 2和shutdown关断模式。可以通过内部的寄存器配置，来切换工作模式，在不同的模式下会有不同的唤醒时间。尤其是L4产品中shutdown模式，做到了非常低的功耗。</p><p><strong>低功耗运行模式</strong></p><p>其实低功耗运行模式还是一种运行模式，只是它的电流消耗很低，它与运行模式最大的区别是给内核供电的内部电压调节器电压要低于正常的运行模式下的电压值，即使用的低功耗的电压器来供电。在此情况下，系统最大的运行频率也会明显降低，例如L4在低功耗运行模式时最大的频率不超过2MHz。</p><p><strong>睡眠模式</strong></p><p>在睡眠模式，系统的CPU也就是Cortex-M内核的时钟被关闭了，但外设是可以继续保持运转的，它整个I/O的引脚状态与运行模式下也是相同的。</p><p><strong>低功耗睡眠模式</strong></p><p>低功耗睡眠模式是基于睡眠模式下的低功耗模式，是具有极低电流消耗的睡眠模式，它内核的时钟也是被关闭的，同时外设时钟频率受到了限制，因为它的电压调节器属于低功耗状态，内部的FLASH是要被停止的，所以低功耗睡眠模式只能从低功耗运行模式进入，这个是和其他模式不同的，其他模式都可以从运行模式直接做切换。</p><p>在低功耗运行和睡眠模式下，可以有一个BAM模式，它的工作方式是通过RTC加一个外设加DMA加SRAM，在不需要CPU干预的情况下就可以自行做数据采集，一旦到了数据采集需要到CPU处理的条件时，然后再把CPU唤醒做处理，所以这整个一个小系统就实现了一个协处理器的功能。</p><p><strong>停止模式</strong></p><p>首先说一下其的供电系统，其中有一个Vcore，它是内核的一个供电区域，负责给CPU内核供电，并且还给系统内部的存储器和它的数字外设供电。</p><p>停止模式中，除了CPU，也就是Cortex-M内核的时钟被关闭外，内核供电域的时钟也被停止，在停止模式下，内核供电域的时钟全部都停掉，PLL内部、外部的高速时钟全部都停掉，电压调节器为内核供电域供电，保留寄存器和内部SRAM中的内容。</p><p>在L4和L4+系列中，停止模式被细分为stop 0、stop 1和stop 2三种模式，按照功耗从低到高来说，stop 2是功耗最低的一个stop模式，它整个Vcore电源域放在了更低的漏电流模式下，使用了低功耗的电压调节器，只有最少的外设可以工作，所以它的功耗相对来说是最低的，但是唤醒时间是最长的。</p><p>Stop 1模式提供了更多的外设和唤醒源，唤醒时间也会更长一些；</p><p>Stop 0模式主电压调节器打开，可以得到最快的唤醒时间；</p><p>在所有的stop模式下，所有的高速振荡器停止，而低速振荡器保持活动，外设设置为active，需要的时候就可以使用这些高速时钟，能保证它在一些特定的事件下去唤醒设备。</p><p><strong>待机模式</strong></p><p>在待机模式下，内核的供电是直接断电的，电压调节器掉电区寄存器的内容会完全丢失，包括内部的SRAM，所以最大的区别即，系统从待机模式下的低功耗唤醒的时候，系统是要复位的。</p><p>默认条件的待机模式下，SRAM的内容是会丢失的，但是在L4里增加了SRAM 2，如果需要在待机模式后系统唤醒的时候有SRAM能保存一些内容，那就可以使用SRAM 2，它需要有多余220nA的额外电流消耗。</p><p><strong>Shutdown模式</strong></p><p>在shutdown模式，系统达到了最低的功耗，电压调节器的供电就被关断了，内核的供电也完全被断开，只有备份域的LSE、RTC可以工作所以在L4器件实现了一个新的模式，这个模式主要实现的目的就是为了延长电池供电之后整个器件的使用寿命，它其实是通过关闭内部的稳压器以及禁止使用耗电的监控，所以这个模式可以达到最低的功耗电流。</p><h2 id="MSP系列芯片"><a href="#MSP系列芯片" class="headerlink" title="MSP系列芯片"></a>MSP系列芯片</h2><p>MSP430的电压已经降到了3.3v，且MSP430比芯片分成了许多不同的模块部分，不用的部分功能模块可以关闭，电流近似为零，这样就极大的节省了能耗；另一个值得注意的是，其可以有三个时钟源，并产生更多的内部可用工作频率，让内部各个模块工作在不同的频率，不用的时钟也可以关掉。<strong>具体寄存器和模式切换TODO</strong></p><h2 id="MSP430-VS-STML4"><a href="#MSP430-VS-STML4" class="headerlink" title="MSP430 VS STML4"></a>MSP430 VS STML4</h2><p><strong>ULP Benchmark</strong></p><p>在超低功耗MCU领域，有一些评测不同芯片功耗水平的Benchmark。</p><p>其中比较有名和有代表性的是：ULP Benchmark <a href="https://www.eembc.org/ulpmark/scores.php">https://www.eembc.org/ulpmark/scores.php</a></p><p>NOTE：不同的编译方法，在真实功耗上会有很大的差异性。例如使用IAR和ARM GCC编译器在同一个芯片上的表现可能还不同。</p><p>从表中可以看到，STM32L4的大部分产品的在该Benchmark上的得分都是高于MSP430系列的。</p><p>其中，我选择了STM32L433（L4中较常见的一款，淘宝可买到）和MSP430FR5969（为MSP430Core）对比，可以看到STM32L4的功耗在该Benchmark下更低。<a href="https://www.eembc.org/viewer/?benchmark_seq=2760,2679">DETAIL</a></p><p><img src= "/img/loading.gif" data-lazy-src="C:\Users\12552\AppData\Roaming\Typora\typora-user-images\image-20201014121221114.png" alt="image-20201014121221114"></p><h2 id="耗电量估算"><a href="#耗电量估算" class="headerlink" title="耗电量估算"></a>耗电量估算</h2><p>ESP8266系列功耗</p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/27/gjav2t9mG5BpFUT.png" alt="image-20200927172219640" style="zoom:67%;" /><p>TODO：</p><p>LINKS：</p><p>ULP Benchmark <a href="https://www.eembc.org/ulpmark/scores.php">https://www.eembc.org/ulpmark/scores.php</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;解决方案调研（2）&quot;&gt;&lt;a href=&quot;#解决方案调研（2）&quot; class=&quot;headerlink&quot; title=&quot;解决方案调研（2）&quot;&gt;&lt;/a&gt;解决方案调研（2）&lt;/h1&gt;&lt;p&gt;GOAL：针对上一次老师提出的点进行补充调研。&lt;a href=&quot;https://ie
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="TPU" scheme="http://canVa4.github.io/tags/TPU/"/>
    
      <category term="单片机" scheme="http://canVa4.github.io/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="STM32" scheme="http://canVa4.github.io/tags/STM32/"/>
    
      <category term="msp" scheme="http://canVa4.github.io/tags/msp/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment3 遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/27/CS231n-Assignment3-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-27T09:04:38.000Z</published>
    <updated>2020-09-13T09:23:11.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment3-遇到的问题"><a href="#CS231n-Assignment3-遇到的问题" class="headerlink" title="CS231n Assignment3 遇到的问题"></a>CS231n Assignment3 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Image-Captioning-with-RNNs"><a href="#Image-Captioning-with-RNNs" class="headerlink" title="Image Captioning with RNNs"></a>Image Captioning with RNNs</h2><p>本部分主要是实现RNN的基础版本。即如下的RNN，不过需要注意的是在代码中实现时要注意矩阵相乘的顺序。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/FQu3U9OsD5gtvYi.png" alt="image-20200903233248910"></p><p>首先是每次time step时的forward的backward，这里比较简单，按照上图公式implement一下就ok了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span>(<span class="params">x, prev_h, Wx, Wh, b</span>):</span></span><br><span class="line">    next_h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    next_h = np.tanh(np.dot(prev_h, Wh) + np.dot(x, Wx) + b)</span><br><span class="line">    cache = Wx, Wh, next_h, prev_h, x, b</span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span>(<span class="params">dnext_h, cache</span>):</span></span><br><span class="line">    dx, dprev_h, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    Wx, Wh, next_h, prev_h, x, b = cache</span><br><span class="line">    dmid = (<span class="number">1</span> - np.square(next_h)) * dnext_h</span><br><span class="line">    dprev_h = np.dot(dmid, Wh.T)</span><br><span class="line">    dx = np.dot(dmid, Wx.T)</span><br><span class="line">    dWh = np.dot(prev_h.T, dmid)</span><br><span class="line">    dWx = np.dot(x.T, dmid)</span><br><span class="line">    db = np.sum(dmid, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>然后是在一定time sequence上的forward和backward，forward就是多层step forward的叠加，backward计算梯度就是将每次对x的梯度持续回传，将对W权值矩阵的梯度叠加即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span>(<span class="params">x, h0, Wx, Wh, b</span>):</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    cache = []</span><br><span class="line">    h = np.zeros((N, T, h0.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, c = rnn_step_forward(x[:, i, :], h0, Wx, Wh, b)</span><br><span class="line">        h[:, i] += h0</span><br><span class="line">        cache.append(c)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span>(<span class="params">dh, cache</span>):</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    D = cache[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, H))</span><br><span class="line">    dWh = np.zeros((H, H))</span><br><span class="line">    db = np.zeros((H,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        dx[:, i], dh0, dWx_mid, dWh_mid, db_mid = rnn_step_backward(dh[:, i] + dh0, cache.pop())</span><br><span class="line">        dWx += dWx_mid</span><br><span class="line">        dWh += dWh_mid</span><br><span class="line">        db += db_mid</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>实现这些基本核心组件后，还需要实现的就是根据word 生成 embedding，这里使用的是类似于查询的方法，有一个对应的生成embedding的矩阵，这个也是可以学习的。forward很简单，就是类似的查询，backward的实现我遇到了实现上的问题，最后借鉴了一下别人的code。:)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_forward</span>(<span class="params">x, W</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    out = W[x]</span><br><span class="line">    cache = x, W</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dW = <span class="literal">None</span></span><br><span class="line">    x, W = cache</span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line">    N, T, D = dout.shape</span><br><span class="line">    np.add.at(dW, x.flatten(), dout.reshape(<span class="number">-1</span>, D))     <span class="comment"># 这里借鉴了一下别人的代码</span></span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure><p>最后要实现的就是class RNN了。这个部分只要认真看代码中的提升，注意下细节根据流程和之前实现好的模块实现即可了。</p><p><strong>forward 函数</strong>，位于<code>rnn.py</code> rnn类内。这里的处理是将caption分为两部分：captions_in除了最后一个单词外，所有内容都将被输入到RNN； 而captions_out只不包含第一个单词。这就是期望RNN生成的东西。 它们彼此相对偏移一个，因为RNN在接收到单词t之后会产生单词（t + 1）。 captions_in的第一个元素将是START caption，我们的期望是captions_out的第一个元素将是第一个单词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, features, captions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute training-time loss for the RNN. We input image features and</span></span><br><span class="line"><span class="string">    ground-truth captions for those images, and use an RNN (or LSTM) to compute</span></span><br><span class="line"><span class="string">    loss and gradients on all parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Input image features, of shape (N, D)</span></span><br><span class="line"><span class="string">    - captions: Ground-truth captions; an integer array of shape (N, T) where</span></span><br><span class="line"><span class="string">      each element is in the range 0 &lt;= y[i, t] &lt; V</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Scalar loss</span></span><br><span class="line"><span class="string">    - grads: Dictionary of gradients parallel to self.params</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Cut captions into two pieces: captions_in has everything but the last word</span></span><br><span class="line">    <span class="comment"># and will be input to the RNN; captions_out has everything but the first</span></span><br><span class="line">    <span class="comment"># word and this is what we will expect the RNN to generate. These are offset</span></span><br><span class="line">    <span class="comment"># by one relative to each other because the RNN should produce word (t+1)</span></span><br><span class="line">    <span class="comment"># after receiving word t. The first element of captions_in will be the START</span></span><br><span class="line">    <span class="comment"># token, and the first element of captions_out will be the first word.</span></span><br><span class="line">    captions_in = captions[:, :<span class="number">-1</span>]</span><br><span class="line">    captions_out = captions[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You&#x27;ll need this</span></span><br><span class="line">    mask = (captions_out != self._null)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the affine transform from image features to initial</span></span><br><span class="line">    <span class="comment"># hidden state</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Word embedding matrix</span></span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input-to-hidden, hidden-to-hidden, and biases for the RNN</span></span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weight and bias for the hidden-to-vocab transformation.</span></span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward and backward passes for the CaptioningRNN.   #</span></span><br><span class="line">    <span class="comment"># In the forward pass you will need to do the following:                   #</span></span><br><span class="line">    <span class="comment"># (1) Use an affine transformation to compute the initial hidden state     #</span></span><br><span class="line">    <span class="comment">#     from the image features. This should produce an array of shape (N, H)#</span></span><br><span class="line">    <span class="comment"># (2) Use a word embedding layer to transform the words in captions_in     #</span></span><br><span class="line">    <span class="comment">#     from indices to vectors, giving an array of shape (N, T, W).         #</span></span><br><span class="line">    <span class="comment"># (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #</span></span><br><span class="line">    <span class="comment">#     process the sequence of input word vectors and produce hidden state  #</span></span><br><span class="line">    <span class="comment">#     vectors for all timesteps, producing an array of shape (N, T, H).    #</span></span><br><span class="line">    <span class="comment"># (4) Use a (temporal) affine transformation to compute scores over the    #</span></span><br><span class="line">    <span class="comment">#     vocabulary at every timestep using the hidden states, giving an      #</span></span><br><span class="line">    <span class="comment">#     array of shape (N, T, V).                                            #</span></span><br><span class="line">    <span class="comment"># (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #</span></span><br><span class="line">    <span class="comment">#     the points where the output word is &lt;NULL&gt; using the mask above.     #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># In the backward pass you will need to compute the gradient of the loss   #</span></span><br><span class="line">    <span class="comment"># with respect to all model parameters. Use the loss and grads variables   #</span></span><br><span class="line">    <span class="comment"># defined above to store loss and gradients; grads[k] should give the      #</span></span><br><span class="line">    <span class="comment"># gradients for self.params[k].                                            #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># Note also that you are allowed to make use of functions from layers.py   #</span></span><br><span class="line">    <span class="comment"># in your implementation, if needed.                                       #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    caches = []</span><br><span class="line">    out, cache = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    word_in, cache = word_embedding_forward(captions_in, W_embed)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:  <span class="comment"># must rnn or lstm</span></span><br><span class="line">        out, cache = rnn_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out, cache = lstm_forward(word_in, out, Wx, Wh, b)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    out, cache = temporal_affine_forward(out, W_vocab, b_vocab)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss, dx = temporal_softmax_loss(out, captions_out, mask)</span><br><span class="line"></span><br><span class="line">    dx, grads[<span class="string">&#x27;W_vocab&#x27;</span>], grads[<span class="string">&#x27;b_vocab&#x27;</span>] = temporal_affine_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = rnn_backward(dx, caches.pop())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        d_caption, dx, grads[<span class="string">&#x27;Wx&#x27;</span>], grads[<span class="string">&#x27;Wh&#x27;</span>], grads[<span class="string">&#x27;b&#x27;</span>] = lstm_backward(dx, caches.pop())</span><br><span class="line">    grads[<span class="string">&#x27;W_embed&#x27;</span>] = word_embedding_backward(d_caption, caches.pop())</span><br><span class="line">    _, grads[<span class="string">&#x27;W_proj&#x27;</span>], grads[<span class="string">&#x27;b_proj&#x27;</span>] = affine_backward(dx, caches.pop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p><strong>sample 函数</strong>，位于<code>rnn.py</code> rnn类内。其要实现的效果如下图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/03/AtirSuRMczObNC9.png" alt="image-20200903234531509"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, features, max_length=<span class="number">30</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Run a test-time forward pass for the model, sampling captions for input</span></span><br><span class="line"><span class="string">    feature vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep, we embed the current word, pass it and the previous hidden</span></span><br><span class="line"><span class="string">    state to the RNN to get the next hidden state, use the hidden state to get</span></span><br><span class="line"><span class="string">    scores for all vocab words, and choose the word with the highest score as</span></span><br><span class="line"><span class="string">    the next word. The initial hidden state is computed by applying an affine</span></span><br><span class="line"><span class="string">    transform to the input image features, and the initial word is the &lt;START&gt;</span></span><br><span class="line"><span class="string">    token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For LSTMs you will also have to keep track of the cell state; in that case</span></span><br><span class="line"><span class="string">    the initial cell state should be zero.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Array of input image features of shape (N, D).</span></span><br><span class="line"><span class="string">    - max_length: Maximum length T of generated captions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - captions: Array of shape (N, max_length) giving sampled captions,</span></span><br><span class="line"><span class="string">      where each element is an integer in the range [0, V). The first element</span></span><br><span class="line"><span class="string">      of captions should be the first sampled word, not the &lt;START&gt; token.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = features.shape[<span class="number">0</span>]</span><br><span class="line">    captions = self._null * np.ones((N, max_length), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unpack parameters</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">&#x27;W_proj&#x27;</span>], self.params[<span class="string">&#x27;b_proj&#x27;</span>]</span><br><span class="line">    W_embed = self.params[<span class="string">&#x27;W_embed&#x27;</span>]</span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">&#x27;Wx&#x27;</span>], self.params[<span class="string">&#x27;Wh&#x27;</span>], self.params[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">&#x27;W_vocab&#x27;</span>], self.params[<span class="string">&#x27;b_vocab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement test-time sampling for the model. You will need to      #</span></span><br><span class="line">    <span class="comment"># initialize the hidden state of the RNN by applying the learned affine   #</span></span><br><span class="line">    <span class="comment"># transform to the input image features. The first word that you feed to  #</span></span><br><span class="line">    <span class="comment"># the RNN should be the &lt;START&gt; token; its value is stored in the         #</span></span><br><span class="line">    <span class="comment"># variable self._start. At each timestep you will need to do to:          #</span></span><br><span class="line">    <span class="comment"># (1) Embed the previous word using the learned word embeddings           #</span></span><br><span class="line">    <span class="comment"># (2) Make an RNN step using the previous hidden state and the embedded   #</span></span><br><span class="line">    <span class="comment">#     current word to get the next hidden state.                          #</span></span><br><span class="line">    <span class="comment"># (3) Apply the learned affine transformation to the next hidden state to #</span></span><br><span class="line">    <span class="comment">#     get scores for all words in the vocabulary                          #</span></span><br><span class="line">    <span class="comment"># (4) Select the word with the highest score as the next word, writing it #</span></span><br><span class="line">    <span class="comment">#     (the word index) to the appropriate slot in the captions variable   #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># For simplicity, you do not need to stop generating after an &lt;END&gt; token #</span></span><br><span class="line">    <span class="comment"># is sampled, but you can if you want to.                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You will not be able to use the rnn_forward or lstm_forward       #</span></span><br><span class="line">    <span class="comment"># functions; you&#x27;ll need to call rnn_step_forward or lstm_step_forward in #</span></span><br><span class="line">    <span class="comment"># a loop.                                                                 #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> we are still working over minibatches in this function. Also if   #</span></span><br><span class="line">    <span class="comment"># you are using an LSTM, initialize the first cell state to zeros.        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    next_h, _ = affine_forward(features, W_proj, b_proj)</span><br><span class="line">    next_c = np.zeros((N, W_proj.shape[<span class="number">1</span>]))</span><br><span class="line">    word = self._start * np.ones((N,), dtype=np.int32)</span><br><span class="line">    <span class="comment"># generate start token</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">        word, _ = word_embedding_forward(word, W_embed)</span><br><span class="line">        <span class="comment"># embed the word to vector</span></span><br><span class="line">        <span class="keyword">if</span> self.cell_type == <span class="string">&#x27;rnn&#x27;</span>:</span><br><span class="line">            next_h, _ = rnn_step_forward(word, next_h, Wx, Wh, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_h, next_c, _ = lstm_step_forward(word, next_h, next_c, Wx, Wh, b)</span><br><span class="line"></span><br><span class="line">        out, _ = affine_forward(next_h, W_vocab, b_vocab)</span><br><span class="line">        <span class="comment"># get the output</span></span><br><span class="line">        word = out.argmax(axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># sample</span></span><br><span class="line">        captions[:, i] = word</span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="keyword">return</span> captions</span><br></pre></td></tr></table></figure><h2 id="Image-Captioning-with-LSTMs"><a href="#Image-Captioning-with-LSTMs" class="headerlink" title="Image Captioning with LSTMs"></a>Image Captioning with LSTMs</h2><p>本部分主要就是将vanilla RNN变为LSTM在重复上述的任务。</p><p>首先是forward，根据公式敲一下就ok了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span>(<span class="params">x, prev_h, prev_c, Wx, Wh, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that a sigmoid() function has already been provided for you in this file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - prev_h: Previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - prev_c: previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - next_c: Next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for backward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    next_h, next_c, cache = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, H = prev_h.shape</span><br><span class="line">    a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    i = sigmoid(a[:, :H])</span><br><span class="line">    f = sigmoid(a[:, H:<span class="number">2</span>*H])</span><br><span class="line">    o = sigmoid(a[:, <span class="number">2</span>*H:<span class="number">3</span>*H])</span><br><span class="line">    g = np.tanh(a[:, <span class="number">3</span>*H:])</span><br><span class="line">    next_c = f * prev_c + i * g</span><br><span class="line">    next_h = o * np.tanh(next_c)</span><br><span class="line">    cache = i, f, o, g, next_c, Wh, Wx, prev_c, prev_h, x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_h, next_c, cache</span><br></pre></td></tr></table></figure><p>经历过那么多次求导，出现的问题越来越少了，实现起来也比较顺畅，下面给出我整理后的求导过程。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/06/Z4HB6QspSybWL9A.png" alt="image-20200906111112135"></p><p>然后就是敲一下代码了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span>(<span class="params">dnext_h, dnext_c, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    i, f, o, g, next_c, Wh, Wx, prev_c, prev_h, x = cache</span><br><span class="line">    dprev_c = dnext_c * f + dnext_h * o * f * (<span class="number">1</span> - np.tanh(next_c)**<span class="number">2</span>)</span><br><span class="line">    dc = dnext_c + (<span class="number">1</span> - np.tanh(next_c)**<span class="number">2</span>) * o * dnext_h     <span class="comment"># 这里遇到了问题</span></span><br><span class="line">    di = dc * g * i * (<span class="number">1</span> - i)</span><br><span class="line">    df = dc * prev_c * f * (<span class="number">1</span> - f)</span><br><span class="line">    do = dnext_h * np.tanh(next_c) * o * (<span class="number">1</span> - o)</span><br><span class="line">    dg = dc * i * (<span class="number">1</span> - g**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    da = np.hstack((di, df, do, dg))</span><br><span class="line">    dprev_h = np.dot(da, Wh.T)</span><br><span class="line">    dx = np.dot(da, Wx.T)</span><br><span class="line">    db = np.sum(da, axis=<span class="number">0</span>)</span><br><span class="line">    dWx = np.dot(x.T, da)</span><br><span class="line">    dWh = np.dot(prev_h.T, da)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>接下来就是对于一个sequence而不是单独的time step使用LSTM了。首先是forward</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span>(<span class="params">x, h0, Wx, Wh, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></span><br><span class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></span><br><span class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    N, H = h0.shape</span><br><span class="line">    h = np.zeros((N, T, H))</span><br><span class="line">    cache = []</span><br><span class="line">    c0 = np.zeros_like(h0)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, c0, c = lstm_step_forward(x[:, i, :], h0, c0, Wx, Wh, b)</span><br><span class="line">        h[:, i, :] = h0</span><br><span class="line">        cache.append(c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br></pre></td></tr></table></figure><div class="note warning">            <p>需要注意的是，在backward时，如何传递梯度：up stream的当前time step的loss + 上一个step step传回来的loss</p>          </div><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span>(<span class="params">dh, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    _, D = cache[<span class="number">0</span>][<span class="number">-1</span>].shape    <span class="comment"># cache[0][-1]对应x</span></span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dc = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, <span class="number">4</span> * H))</span><br><span class="line">    dWh = np.zeros((H, <span class="number">4</span> * H))</span><br><span class="line">    db = np.zeros((<span class="number">4</span> * H,))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        dx[:, i, :], dh0, dc, dWx_, dWh_, db_ = lstm_step_backward(dh[:, i, :] + dh0, dc, cache.pop())</span><br><span class="line">        db += db_</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>剩下的部分就比较ez了，只需对于上一个task的代码稍作修改即可，具体的代码就不再列出。</p><h2 id="Network-Visualization-PyTorch"><a href="#Network-Visualization-PyTorch" class="headerlink" title="Network Visualization (PyTorch)"></a>Network Visualization (PyTorch)</h2><p>本部分主要实现<strong>Saliency Maps,  Fooling image, Class visualization</strong> 三种的实现本质都是计算输入图片的梯度，Saliency Map是直接将关于正确label的loss的梯度的绝对值显示出来；Fooling Image 则是利用梯度信息，将一个A类别的输入变为网络识别为B类别；Class Visualization则是输入一个噪声，使用gradient ascent利用梯度信息将该图片在期望变成的类别下，神经网络的输出的期望类别的class score最大。由于使用pytorch，可以直接计算grad，整体实现比较简单，理解好这几个过程就可以了。代码部分不再单独给出，详情见我的github仓库。</p><div class="note warning">            <p>在代码运行时，可能会遇到使用numpy.load()函数报错的情况，提示需要将allow_pickle=Ture，此时只需要在该np.load的参数内加入‘allow_pickle=True’即可。具体细节可见<code>cs231n/data_utils.py/load_imagenet_val</code></p>          </div><h2 id="Style-Transfer"><a href="#Style-Transfer" class="headerlink" title="Style Transfer"></a>Style Transfer</h2><p>本部分就是实现style transfer的部分了！难度不高，核心就是实现好3个loss，并将整个流程梳理下来即可。实现后真的非常好玩！</p><p>首先是Style Transfer（2016）整体的框图</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/6FYEV92PxcUlXDH.png" alt="image-20200910100801040"></p><p>从图中就可以看出，包含两个loss，分别是：style image每层的每个filter的activation map的gram matrix（用来衡量相似性）和input image的gram matrix之间产生的loss（<strong>Style Loss</strong>）和 input image和content image之间的差异性（<strong>Content Loss</strong>）。最后为了使生成的图片更加真实，会加入一个正则项，这里使用的是<strong>Total-variation regularization</strong>。所以最终的loss就是：**Style Loss+Content Loss + Total-variation regularization **。之后使用这个loss计算输入图片的梯度，并使用Adam或者SDG等方法更新输入图片即可。</p><p>提取特征的网络使用的是squeeze net，因为其模型参数少，运算快且性能适中。</p><h3 id="Content-Loss"><a href="#Content-Loss" class="headerlink" title="Content Loss"></a>Content Loss</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/3cBroD8hxIkadGQ.png" alt="image-20200910101647611"></p><p>比较简单，如框图所示，就是将input image和content image在某一层的activation map做一下reshape，从1*C*H<em>W变为1\</em>C*(H*W)即可。相减后逐元素平方再求和即可。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span>(<span class="params">content_weight, content_current, content_original</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Compute the content loss for style transfer.</span></span><br><span class="line"><span class="string">Inputs:</span></span><br><span class="line"><span class="string">- content_weight: Scalar giving the weighting for the content loss.</span></span><br><span class="line"><span class="string">- content_current: features of the current image; this is a PyTorch Tensor of shape</span></span><br><span class="line"><span class="string">  (1, C_l, H_l, W_l).</span></span><br><span class="line"><span class="string">- content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">- scalar content loss</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    loss = content_weight * torch.sum(torch.square(content_current.squeeze() - content_original.squeeze()))</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Style-Loss"><a href="#Style-Loss" class="headerlink" title="Style Loss"></a>Style Loss</h3><p>稍微复杂一点，核心就是计算一个Gram matrix，该矩阵用来衡量similarity。下图为Gram matrix计算的示意图。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/LSUCjJ5Y19N2AtF.png" alt="image-20200910102135973"></p><p>左边是某一层的activation map，之后将其变为C*(H*W)，所有行向量直接做內积，就有了C*C的gram matrix。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">features, normalize=True</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the Gram matrix from features.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: PyTorch Tensor of shape (N, C, H, W) giving features for</span></span><br><span class="line"><span class="string">      a batch of N images.</span></span><br><span class="line"><span class="string">    - normalize: optional, whether to normalize the Gram matrix</span></span><br><span class="line"><span class="string">        If True, divide the Gram matrix by the number of neurons (H * W * C)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - gram: PyTorch Tensor of shape (N, C, C) giving the</span></span><br><span class="line"><span class="string">      (optionally normalized) Gram matrices for the N input images.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N,C,H,W = features.size()</span><br><span class="line">    new_features = features.reshape((N,C,H*W))</span><br><span class="line">    gram_mat = torch.zeros(N,C,C)</span><br><span class="line">    gram_mat = torch.bmm(new_features, new_features.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        <span class="keyword">return</span> gram_mat / (H*W*C)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> gram_matm</span><br></pre></td></tr></table></figure><p>之后就是如框图所示，将style image各层的gram matrix和input image各层的gram matrix相减后逐元素平方再求和即可。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span>(<span class="params">feats, style_layers, style_targets, style_weights</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the style loss at a set of layers.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - feats: list of the features at every layer of the current image, as produced by</span></span><br><span class="line"><span class="string">      the extract_features function.</span></span><br><span class="line"><span class="string">    - style_layers: List of layer indices into feats giving the layers to include in the</span></span><br><span class="line"><span class="string">      style loss.</span></span><br><span class="line"><span class="string">    - style_targets: List of the same length as style_layers, where style_targets[i] is</span></span><br><span class="line"><span class="string">      a PyTorch Tensor giving the Gram matrix of the source style image computed at</span></span><br><span class="line"><span class="string">      layer style_layers[i].</span></span><br><span class="line"><span class="string">    - style_weights: List of the same length as style_layers, where style_weights[i]</span></span><br><span class="line"><span class="string">      is a scalar giving the weight for the style loss at layer style_layers[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - style_loss: A PyTorch Tensor holding a scalar giving the style loss.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tensor = torch.tensor(())</span><br><span class="line">    loss = tensor.new_zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(style_layers)):</span><br><span class="line">        gram_mat = gram_matrix(feats[style_layers[i]])</span><br><span class="line">        loss += style_weights[i] * torch.sum((gram_mat - style_targets[i]).square())</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Total-variation-regularization"><a href="#Total-variation-regularization" class="headerlink" title="Total-variation regularization"></a>Total-variation regularization</h3><p>该项是一个正则化项，公式如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/FXuz216PhIvrbx9.png" alt="image-20200910102605899"></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span>(<span class="params">img, tv_weight</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute total variation loss.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.</span></span><br><span class="line"><span class="string">    - tv_weight: Scalar giving the weight w_t to use for the TV loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - loss: PyTorch Variable holding a scalar giving the total variation loss</span></span><br><span class="line"><span class="string">      for img weighted by tv_weight.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = tv_weight * (torch.sum((img[<span class="number">0</span>,:,<span class="number">1</span>:,:] - img[<span class="number">0</span>,:,:<span class="number">-1</span>,:]).square()) + torch.sum((img[<span class="number">0</span>,:,:,<span class="number">1</span>:] - img[<span class="number">0</span>,:,:,:<span class="number">-1</span>]).square()))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="Over-ALL"><a href="#Over-ALL" class="headerlink" title="Over ALL"></a>Over ALL</h3><p>完成的上面的部分后就是整合了，note book中的代码已经给出。不再赘述。值得注意的是，使用同样的代码，我们还可以完成<strong>Feature Inversion</strong>和<strong>texture synthesis</strong>这两个任务。对于<strong>texture synthesis</strong>，只需将content loss置零即可。对于<strong>Feature Inversion</strong>只需将style loss置零即可。下面是一些自己测试的结果。</p><div class="fig figcenter fighighlight">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/MojOJTfCVnYA46L.png" width="30%">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/BrTLcfMZoCpSU98.png" width="20%" style="border-left: 1px solid black;">  <div class="figcaption">Style Transfer Left: Style img and Input img. Right: result(200 iteration).</div></div><div class="fig figcenter fighighlight">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/yLZAnhrJ5VItsux.png" width="30%">  <img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/09/10/enD34UK7J1oqut5.png" width="20%" style="border-left: 1px solid black;">  <div class="figcaption">texture synthesis Left: Style img. Right: result(200 iteration).</div></div><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>本部分的GAN实现了报过最原始的2014年 GoodFellow的原始GAN，以及Last Square GAN（优化了损失函数）和Deep Convolutional GANs。整体来讲使用pytorch建立模型和训练都比较简单，note book中主要实现的就是discriminator和generator的loss，将这个实现好即可。这部分的难度不是很高，就不再列出了，具体代码和结果可以参考我的github仓库。</p><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>至此，整个CS231n的课程就结束啦！为期了一个多月，中间因为各种事前耽误了几天，本来计划1个月内就搞定的。感觉CS231n课程整体来讲的难度适中，在数学推倒部分设计的稍微少了一些，但可以建立很多intuition的东西，总之还是收获颇丰的。</p><p>最大的收获就是他的assignment了，有一定难度的同时也极大的加深了对于这些知识的理解。</p><p>TODO：</p><ul><li>下一步可能会写一个GAN相关的小总结。</li><li>看一些more mathematic的东西</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment3-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment3-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment3 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment2 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/12/CS231n-Assignment2-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-12T09:16:42.000Z</published>
    <updated>2020-08-26T14:09:37.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment2-遇到的问题"><a href="#CS231n-Assignment2-遇到的问题" class="headerlink" title="CS231n Assignment2 遇到的问题"></a>CS231n Assignment2 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><p>我的assignment的<a href="https://github.com/canVa4/CS231n-Assignments">github仓库</a>，包含全部的代码和notebook。</p><h2 id="Fully-connected-Neural-Network"><a href="#Fully-connected-Neural-Network" class="headerlink" title="Fully-connected Neural Network"></a>Fully-connected Neural Network</h2><p>相比于Assignment1，对于整个网络的实现进行了进一步的封装，可以实现任意深度，大小的MLP。</p><div class="note info">            <p>值得一看的代码部分！solver.py中实现调用更新规则函数（在optim.py中实现），实现的很有趣！</p>          </div><p>基本思路为：使用getattr获取定义在optim.py中定义好的update rule函数！我是第一次见这种写法，感觉很巧妙，值得学习一波:)</p><p>当然，要先import optim。</p><p>Core Code(extract):</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.update_rule = kwargs.pop(<span class="string">&#x27;update_rule&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>)</span><br><span class="line">...</span><br><span class="line">   <span class="comment"># Make sure the update rule exists, then replace the string</span></span><br><span class="line">   <span class="comment"># name with the actual function</span></span><br><span class="line">   <span class="keyword">if</span> <span class="keyword">not</span> hasattr(optim, self.update_rule):</span><br><span class="line">       <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)</span><br><span class="line">self.update_rule = getattr(optim, self.update_rule)</span><br></pre></td></tr></table></figure><p>其余部分的实现（如：affine，ReLU的forward和backward；优化算法）注意好细节后都比较容易实现，因为比较复杂的代码框架已经提供好了。</p><p>值得注意的是，官方github仓库中的关于课程内容的markdown笔记值得一读。<a href="http://github.com/cs231n/cs231n.github.io" title="官方课程github仓库">课程github仓库</a>. </p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>本部分的实现我遇到了一些问题，在完成上也花费了很多时间，主要遇到的问题还是导数的求取，以及如何将其变化为numpy数组的形式。</p><h3 id="The-Gradient-of-Batch-Normalization"><a href="#The-Gradient-of-Batch-Normalization" class="headerlink" title="The Gradient of Batch Normalization"></a>The Gradient of Batch Normalization</h3><p>由于本部分想知道自己的代码是正确与否需要变为代码后，进行numerical check才能验证。所以，在第一次求导后，我花了很长时间debug，但最后发现是导数求错了。。。</p><p>求导中还是遇到了不少的问题，尤其是在使用链式法则的时候遇到了问题。看来是好久没有好好推公式了QuQ，于是重新复习了一下chain rule和矩阵求导之类的；并重新推到了一下公式。NOTE：以下推导可能并不非常严谨（部分可能不符合矩阵相乘维数），但作为示意和实现代码足够了。</p><p>首先是正向（forward pass）的公式。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/heQRF4Um6j7pOxk.png" alt="image-20200819013718586"></p><p>然后是backward计算梯度。这里在计算关于x的偏导时，我遇到了一些问题，很容易丢掉一个导数项；画出变量之间的关系图可以很好地解决这个问题。推导如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ovXSVBY5bPkcyA3.png" alt="image-20200819013948134.png"></p><p>有了这些部分，就可以实现第一个函数batchnorm_backward()和batchnorm_forward()了！</p><p>实际上，上式还可以继续化简，化简的结果更加简洁，省去很多中间变量。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/eudk5aMOFLJCApj.png" alt="image-20200819014219098"></p><p>至此，batch normalization的公式推导部分就OK了，下面就是代码实现了。</p><p>这里需要注意的是，由于在predict的时候，一般是没有batch数据的，所以此时没法直接使用batch normalization，所以一种方法就是利用训练时得到的均值和方差来作为predict时的均值和方差。其更新方法使用momentum更新为：</p><pre><code>running_mean = momentum * running_mean + (1 - momentum) * sample_meanrunning_var = momentum * running_var + (1 - momentum) * sample_var</code></pre><h3 id="Code-Implement-of-Batch-Normalization"><a href="#Code-Implement-of-Batch-Normalization" class="headerlink" title="Code Implement of Batch Normalization"></a>Code Implement of Batch Normalization</h3><p>forward没啥可说的，很easy，分开train与test即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span>(<span class="params">x, gamma, beta, bn_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: &#x27;train&#x27; or &#x27;test&#x27;; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mode = bn_param[<span class="string">&#x27;mode&#x27;</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">&#x27;momentum&#x27;</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        sample_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line"></span><br><span class="line">        x_norm = (x - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">        cache = x, x_norm, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        x_norm = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid forward batchnorm mode &quot;%s&quot;&#x27;</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward实现两个版本，我是一个根据未化简公式来计算，另一个是根据化简后公式来计算，对比后明显可以看到化简后大量减少了运算次数，可以达到原来速度的3倍。</p><p>未化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_norm.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_norm * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx_norm = dout * gamma</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - sample_mean), axis=<span class="number">0</span>) * np.power(sample_var + eps, <span class="number">-1.5</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(sample_var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">-1</span>/N) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=<span class="number">0</span>) + dL_dvar * np.sum(<span class="number">-2</span>*(x - sample_mean)/N, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / N) * (x - sample_mean) * dL_dvar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><p>化简公式版：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass. </span></span><br><span class="line"><span class="string">    See the jupyter notebook for more hints.</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dx = (<span class="number">1</span> / N) * mid * (N * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>LN按照如下公式来输出，实际上就是把BN倒过来。。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/19/ugYP7bKtTAvhzOj.png" alt="image-20200819015437962"></p><p>LN的操作类似于将BN做了一个“<strong>转置</strong>”，对同一层网络的输出做一个标准化。注意，同一层的输出是单个图片的输出，比如对于一个batch为32的神经网络训练，会有32个均值和方差被得出，<strong>每个均值和方差都是由单个图片的所有channel之间做一个标准化</strong>。这么操作，就使得LN不受batch size的影响。</p><p>在代码的实现上只需将所有的相关矩阵装置一下就OK啦，即对于转置过来的输入x做BN即可！注意要保证输出的维数正确。</p><p>Layer Normalization Forward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_forward</span>(<span class="params">x, gamma, beta, ln_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Forward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During both training and test-time, the incoming data is normalized per data-point,</span></span><br><span class="line"><span class="string">    before being scaled by gamma and beta parameters identical to that of batch normalization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note that in contrast to batch normalization, the behavior during train and test-time for</span></span><br><span class="line"><span class="string">    layer normalization are identical, and we do not need to keep track of running averages</span></span><br><span class="line"><span class="string">    of any sort.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - ln_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">        - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    sample_var = np.var(x.T, axis=<span class="number">0</span>)</span><br><span class="line">    x_norm = (x.T - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">    out = gamma * x_norm.T + beta</span><br><span class="line">    cache = x, x_norm.T, sample_mean, sample_var, gamma, beta, eps</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>Layer Normalization Backward：这里我实现了两个版本，分别是基于化简后的公式和未化简后的公式，均通过测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Backward pass for layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you can heavily rely on the work you&#x27;ve done already</span></span><br><span class="line"><span class="string">    for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from layernorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, x_hat, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    N, D = x_hat.shape</span><br><span class="line"></span><br><span class="line">    mid = <span class="number">1</span> / np.sqrt(sample_var + eps)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dxhat = dout * gamma</span><br><span class="line">    dxhat = dxhat.T</span><br><span class="line">    x_hat = x_hat.T</span><br><span class="line">    dx = (<span class="number">1</span> / D) * mid * (D * dxhat - np.sum(dxhat, axis=<span class="number">0</span>) - x_hat * np.sum(dxhat * x_hat, axis=<span class="number">0</span>))</span><br><span class="line">    dx = dx.T</span><br><span class="line"></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment">#    Another vision of  LN backward (based on the origin vision of bn backward)     #</span></span><br><span class="line">    <span class="comment">#####################################################################################</span></span><br><span class="line">    <span class="comment"># x, x_norm, sample_mean, sample_var, gamma, beta, eps = cache</span></span><br><span class="line">    <span class="comment"># N, D = x_norm.shape</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dbeta = np.sum(dout, axis=0)</span></span><br><span class="line">    <span class="comment"># dgamma = np.sum(x_norm * dout, axis=0)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># x = x.T</span></span><br><span class="line">    <span class="comment"># dout = dout.T</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># dx_norm = dout.T * gamma</span></span><br><span class="line">    <span class="comment"># dx_norm = dx_norm.T</span></span><br><span class="line">    <span class="comment"># dL_dvar = -0.5 * np.sum(dx_norm * (x - sample_mean), axis=0) * np.power(sample_var + eps, -1.5)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx = dx_norm / np.sqrt(sample_var + eps)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (-1/D) * np.sum(dx_norm / np.sqrt(sample_var + eps), axis=0) + dL_dvar * np.sum(-2*(x - sample_mean)/N, axis=0)</span></span><br><span class="line">    <span class="comment"># # add L--&gt;var--&gt;x_i</span></span><br><span class="line">    <span class="comment"># dx += (2 / D) * (x - sample_mean) * dL_dvar</span></span><br><span class="line">    <span class="comment"># dx = dx.T</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h3 id="BN-vs-LN"><a href="#BN-vs-LN" class="headerlink" title="BN vs LN"></a>BN vs LN</h3><h4 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h4><p>首先是BN，BN是通过mini-batch来对相应的activation做规范化操作，使得输出的各个维度的均值为0，方差为1（标准化）。而最后的“scale and shift”，即加入一个放射变换，则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入，同时也缓解因为数据可能会因此丢失了一些信息，所以再加上beta和gama来恢复原始数据，这里beta和gama是可学习的。</p><p><strong>BN的好处：</strong></p><p>(1) 减轻了对参数、权重初始化的依赖。</p><p>(2) 训练更快，可以使用更高的学习率。</p><p>(3) BN一定程度上增加了泛化能力。</p><p><strong>BN的缺点：</strong></p><p>batch normalization依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。会引入很多噪声误差，若网络队伍差很敏感，则会难以训练和收敛。</p><p>这一个特性，导致batch normalization不适合以下的几种场景。</p><p>(1)batch非常小，比如训练资源有限无法应用较大的batch。</p><p>(2)RNN，因为它是一个动态的网络结构，即输入的size是不固定的，同一个batch中训练实例有长有短，无法根据BN的公式进行标准化。</p><p>关于Normalization的<strong>有效的原因</strong>：</p><p>Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，目前大部分资料都这样解释，比如BN的原始论文认为的缓解了Internal Covariate Shift(ICS)问题。加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。</p><h4 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h4><p>BN 的一个缺点是需要较大的 batchsize 才能合理估训练数据的均值和方差，这在计算资源比较有限的时候往往不能达到，同时它也很难应用在数据长度不同的 RNN 模型上。Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化，他是针对于per datapoint的更新。</p><p>整体而言，LN用于RNN效果比较明显，但是在CNN上，不如BN。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout的代码部分非常简单，material中已经给出了代码实现，只需要实现一下forward和backw以及更新一下计算loss的函数即可。需要注意的是增加loss的部分，这里我使用caches当做<strong>堆栈</strong>存储前向计算loss时产生的caches，这样反向传播时只需要依次pop并根据网络结构计算梯度即可。本部分代码位于：<code>cs231n/classifiers/fc_net.py</code>。 该loss位于为FullyConnectedNet类内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute loss and gradient for the fully-connected net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / output: Same as TwoLayerNet above.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    X = X.astype(self.dtype)</span><br><span class="line">    mode = <span class="string">&#x27;test&#x27;</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">&#x27;train&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">    <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">    <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">        self.dropout_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    <span class="keyword">if</span> self.normalization==<span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">        <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">            bn_param[<span class="string">&#x27;mode&#x27;</span>] = mode</span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    scores = X</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">        W = self.params[<span class="string">&#x27;W&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        b = self.params[<span class="string">&#x27;b&#x27;</span> + str(i+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            scores, cache = affine_forward(scores, W, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                scores, cache = affine_relu_forward(scores, W, b)</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;batchnorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_bn_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">elif</span> self.normalization == <span class="string">&quot;layernorm&quot;</span>:</span><br><span class="line">                gamma = self.params[<span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                beta = self.params[<span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)]</span><br><span class="line">                scores, cache = affine_ln_relu_forward(scores, W, b, gamma, beta, self.bn_params[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cache = <span class="literal">None</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout <span class="keyword">and</span> i != self.num_layers<span class="number">-1</span>:</span><br><span class="line">            scores, cache = dropout_forward(scores, self.dropout_param)</span><br><span class="line">            caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If test mode return early</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">    reg = self.reg</span><br><span class="line">    loss, dx = softmax_loss(scores, y)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(self.num_layers)):</span><br><span class="line">        w = <span class="string">&#x27;W&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        b = <span class="string">&#x27;b&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        gamma = <span class="string">&#x27;gamma&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        beta = <span class="string">&#x27;beta&#x27;</span> + str(i + <span class="number">1</span>)</span><br><span class="line">        loss += <span class="number">0.5</span> * reg * np.sum(W * W)  <span class="comment"># add reg term</span></span><br><span class="line">        <span class="keyword">if</span> i == self.num_layers - <span class="number">1</span>:</span><br><span class="line">            dx, grads[w], grads[b] = affine_backward(dx, caches.pop())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">                dx = dropout_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                dx, grads[w], grads[b] = affine_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;batchnorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_bn_relu_backward(dx, caches.pop())</span><br><span class="line">            <span class="keyword">if</span> self.normalization == <span class="string">&#x27;layernorm&#x27;</span>:</span><br><span class="line">                dx, grads[w], grads[b], grads[gamma], grads[beta] = affine_ln_relu_backward(dx, caches.pop())</span><br><span class="line">        grads[w] += reg * self.params[w]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><h2 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h2><p>这部分就是实现CNN了！核心就是实现好卷积层和pooling层。同时也修改batch normalization以便适用于CNN，同时增加group normalization。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>由于在实现时并不需要考虑计算复杂度和时间复杂度，我使用了最简单直接的方法，在forward时，同官方给的note一样，每次更新一个激活神经元的值，即使用4层循环嵌套，直观的实现卷积的过程。TODO：向量化方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span>(<span class="params">x, w, b, conv_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - &#x27;pad&#x27;: The number of pixels that will be used to zero-pad the input. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, &#x27;pad&#x27; zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) / stride)</span><br><span class="line">    out = np.zeros((N, F, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.sum(</span><br><span class="line">                        x_pad[n, :, h * stride:h * stride + HH, w_mid * stride:w_mid * stride + WW] * w[f, :, :, :]) + b[f]</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>在实现backward时，我也写出了简单情况下更新的公式，并根据这个最简单的展开形式以此来反向求梯度。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/26/7R2qvGbEiOLPgZw.png" alt="image-20200826005100903"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    N, F, H_out, W_out = dout.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    dx_pad = np.zeros_like(x_pad)</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.sum(dout, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW]</span><br><span class="line">                    dx_pad[n, :, stride * h_mid:stride * h_mid + HH, stride * w_mid:stride * w_mid + WW] += \</span><br><span class="line">                        dout[n, f, h_mid, w_mid] * w[f, :, :, :]</span><br><span class="line">                    dw[f, :, :, :] += window * dout[n, f, h_mid, w_mid]</span><br><span class="line">    dx = dx_pad[:, :, pad:pad + H, pad:pad + W]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>forward很简单，只需取respect field中最大的即可；backward时，将取最大值的位置处的梯度直接回传，其余置一即可。比较简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span>(<span class="params">x, pool_param</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;pool_height&#x27;: The height of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;pool_width&#x27;: The width of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here. Output size is given by </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    H_out = int(<span class="number">1</span> + (H - pool_height) / stride)</span><br><span class="line">    W_out = int(<span class="number">1</span> + (W - pool_width) / stride)</span><br><span class="line">    out = np.zeros((N, C, H_out, W_out))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    out[n, f, h, w_mid] = np.max(</span><br><span class="line">                        x[n, f, h * stride:h * stride + pool_height, w_mid * stride:w_mid * stride + pool_width])</span><br><span class="line"></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    N, C, H_out, W_out = dout.shape</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> h_mid <span class="keyword">in</span> range(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_mid <span class="keyword">in</span> range(W_out):</span><br><span class="line">                    window = x[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                             stride * w_mid:stride * w_mid + pool_width]</span><br><span class="line">                    mask = window == np.max(window)</span><br><span class="line">                    dx[n, f, stride * h_mid:stride * h_mid + pool_height,</span><br><span class="line">                    stride * w_mid:stride * w_mid + pool_width] = mask * dout[n, f, h_mid, w_mid]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h3 id="Spatial-Batch-Normalization"><a href="#Spatial-Batch-Normalization" class="headerlink" title="Spatial Batch Normalization"></a>Spatial Batch Normalization</h3><p>实现起来非常简单，只需要重新reshape输入，之后使用之前实现过的正常的Batch Normalization就OK了，代码请看我的github仓库，这部分没有遇到问题。</p><h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><p>forward只需要稍微修改正常的Batch Normalization即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_forward</span>(<span class="params">x, gamma, beta, G, gn_param</span>):</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = gn_param.get(<span class="string">&#x27;eps&#x27;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_new = x.reshape((N, G, C // G, H, W))</span><br><span class="line">    mean = np.mean(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var = np.var(x_new, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_norm = (x_new - mean) / np.sqrt(var + eps)</span><br><span class="line">    x_norm = x_norm.reshape((N, C, H, W))</span><br><span class="line">    gamma_new = gamma.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    beta_new = beta.reshape((<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    out = gamma_new * x_norm + beta_new</span><br><span class="line">    cache = G, x, x_norm, mean, var, gamma, beta, eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward也并不复杂，本质上的求导与正常的batch normalization一致，不过在多个导数求和时，需要注意怎么进行sum。这里如果想要通过Gradient check也有一个小坑。。。 ps: 这里我调试了很久。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_backward</span>(<span class="params">dout, cache</span>):</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    G, x, x_norm, mean, var, gamma, beta, eps = cache</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout * x_norm, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.reshape(N, G, C // G, H, W)</span><br><span class="line">    <span class="comment"># 这里想通过Gradientcheck必须需要将其reshape为(1, C, 1, 1)</span></span><br><span class="line">    dbeta = np.sum(dout, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    dx_norm = (dout * gamma).reshape(N, G, C // G, H, W)</span><br><span class="line">    mean = mean.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    var = var.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    dL_dvar = <span class="number">-0.5</span> * np.sum(dx_norm * (x - mean), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)) * np.power(var.squeeze() + eps, <span class="number">-1.5</span>)</span><br><span class="line">    dL_dvar = dL_dvar.reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    mid = H * W * C // G</span><br><span class="line">    <span class="comment"># add L--&gt;y--&gt;x_hat--&gt;x_i</span></span><br><span class="line">    dx = dx_norm / np.sqrt(var + eps)</span><br><span class="line">    <span class="comment"># add L--&gt;mean--&gt;x_i</span></span><br><span class="line">    dx += ((<span class="number">-1</span> / mid) * np.sum(dx_norm / np.sqrt(var + eps), axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>) + dL_dvar * (</span><br><span class="line">        np.sum(<span class="number">-2</span> * (x - mean) / mid, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).reshape(N, G, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># add L--&gt;var--&gt;x_i</span></span><br><span class="line">    dx += (<span class="number">2</span> / mid) * (x - mean) * dL_dvar</span><br><span class="line">    dx = dx.reshape((N, C, H, W))</span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><h2 id="PyTorch-on-CIFAR-10"><a href="#PyTorch-on-CIFAR-10" class="headerlink" title="PyTorch on CIFAR-10"></a>PyTorch on CIFAR-10</h2><p>这部分比较简单，我在实现时没有遇到问题。偷了懒，没有实现最后的CIFAR-10 open-ended challenge。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment2-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment2-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment2 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>单片机解决方案调研</title>
    <link href="http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/"/>
    <id>http://canva4.github.io/2020/08/12/%E5%8D%95%E7%89%87%E6%9C%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94/</id>
    <published>2020-08-12T02:35:36.000Z</published>
    <updated>2021-03-07T13:52:40.336Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单片机解决方案调研"><a href="#单片机解决方案调研" class="headerlink" title="单片机解决方案调研"></a>单片机解决方案调研</h1><p>目标：<strong>模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线通信；具有一定的算力</strong></p><p>本文主要讨论3种不同解决方案。Arduino，stm32和C51系列。列出的这三种都是我有过使用经历的。我经验较多的是stm32，没有用arduino做过比较大型的东西。</p><p>实际上三者并不能直接比较，Arduino算是一个硬件平台，他的早期，也是最广泛的核心是基于AVR单片机（这种芯片我没单独用过）。</p><p>后两者stm32与C51则是两种特定系列的单片机了。</p><table><thead><tr><th></th><th>Arduino</th><th>stm32</th><th>C51</th></tr></thead><tbody><tr><td>模块化</td><td>强（有很多各种各样现成的模块）</td><td>中（配合开发板使用，可以达到部分模块化的效果）</td><td>中（同stm32）</td></tr><tr><td>运算能力</td><td>中、高（一般使用AVR的算力差，现在有支持STM32系列的和esp32的了）</td><td>中、高</td><td>低</td></tr><tr><td>功耗</td><td>低、电池供电足够</td><td>低（极低）、电池供电足够</td><td>极低、电池供电足够</td></tr><tr><td>开发难度</td><td>低、开发速度快、代码易于迭代更新，不必考虑寄存器层面编程</td><td>较高（寄存器复杂，但ST提供封装的的库函数）</td><td>中（硬件资源少，编程难度高）</td></tr><tr><td>价格</td><td>较高</td><td>中</td><td>极低</td></tr><tr><td>支持无线通信</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>优点</td><td>开放周期较短，模块化强，代码移植性强，社区丰富</td><td>功能强大，增加功能灵活，社区丰富</td><td>极其便宜，功耗低</td></tr><tr><td>缺点</td><td>扩展模块可选有限；算力有限；</td><td>开发难度大；程序移植性差；</td><td>算力低下，框架老旧</td></tr><tr><td>总结&amp;建议</td><td><strong>可选方案</strong>。方便开发，算力比较OK；但价格较高。</td><td><strong>可选方案</strong>。芯片功能极其强大；但开发周期和难度可能较长。</td><td>不建议使用，如果要批量生成且算力要求不高，可以考虑</td></tr></tbody></table><h2 id="Arduino"><a href="#Arduino" class="headerlink" title="Arduino"></a>Arduino</h2><p>Arduino准确的说是一个单片机及其外设的集合，比较经典板子的主控是ATMEL出的AVR单片机，比51系列性能强一点。这个集合之所以出名在于其操作简单，不需要涉及很多底层、寄存器层面的编程。例如，stm32库函数的一大堆命令，在这里只需要一句即可完成功能，并且有相当丰富的外设模块。</p><p>总体而言做原型，快速开发的时候，硬件搭设方便，基本不用去设计电路板，画板子之类的，基本上导线连接模块就OK了。代码比较简单易懂的。基本不涉及到寄存器级的操作。总得来说就是开发快。小量定制化还是划算，做产品或者较多数量的成本很高；且由于其代码的高度封装会导致程序效率底下以及资源开销大。</p><p>我个人对于Arduino的使用不是很多，还需进一步调研。</p><p>该图为比较常见的Arduino型号的单片机的性能参数。<a href="https://www.arduino.cn/thread-42417-1-1.html">原文链接</a> 原文发布于2017年</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/aodKOxTjRWQztpb.jpg" alt="211543xjgg8attjataqkgk"></p><h2 id="STM32系列单片机"><a href="#STM32系列单片机" class="headerlink" title="STM32系列单片机"></a>STM32系列单片机</h2><p>stm32是st半导体公司向arm公司购买了核心(嵌入式)版权，加上自己的外设生产的一个系列的芯片。其特点是：功能强大、速度快、外设多。STM32比较常见的框架是ARM CORTEX-m3或m4。并且其：寄存器复杂，直接用汇编操作比较麻烦，但ST官方了提供封装的的库函数，现在还出了专门的代码生成软件cube来简化操作。</p><p>一个STM32常用型号之间对比：<a href="https://blog.csdn.net/ybhuangfugui/article/details/88266385">https://blog.csdn.net/ybhuangfugui/article/details/88266385</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/17/nkjBiR6fT2lSQb7.jpg" alt="en.microcontrollers_FM141.jpg"></p><p>上图为ST官网对于其系列芯片的简介与分类。</p><p>我对于STM32系列芯片的使用比较有经验，我使用的型号为主要为：STM32F1系列和F4系列。</p><p>STM32F407这款芯片，使我们机器人队使用的主控芯片，其最高主频可达<strong>168MHz</strong>（远远大于Arduino的常见型号的16MHz），可见其算力的强大。</p><p>我们队内并没有使用STM32系列的开发板，而是买了裸的芯片，之后自己设计了电路板（设计部分我不是很擅长），画板子、印板子、焊板子、改板子这样进行开发，导致开发一代新的主控板周期比较长。</p><p>不过市面上也有很多STM32现成的开发板，预留了很多IO接口，初步观察感觉基本满足需求，价格适中，如STM32F4系列的开发板不足100元。使用这种开发板一般也不需要自己设计电路，只需购买不同的模块即可交互使用。</p><center class="half">    <img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/7DGvqR42cTMiNVA.jpg width="400"/><img src= "/img/loading.gif" data-lazy-src=https://i.loli.net/2020/08/17/gTPjAvzL4KEnIFC.jpg width="400"/> </center><p>左图的为我近期购买的STM32F1系列的的开发板；右图为STM32F4系列的开发板。例如左边的F1开发板，可以看到这种开发板也像Arduino一样支持很多的扩展功能，而且只需要插接即可。</p><p>整体而言，使用STM32系列也绝对可以实现我们的预期目标，使用开发板也基本可以避免电路的设计等工作。由于STM32系列芯片本身功能强大，其上限应该是高于Arduino方案的。但其开发难度会比Arduino方案难上不少。这里指的STM32开发方案是指使用STM32 CUBEMX硬件配置和生成代码模板（HAL库），之后在代码模板上进行开发（一般使用IAR作为IDE）。</p><h2 id="Arm-Cortex"><a href="#Arm-Cortex" class="headerlink" title="Arm Cortex"></a>Arm Cortex</h2><p>Cortex-M分为：<strong>M0，M0+，M3，M4，M7</strong></p><p>M0，M0+：基础版本，无高性能的MCU；<br>M3：目前最主流的设计内核选型，应用范围广；</p><p>M4：相比于M3的内核来说，M4处理器添加了DSP，和专门的FPU（浮点数处理单元）；重点解释一下：对于CPU（不是SOC）来说，运算浮点类型的数据是很麻烦的一件事，在选型的时候，如若用应用的领域需要大量浮点数据的运算的时候，那么就要选择M4的内核，M4会大大提高处理器性能和运算速度，而如果要要处理的浮点数据不多，则可以直接选择M3内核处理器。</p><p>M7：性能好和功耗高兼具，适合追求极致性能项目；</p><h2 id="Arduino-与-STM32"><a href="#Arduino-与-STM32" class="headerlink" title="Arduino 与 STM32"></a>Arduino 与 STM32</h2><p>通过进一步的了解，我发现了arduino支持了STM32的开发！即可以使用Arduino的IDE来编程。这样可能会降低部分开发难度。</p><p>github链接<a href="https://github.com/rogerclarkmelbourne/Arduino_STM32">https://github.com/rogerclarkmelbourne/Arduino_STM32</a>。目前支持STM32F4和F1系列，其可以将Stm32F103（主要）系列单片机刷入Arduino的Bootloader，并且使用Arduino的编译器和IDE来完成代码的编写，省去了一大部分配置寄存器和学习的时间，完整的性能和灵活性还有待探究。</p><p>同时也有一个类似于arduino+STM32的project，其链接如下。<a href="https://www.leaflabs.com/maple">https://www.leaflabs.com/maple</a>。该板子在淘宝有售，其芯片使stm32f103 arm cortex-M3 32位处理器，主频最高可达72MHz，远远大于常见的Arduino的8位（AVR）MCU。</p><p>Leaf Maple 是一个类似Arduino的开发平台，使用的Cotrex M3内核的32位MCU，所以要比Arduino的8位（AVR）MCU强悍很多，有更高的主频，更丰富的资源。 Leaf Maple也提供了一个类似Arduino IDE的IDE， 并且很多简单上层函数兼容Arduino的函数库，让移植库和代码变得相当简单。比起使用CubeMX+IAR来开发一个STM32项目，使用leaf maple会节省很多时间，更适合初学者和需要快速原型开发的用户。</p><h2 id="Arduino新品M5Stack（使用esp32）"><a href="#Arduino新品M5Stack（使用esp32）" class="headerlink" title="Arduino新品M5Stack（使用esp32）"></a>Arduino新品M5Stack（使用esp32）</h2><p>Arduino的生态总体来讲还是很好用的，目前了解到一款<strong>模块化极强</strong>，性能出色，上市时间不长的支持Arduino开发平台的开发板。M5系列。</p><p>M5Stack是一种模块化、可堆叠扩展的开发板，每个模块一般为5cmX5cm的尺寸，这也是M5Stack名字的由来。与常规的开发板不同，M5Stack更注重产品形态的完整性，更注重用户的应用场景和研发的简易性，M5没有密密麻麻的飞线，没有错乱无章的接口插头，不需要繁琐的开发流程，简简单单、轻轻松松地完成高质量的电子原型创作。（官方介绍）</p><p>M5Stack主要采用ESP32芯片体系，CORE主机内已集成了240M双核主频CPU（esp32）、 WiFi、蓝牙、2.0寸彩色屏幕LCD、扬声器、按键、TF卡、陀螺仪以及内置电池（部分有）。CORE基本满足一般的功能需求，功能模块Function Module则根据应用的情况选择，比如电机驱动、信号采集、通信等功能。另外，也会配备不同的应用底座及配件，方便用户做出高质量的研发。</p><p><a href="https://github.com/m5stack">官方github</a> 与 <a href="https://m5stack.taobao.com/index.htm?spm=2013.1.w5002-22404213498.2.2149622fvBd0zs">官方淘宝链接</a></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/oXGRIYD8r7OMFq6.jpg" alt="img"></p><p>比如其Basic款。其内含2.4G Wi-Fi和蓝牙4.0。外设接口有Type-c，I2C，GPIO和UART接口（数量较少）。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/27/gRsKaQhdYloI5Ut.png" alt="image-20200827162008345"></p><h2 id="简易方案"><a href="#简易方案" class="headerlink" title="简易方案"></a>简易方案</h2><p>简易方案均假定使用2个加速度传感器，并且保证支持WIFI通信，均基本不需要大量焊接（基本做线之类的还是需要的）</p><table><thead><tr><th></th><th>Arduino(M5Stack)</th><th>STM32</th></tr></thead><tbody><tr><td>CORE MCU Unit</td><td><a href="https://item.taobao.com/item.htm?spm=a1z10.1-c-s.w5003-22404213505.1.582d7ef3sJFgr9&ft=t&id=610634829528&scene=taobao_shop">M5Stack Basic</a> 支持I2C总线，内置WiFi  200￥</td><td><a href="https://detail.tmall.com/item.htm?spm=a220o.1000855.0.0.45dd72b09Nb8ev&id=609293737870&skuId=4571066386420">STM32F103最小开发板 </a> 100￥</td></tr><tr><td>Senor</td><td><a href="https://item.taobao.com/item.htm?spm=a1z10.5-c-s.w4002-22404213529.21.23df38edwHEAwd&id=610411236397">ADXL345三轴加速度</a> * 2 (I2C总线 ±16g 13位分辨率) 30￥</td><td><a href="https://item.taobao.com/item.htm?id=45567315525&ali_refid=a3_430582_1006:1103191143:N:nfpYj0PVRKdxrBfSBLtPWA==:e32ca8378afe7d9c3b105a70f8d92779&ali_trackid=1_e32ca8378afe7d9c3b105a70f8d92779&spm=a230r.1.14.13#detail">MPU6050</a> 三轴加速度+三轴陀螺仪 I2C接口 25￥</td></tr><tr><td>WiFi</td><td></td><td><a href="https://detail.tmall.com/item.htm?id=609757779633&ali_refid=a3_430582_1006:1267360122:N:9/mfWI1BJMLzXLT4ATlUnA==:de4e276b258975c722c4a03cf64e8c17&ali_trackid=1_de4e276b258975c722c4a03cf64e8c17&spm=a230r.1.14.8">ATK-ESP8266</a> 串口转WIFI模块 28￥</td></tr><tr><td>Battery</td><td>内置110mAh 锂电池</td><td><a href="https://item.taobao.com/item.htm?spm=a230r.1.14.137.5c3d9640Qtfoh2&id=546584044959&ns=1&abbucket=18#detail">5V锂电池</a> 20￥</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;单片机解决方案调研&quot;&gt;&lt;a href=&quot;#单片机解决方案调研&quot; class=&quot;headerlink&quot; title=&quot;单片机解决方案调研&quot;&gt;&lt;/a&gt;单片机解决方案调研&lt;/h1&gt;&lt;p&gt;目标：&lt;strong&gt;模块化强；底层开发难度低、成本低；低功耗（电池供电）；支持无线
      
    
    </summary>
    
    
      <category term="Works" scheme="http://canVa4.github.io/categories/Works/"/>
    
    
      <category term="单片机" scheme="http://canVa4.github.io/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="arduino" scheme="http://canVa4.github.io/tags/arduino/"/>
    
      <category term="STM32" scheme="http://canVa4.github.io/tags/STM32/"/>
    
  </entry>
  
  <entry>
    <title>Python学习杂记(1)</title>
    <link href="http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/"/>
    <id>http://canva4.github.io/2020/08/08/Python%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0-1/</id>
    <published>2020-08-08T13:09:52.000Z</published>
    <updated>2021-01-31T12:24:01.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python学习杂记-1"><a href="#Python学习杂记-1" class="headerlink" title="Python学习杂记(1)"></a>Python学习杂记(1)</h1><p>本篇文章是为了记录进一步学习python时遇到的一些问题和相对陌生的知识点。</p><ul><li><p>range()函数返回的是一个独特的“范围类”对象！</p></li><li><p>for 变量 in 字符串|集合|范围|任何可迭代对象:</p><p> <strong>可迭代对象：指该对象中包含一个–iter–方法</strong></p></li><li><p><strong>isinstance()函数</strong>，判断某个变量是否为指定类型的实例，前一个参数是要判断的变量，后一个参数是类型。如：isinstance(2, int)</p></li><li><p>zip()函数，将多个列表压缩为一个zip对象（可迭代对象），这样就可以使用一个循环遍历两个列表</p></li><li><p>reversed()函数：接受序列（元组、列表、区间等），然后返回一个“反序排列”的迭代器，sorted()类似</p></li><li></li></ul><h1 id="Chapter-5-函数和Lambda表达式"><a href="#Chapter-5-函数和Lambda表达式" class="headerlink" title="Chapter 5 函数和Lambda表达式"></a>Chapter 5 函数和Lambda表达式</h1><ol><li>函数的参数收集（个数可变的参数），可以在调用函数时传入任意多个参数） P105</li></ol><p>在形参前面添加一个*，多个参数当做元组传入。一般都放在形参的末尾。如果放在前面，需要将后面的参数均使用<strong>关键字参数的</strong>方式输入。此种方法输入的参数被看为<strong>元组</strong></p><p>在形参前面添加**，多个参数当做<strong>字典</strong>传入。</p><ol start="2"><li>逆向参数收集 P106</li></ol><p>指的是程序在已有列表、元组、字典等对象的前提下，把他们的元素“拆开后”传给函数的参数。拆开元组使用*，拆开字典使用**。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def test(name, *num):</span><br><span class="line">    print(&#39;user:&#39;, name)</span><br><span class="line">    print(&#39;say:&#39;, num)</span><br><span class="line">mytuple &#x3D; (1,2,3)</span><br><span class="line"></span><br><span class="line">test(&#39;canVa4&#39;,mytuple)</span><br><span class="line"></span><br><span class="line">res&gt;&gt;&gt;&gt;</span><br><span class="line">user:canVa4</span><br><span class="line">say: (1,2,3)</span><br></pre></td></tr></table></figure><ol start="3"><li>locals()和globals()函数 P111</li></ol><p>可以globals()方法在函数内部访问全局变量（同名的被hide的全局变量）</p><ol start="4"><li>global关键字</li></ol><p>可以global关键字在函数内部声明已经定义好的全局变量</p><ol start="5"><li>局部函数&amp;nonlocal关键字 P114</li></ol><p>在函数体内定义的函数。默认对外部是隐藏的</p><ol start="6"><li>函数变量 与 将函数作为函数的参数 P115</li><li>将函数作为返回值 P116</li></ol><h1 id="Chapter-6-类和对象"><a href="#Chapter-6-类和对象" class="headerlink" title="Chapter 6 类和对象"></a>Chapter 6 类和对象</h1><ol><li>对象的动态性   P123</li></ol><p>对对象（实例化后）的可以增加实例变量，直接赋值即可；同时也可以用del函数，删除对象（实例化后）的在构造函数__init__中定义的函数）。</p><p>NOTE：也可以定义动态实例方法，但是不会将自身自动传入self参数（第一个参数），再调用时需手动输入。若想可以自动填入，需要MethodType()函数</p><ol start="2"><li>类也可以调用实例方法</li></ol><p>但！！此时不会自动填入第一个self参数（第一个自动填入调用者本身，而类没有实例，所以也没法自动填入）。所以在使用类调用实例方法时，需要手动填入第一个self参数。</p><ol start="3"><li>类方法与静态方法（主要使用类来直接调用） P128</li></ol><p>使用@classmethod修饰的是类方法，使用@staticmethod修饰的是静态方法</p><ol start="4"><li>@函数修饰器 P128 一个更好的解释说明 <a href="https://www.runoob.com/w3cnote/python-func-decorators.html">LINK</a></li><li>类变量和实例变量</li></ol><p>类变量是直接在类里面定义的变量（注意在类里面函数中定义的变量不是类变量！！！）</p><ol start="6"><li>property函数与property修饰器  P135</li><li>类的封装和隐藏  P137  python本质并没有隐藏的功能！！！ 可以在方法和变量前面增加__（两个下划线）起隐藏的效果（但仍可以直接访问到）</li><li>用super()可以直接调用父类方法，或者使用父类类名来调用父类方法 <a href="https://www.runoob.com/python/python-func-super.html">LINK</a></li><li>python的动态特性 P143 即：类可以动态的增加：方法、属性和变量等。</li><li>使用__slot__()可以限制动态的添加，slot更多细节  P144</li><li>python还可以动态的创建类：使用type()函数</li><li>mateclass 更好的教程：LINK[<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072]">https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072]</a></li></ol><p>metaclass，直译为元类，简单的解释就是：当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。</p><p>但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。</p><p>连接起来就是：先定义metaclass，就可以创建类，最后创建实例。所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。</p><p><strong>NOTE: 基本用不到</strong></p><ol start="13"><li>可以使用issubclass()和isinstance()来判断是不是某个类的子类 或 一个实例是否为某个类的实例</li><li>每个类都有一个__bases__属性（用来给出其父类）和一个__subclasses__()方法用来给出其直接子类</li><li><strong>枚举类</strong>  P150</li></ol><p>指一个类的对象（实例）是有限且固定的  更好的教程<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017595944503424">LINK</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python学习杂记-1&quot;&gt;&lt;a href=&quot;#Python学习杂记-1&quot; class=&quot;headerlink&quot; title=&quot;Python学习杂记(1)&quot;&gt;&lt;/a&gt;Python学习杂记(1)&lt;/h1&gt;&lt;p&gt;本篇文章是为了记录进一步学习python时遇到的一些问题
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment1 实现时遇到的问题</title>
    <link href="http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://canva4.github.io/2020/08/07/CS231n-Assignment1-%E5%AE%9E%E7%8E%B0%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-07T08:21:14.000Z</published>
    <updated>2020-08-08T13:01:59.970Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n-Assignment1-遇到的问题"><a href="#CS231n-Assignment1-遇到的问题" class="headerlink" title="CS231n Assignment1 遇到的问题"></a>CS231n Assignment1 遇到的问题</h1><ul><li>实现基于2019年版的课程</li><li>主要记录遇到的问题</li></ul><h2 id="Softmax-implement"><a href="#Softmax-implement" class="headerlink" title="Softmax implement"></a>Softmax implement</h2><p>不论是实现softmax，SVM损失函数，二者遇到的问题都比较相似，主要为<strong>导数的推导</strong>和<strong>numpy的使用</strong>。由于softmax的实现稍微复杂一些，这里只记录softmax实现时的问题。</p><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>使用SGD核心的工作就是计算softmax关于权值W的梯度。课程中没有给出推导过程，这里推导一下。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/mg9rJC2LzVaAxO3.png" alt="image-20200807173340447"></p><h3 id="Numeric-Stability-Trick"><a href="#Numeric-Stability-Trick" class="headerlink" title="Numeric Stability Trick"></a>Numeric Stability Trick</h3><p>为了防止出现数值计算不稳定，要在计算损失函数式加入修正项（对Gradient无影响）。</p><p>原始为：<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/w469bMc7skBYd52.png" alt="image-20200807174209667" style="zoom:50%;" /></p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/t7zyhReGVp6QEi2.png" alt="image-20200807174255380" style="zoom: 80%;" /><h3 id="Implement-with-numpy"><a href="#Implement-with-numpy" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><h4 id="Navie-Version"><a href="#Navie-Version" class="headerlink" title="Navie Version"></a>Navie Version</h4><p>给出naive版本的代码。如何计算的示意图已在推导过程中给出。naive版本的代码基本按照推导的公式梳理下来即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)    <span class="comment"># 一个数值修正的技巧，防止出现数值不稳定的问题</span></span><br><span class="line">        scores = np.exp(scores)</span><br><span class="line"></span><br><span class="line">        sum_scores = np.sum(scores)        <span class="comment"># 可以简化写法，节省空间，懒得修改了</span></span><br><span class="line">        P = scores / sum_scores</span><br><span class="line">        L = -np.log(P)</span><br><span class="line"></span><br><span class="line">        loss += L[y[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):    <span class="comment"># 计算梯度，分类讨论</span></span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:, j] += (<span class="number">-1</span> + P[y[i]])*X[i].T</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dW[:, j] += P[j]*X[i].T</span><br><span class="line"></span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += reg * W</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h4 id="Vectorized-Version"><a href="#Vectorized-Version" class="headerlink" title="Vectorized Version"></a>Vectorized Version</h4><p>向量化版本。这里就有非常多的细节需要注意了。首先还是给出完整代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    scores = X @ W  <span class="comment"># ( N*C )</span></span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss += np.sum(L[np.arange(num_train), y])</span><br><span class="line"></span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)   <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算gradient</span></span><br><span class="line">    mid = np.zeros_like(P)  <span class="comment"># 生成一个和P一样的0矩阵</span></span><br><span class="line">    mid[np.arange(num_train), y] = <span class="number">1</span>  <span class="comment"># 对矩阵中Y所对应的部分加一个1，因为一会要构造出需要的梯度计算</span></span><br><span class="line">    dW = X.T @ (-mid + P)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><p>首先应该画图明白计算中各个量的关系，以及他们是怎么来的，这个很重要。如下图所示</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/Knh24oeJ53Nydqj.png" alt="image-20200807175314544"></p><p>第一处就是在计算Numeric Stability Trick时，要找到每一个输入向量的最大元素，这里注意需要保证keepdims=True。</p><p>其控制了返回数组的shape，这样返回的shape为(N,1)。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>同理在sum时，也要进行类似的处理，这样在归一化时才能work。</p><h4 id="An-Important-Trick"><a href="#An-Important-Trick" class="headerlink" title="An Important Trick!!!"></a>An Important Trick!!!</h4><p>在这里我遇到了不少的问题，主要是numpy使用的不熟练。。。:( 所以特此记录下来。</p><p><strong>L[np.arange(num_train), y] **与 **L[:,y]</strong> 的区别！</p><p>一开始的代码使用的是后者，因为目标就是获得所有行i中，列位置为y[i]的元素。所以想当然的使用了后者。但实际上，后者返回的是所有行x[i]中，x[i,y[j]]的元素！！！</p><p>示例如下：</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/dKFBsGk3qzxbV5c.png" alt="image-20200807180256834"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/QbpEhW9DK63ex1F.png" alt="image-20200807180318182"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/2meKSxGVvcsb3zA.png" alt="image-20200807180331281"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/jJGhMKFWD7iTULy.png" alt="image-20200807180431683"></p><p>而**L[np.arange(num_train), y] **则为：</p><p>如果将np.arange(num_train)看为list，则其长度必须与y相同！！！其效果就是二者分别迭代，每次返回二者迭代结果下标位置处的元素。如图所示。</p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/d7nQCOvX8Goue9z.png" alt="image-20200807180731248"></p><p>所以可见，基于我们的需要，后者才能满足要求。</p><h2 id="Two-Layer-Neural-Network"><a href="#Two-Layer-Neural-Network" class="headerlink" title="Two-Layer Neural Network"></a>Two-Layer Neural Network</h2><p>本部分的工作也与之前的部分比较相似，这里遇到主要问题还是如何处理求导的问题。</p><p>由于在这里我也遇到了一些问题，所以再次给出部分求导流程。</p><p>首先先给出网络的结构。</p><h3 id="Gradient-1"><a href="#Gradient-1" class="headerlink" title="Gradient"></a>Gradient</h3><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/8yBxLVldSgqsrmZ.png" alt="image-20200807221946519"></p><p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/08/07/pb8PvAu7RjhNJHT.png" alt="image-20200807223402245"></p><h3 id="Implement-with-numpy-1"><a href="#Implement-with-numpy-1" class="headerlink" title="Implement with numpy"></a>Implement with numpy</h3><p>下面给出代码实现。由于主要难点就是loss的实现了，之后的SGD和predict函数都非常简单，我没有遇到什么问题，这里只给出遇到了部分问题的loss与grad计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=None, reg=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">    W1, b1 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2, b2 = self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">    N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    h = np.maximum(X @ W1 + b1, <span class="number">0</span>)</span><br><span class="line">    scores = h @ W2 + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we&#x27;re done</span></span><br><span class="line">    <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br><span class="line">    scores = np.exp(scores)</span><br><span class="line">    sum_scores = np.sum(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    P = scores / sum_scores</span><br><span class="line">    L = -np.log(P)</span><br><span class="line">    loss = np.sum(L[np.arange(N), y])</span><br><span class="line"></span><br><span class="line">    loss /= N</span><br><span class="line">    loss += <span class="number">1</span> * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    <span class="comment"># 计算W2，b2</span></span><br><span class="line">    dscore = P</span><br><span class="line">    dscore[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">    dscore /= N        <span class="comment"># 这里需要注意！！！</span></span><br><span class="line">    <span class="comment"># 计算梯度时只需要除一次N，这里debug花了很久。。</span></span><br><span class="line">    grads[<span class="string">&#x27;W2&#x27;</span>] = h.T @ dscore + <span class="number">2</span> * reg * W2</span><br><span class="line">    grads[<span class="string">&#x27;b2&#x27;</span>] = np.sum(dscore, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 计算W1，b1</span></span><br><span class="line">    dh = dscore @ W2.T      <span class="comment"># 目标函数对于h的偏导</span></span><br><span class="line">    dh[h &lt;= <span class="number">0</span>] = <span class="number">0</span>          <span class="comment"># 此时dh变为关于w1@x+b1的导数</span></span><br><span class="line">    grads[<span class="string">&#x27;W1&#x27;</span>] = X.T @ dh + <span class="number">2</span> * reg * W1</span><br><span class="line">    grads[<span class="string">&#x27;b1&#x27;</span>] = np.sum(dh, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><p>基本上按照公式并注意矩阵维数和细节就OK了，遇到不太会的画个图就解决了。</p><div class="note warning">            <p>需要注意的是，在除以输入个数的时候，只需要除一次</p>          </div><p>这里一开始没有注意到，我一开始在每次计算梯度的时候都除了N，导致出现了误差，这里居然debug了很久。。</p><h3 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h3><p>有点懒，这部分工作没有完成。</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>其余的部分（k-Nearest Neighbor classifier, SVM, Higher Level Representations: Image Features）并未遇到很多的问题。具体详情代码见我的github仓库。<a href="https://github.com/canVa4/CS231n-Assignments">https://github.com/canVa4/CS231n-Assignments</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS231n-Assignment1-遇到的问题&quot;&gt;&lt;a href=&quot;#CS231n-Assignment1-遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;CS231n Assignment1 遇到的问题&quot;&gt;&lt;/a&gt;CS231n Assignm
      
    
    </summary>
    
    
      <category term="Notes" scheme="http://canVa4.github.io/categories/Notes/"/>
    
    
      <category term="CS231n" scheme="http://canVa4.github.io/tags/CS231n/"/>
    
      <category term="python" scheme="http://canVa4.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://canVa4.github.io/tags/numpy/"/>
    
  </entry>
  
</feed>
