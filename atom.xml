<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-06T17:34:33.055Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>阿翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SimCLR论文复现</title>
    <link href="http://yoursite.com/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    <id>http://yoursite.com/2020/08/06/SimCLR%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</id>
    <published>2020-08-05T16:31:51.000Z</published>
    <updated>2020-08-06T17:34:33.055Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p>整体的代码使用pytorch实现，基于<a href="https://github.com/sthalles/SimCLR">https://github.com/sthalles/SimCLR</a> （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）</p><a id="more"></a><p>训练为：<em>run.py</em>；修改训练参数、Loss、数据集等需要修改：<em>config.yaml</em> ；评估使用<em>evluation.py</em>（测试训练分开的原因是因为我租了GPU，用GPU训练，用我的PC测试，这样可以更快一些）</p><p>个人运行环境：win10 + pytorch 1.5 + cuda 10.2（租的GPU 1080ti）</p><table><thead><tr><th>日期</th><th>进度</th></tr></thead><tbody><tr><td>5-19 Tue（基本满课+实验）</td><td>论文阅读，选定使用pytorch实现和决定基于上文链接实现代码</td></tr><tr><td>5-20 Wed</td><td>熟悉基础知识、了解代码整体框架，理解loss function，并进行初步尝试编写loss，未调试</td></tr><tr><td>5-21 Thu（满课+实验）</td><td>写完了evaluation部分</td></tr><tr><td>5-22 Fri（基本满课）</td><td>跑代码，发现只用CPU究极龟速；于是装cuda，结果装了一白天的cuda T.T，晚上测试代码并初步验证loss function是否书写正确；初步移植LARS</td></tr><tr><td>5-23 Sat</td><td>测试三个Loss并尝试调参，尝试使用resnet18作为backbone网络，旁晚开始租了个GPU来跑模型，实现triplet loss(sh)</td></tr><tr><td>5-24 Sun</td><td>调参、修复bug、跑代码、微调loss（Logistic loss增加欠采样和改变权重）</td></tr><tr><td>5-25 Mon</td><td>调参、跑代码</td></tr></tbody></table><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Linear evaluation均使用Logistic Regression，均train from scratch（no pretrain）</p><p>GPU: 1080ti    resnet50训练+测试一次需5.5h；resnet18训练+测试一次需2.6h；总代码运行时间：约75h（包括未列出测试）</p><table><thead><tr><th>batch</th><th>epoch</th><th>out dim</th><th>optimizer</th><th>Loss</th><th>BackBone</th><th>t/m</th><th>CIARF10 Top-1</th></tr></thead><tbody><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>0.1</td><td>78.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>79.3%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Xent</td><td>resnet50</td><td>1</td><td>77.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.4</td><td>65.1%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss</td><td>resnet50</td><td>0.8</td><td>70.7%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>Triplet Loss(sh)</td><td>resnet50</td><td>0.8</td><td>73.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(none)</td><td>resnet50</td><td>0.2</td><td>37.5%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.2</td><td>62.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>0.5</td><td>69.9%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic (sampling)</td><td>resnet50</td><td>1</td><td>65.2%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>LARS</td><td>NT-xent</td><td>resnet50</td><td>0.5</td><td>TODO</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-xent</td><td>resnet18</td><td>0.5</td><td>71.4%</td></tr><tr><td>128</td><td>80</td><td>128</td><td>ADAM</td><td>NT-Logistic(weight)</td><td>resnet18</td><td>0.2</td><td>66.5%</td></tr></tbody></table><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于每一个输入图片，模型会生成两个representation，最终优化的目标可以理解为：同一个batch内来自同一张图片的两个representation的距离近，让来自不同输入图片的representation的距离远。注意，论文中给出的是negative loss function</p><h3 id="Logistic-Loss"><a href="#Logistic-Loss" class="headerlink" title="Logistic Loss"></a>Logistic Loss</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ log \sigma(u^Tv^+/\tau) + log\sigma(-u^T v^-/ \tau) $$</li></ul><p>这里对于此公式，我一开始是没有理解的，于是自己尝试推理了一下。</p><p>对于每一个输入样本，模型会生成两个representation，对于一个有N个输入的batch的，就会产生2*N个representation，对于每一对representation计算一个cosine similarity。而每一对representation（下文用 $(i,j)$ 序偶来表示他们）可以根据他们的来源来确定他们label（即：来自同一输入的为正类，来自不同输入的为反类），这样就构成了一个监督任务。</p><p>将这个任务看为监督后，因为论文中提到的这个损失函数的名字是logistic loss，我自然地想到了logistics regression。于是从这个角度入手，来推理这个loss function。</p><p>用$ P(i,j) $表示一对representation为正类的概率。设正类y=1，反类y=0</p><p>那么写出整个数据集的对数似然函数$$ LL(\theta;X)=\sum_{each(i,j)} (y_{(i,j)} logP(i,j)+(1-y_{(i,j)})log(1-P(i,j)) )$$</p><p>对上式化简可以得到：$$ LL(\theta;X)=\sum_{正类} logP(i,j)+\sum_{反类}log(1-P(i,j)) $$</p><p>而cosine similarity并不是一个[0,1]之间的数（或者说没有概率的意义），参照logistics regression，将cosine similarity经过一个sigmoid函数$$ \sigma( \cdot) $$ 之后就变为了一个[0,1之间的数]，而且对于sigmoid有$$ \sigma(-x)=1-\sigma(x) $$,所以有：$$ LL(\theta;X)=\sum_{正类} log[\sigma(sim(i,j))]+\sum_{反类}log[\sigma(-sim(i,j))] , sim(i,j)为(i,j)的相似度指标$$</p><p>只需引入temperature就可将上式变为与论文中公式相同的形式。</p><p>在使用原版loss时，发现最终结果效果很差（见result中的NT-Logistics none）。个人猜测原因如下：</p><ul><li>样本非常不均衡，正例对远远少于反例。</li></ul><p>解决办法：</p><ul><li>对反例样本对使用简单的<em>under-sampling</em>（欠采样）</li><li>对于loss计算时，正反例样本<em>设置不同的权重</em>（效果更好，因为欠采样会丢失部分信息）</li></ul><p>（注：由于训练时间太久，没有来得多次跑weight测试效果）</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line">    representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line">    l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line">    r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line">    positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line">    negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>) * <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    logits_pos = self.sigmoid(positives / self.temperature).log_()</span><br><span class="line">    logits_neg = self.sigmoid(negatives / self.temperature).log_()</span><br><span class="line">    <span class="keyword">if</span> self.method == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># under-sampling</span></span><br><span class="line">        all_one_vec = np.ones((<span class="number">1</span>, <span class="number">2</span> * self.batch_size,))</span><br><span class="line">        all_zero_vec = np.zeros((<span class="number">1</span>, <span class="number">2</span> * self.batch_size * (<span class="number">2</span> * self.batch_size - <span class="number">3</span>)))</span><br><span class="line">        under_sampling_matrix = np.column_stack((all_one_vec, all_zero_vec)).flatten()</span><br><span class="line">        np.random.shuffle(under_sampling_matrix)</span><br><span class="line">        under_sampling_matrix = torch.tensor(under_sampling_matrix).view(</span><br><span class="line">            (<span class="number">2</span> * self.batch_size, <span class="number">2</span> * self.batch_size - <span class="number">2</span>)).type(torch.bool).to(self.device)</span><br><span class="line"></span><br><span class="line">        logits_neg = logits_neg[under_sampling_matrix]</span><br><span class="line">        loss = torch.sum(logits_pos) + torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    <span class="keyword">elif</span> self.method == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># change weight</span></span><br><span class="line">        neg_count = <span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>)</span><br><span class="line">        pos_count = <span class="number">2</span>*self.batch_size</span><br><span class="line">        loss = neg_count * torch.sum(logits_pos) + pos_count*torch.sum(logits_neg)</span><br><span class="line">        <span class="keyword">return</span> -loss/(pos_count+neg_count)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># none</span></span><br><span class="line">        total_logits = torch.cat((logits_pos, logits_neg), dim=<span class="number">1</span>)</span><br><span class="line">        loss = torch.sum(total_logits)</span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Margin-Triplet"><a href="#Margin-Triplet" class="headerlink" title="Margin Triplet"></a>Margin Triplet</h3><p>首先给出论文中的形式（negative loss function）：</p><ul><li>$$ -max(u^Tv^–u^Tv^+m,0)$$</li></ul><p>此公式理解起来相对直观，即对于一个输入样本，计算其和一个负样本相似度减去和正样本的相似度在加上m，并与0取max。该m可以理解：m越大为希望正反样本分开的距离越大。其目标是希望输入样本和正样本的相似度减去和负样本的相似度可以大于阈值m值。下图很形象的描述了这些关系。</p><img src= "/img/loading.gif" data-lazy-src="https://wx1.sbimg.cn/2020/08/07/oBAhT.png" style="zoom:67%;" /><p>所以，对于每一个输入样本k，该样本的<strong>margin tripl loss</strong>为$$ \sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^+m,0) $$</p><p>所以总的loss就是将所有输入样本的loss加起来</p><ul><li><p>$$ \frac{1}{2N*(2N-2)}\sum_{k}^{所有样本}\sum_{i}^{所有反类}max(u_k^Tv_i^–u_k^Tv^++m,0) $$</p></li><li><p>同时也实现了semi-hard negative mining. 即计算loss（梯度）时，只考虑上图中semi-hard negatives的loss。即选择满足：$$ u^Tv^++m&gt;u^Tv^-$$</p></li></ul><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, zis, zjs</span>):</span></span><br><span class="line"> representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"> similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"> <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line"> l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line"> r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line"> positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"> mid = similarity_matrix[self.mask_samples_from_same_repr]</span><br><span class="line"> negatives = mid.view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</span><br><span class="line"> zero = torch.zeros(<span class="number">1</span>).to(self.device)</span><br><span class="line"> triplet_matrix = torch.max(zero, negatives - positives + self.m_param)</span><br><span class="line"> <span class="comment"># max( sim(neg) - sim(pos) + m, 0)</span></span><br><span class="line"> <span class="comment"># 2N,2N-2 每一行代表了对于一个z关于其正类（z+batch）和其他反类的triplet loss</span></span><br><span class="line"> <span class="keyword">if</span> self.semi_hard == <span class="literal">True</span>:</span><br><span class="line">     <span class="comment"># semi-hard</span></span><br><span class="line">     semi_hard = - negatives + positives + self.m_param</span><br><span class="line">     <span class="comment"># print(semi_hard)</span></span><br><span class="line">     semi_hard_mask = torch.max(semi_hard, zero).type(torch.bool)</span><br><span class="line">     <span class="comment"># print(semi_hard_mask)</span></span><br><span class="line">     triplet_matrix_sh = triplet_matrix[semi_hard_mask]</span><br><span class="line">     <span class="comment"># print(triplet_matrix)</span></span><br><span class="line">     <span class="comment"># print(triplet_matrix_sh)</span></span><br><span class="line">     loss = torch.sum(triplet_matrix_sh)</span><br><span class="line">     <span class="keyword">return</span> loss</span><br><span class="line"> <span class="keyword">else</span>:    <span class="comment"># normal</span></span><br><span class="line">     loss = torch.sum(triplet_matrix)     </span><br><span class="line">     <span class="keyword">return</span> loss / (<span class="number">2</span>*self.batch_size*(<span class="number">2</span>*self.batch_size - <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="NT-Xent"><a href="#NT-Xent" class="headerlink" title="NT-Xent"></a>NT-Xent</h3><p>论文中的形式：</p><ul><li>$$ l(i,j)=-log \frac{exp(s_{i,j}/\tau)}{\sum^{2N}<em>{k=1} 1</em>{k\not=i}exp(s_{i,j}/\tau)}$$, $$ L = \frac{1}{2N} \sum^{N}_{k=1}[l(2k-1,2k)+l(2k,2k-1)]$$</li></ul><p>代码实现未进行修改。</p><h2 id="simCLR模型"><a href="#simCLR模型" class="headerlink" title="simCLR模型"></a>simCLR模型</h2><p>主要使用ResNet-50来实现，参照论文B.9中所写：将Resnet第一个卷积层改为了3*3的Conv，stride=1，并去除第一个max pooling层；在augmentation中去除了Guassian Blur。</p><p>projection head同论文中一样，使用两层的MLP。</p><h2 id="遇到的问题与解决方法"><a href="#遇到的问题与解决方法" class="headerlink" title="遇到的问题与解决方法"></a>遇到的问题与解决方法</h2><p>Q1：使用个人笔记本训练，显存不足，使用cpu训练耗时过久。</p><p>A1：尝试使用过resnet18，仍时间仍很长，最终决定租GPU（1080ti）来训练。</p><p>Q2：训练时发现最终测试结果不好。</p><p>A2：最终控制变量，与未修改的代码对比测试，发现个人版本在sampler的时候不小心去掉了很多的训练样例，已修复为同原版。修复后，基本同原版效果</p><p>Q3：使用LARS效果不好，loss不能稳定下降，震荡严重。（unsolve）</p><p>A3：尝试修改debug，修改学习率，由于时间不足，暂未解决。</p><h2 id="关于Loss的个人想法"><a href="#关于Loss的个人想法" class="headerlink" title="关于Loss的个人想法"></a>关于Loss的个人想法</h2><p>从测试结果和论文结果可以看出，NT-xent的效果更佳。个人认为其主要的优势在于：</p><ul><li>NT-xent（cross entropy）利用的是相对相似度，而其余二者不是。这样可以缓解个别样本差异过大导致的不均衡（感觉类似于input的normalization）。</li><li>NT-xent计算了所有positive pair的loss。而NT-logistic和Margin Triplet则使用全部的pair来计算，不使用semi-hard mining的话，可能会造成坍塌。对于此模型生成的样本，可以看到其样本类别并不均衡，对于NT-logistic，这可能会导致训练效果下降。（使用semi-hard negative mining、采样、改变权重可以缓解这个问题）</li></ul><p>经过自己的implement之后，实在是羡慕google的TPU集群了！</p><p>这是我第一次真正接触self-supervised learning，之前只是有所耳闻，感觉这种contrastive learning的想法真的很有趣。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在开头&quot;&gt;&lt;a href=&quot;#写在开头&quot; class=&quot;headerlink&quot; title=&quot;写在开头&quot;&gt;&lt;/a&gt;写在开头&lt;/h2&gt;&lt;p&gt;整体的代码使用pytorch实现，基于&lt;a href=&quot;https://github.com/sthalles/SimCLR&quot;&gt;https://github.com/sthalles/SimCLR&lt;/a&gt; （用pytorch实现simCLR中star最多的）实现了Logistic Loss（支持使用欠采样、改变权重和无操作）和margin triplet loss（支持semi-hard mining），并可选LARS（experimental）和ADAM优化。代码框架支持resnet50和resnet18；dataset支持STL10和CIARF10（测试时使用CIARF10）&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文复现" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>First test Blog</title>
    <link href="http://yoursite.com/2020/08/05/First-test-Blog/"/>
    <id>http://yoursite.com/2020/08/05/First-test-Blog/</id>
    <published>2020-08-05T15:54:29.000Z</published>
    <updated>2020-08-05T15:57:36.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一个测试博客！！！"><a href="#第一个测试博客！！！" class="headerlink" title="第一个测试博客！！！"></a>第一个测试博客！！！</h1><p>语无伦次语无伦次语无伦次</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第一个测试博客！！！&quot;&gt;&lt;a href=&quot;#第一个测试博客！！！&quot; class=&quot;headerlink&quot; title=&quot;第一个测试博客！！！&quot;&gt;&lt;/a&gt;第一个测试博客！！！&lt;/h1&gt;&lt;p&gt;语无伦次语无伦次语无伦次&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2020/08/05/hello-world/"/>
    <id>http://yoursite.com/2020/08/05/hello-world/</id>
    <published>2020-08-05T15:41:38.862Z</published>
    <updated>2020-08-05T15:41:38.862Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
