<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Model Compression overview and resources | Xiang's Blog</title><meta name="description" content="Model Compression Overview and Resources此NOTE主要记录一些关于model compression 方面的overview和一些不错的入门资源、survey and papers.  Model pruning(模型剪枝): removes less important parameters Weight Quantization: uses fewer"><meta name="keywords" content="Model Compression"><meta name="author" content="阿翔"><meta name="copyright" content="阿翔"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/assets/Radiohead.jpg"><link rel="canonical" href="http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Model Compression overview and resources"><meta property="og:url" content="http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/"><meta property="og:site_name" content="Xiang's Blog"><meta property="og:description" content="Model Compression Overview and Resources此NOTE主要记录一些关于model compression 方面的overview和一些不错的入门资源、survey and papers.  Model pruning(模型剪枝): removes less important parameters Weight Quantization: uses fewer"><meta property="og:image" content="https://i.loli.net/2020/11/19/7vzxb8FC2GrXQpN.png"><meta property="article:published_time" content="2020-11-19T12:47:47.000Z"><meta property="article:modified_time" content="2020-11-30T08:05:33.688Z"><meta name="twitter:card" content="summary"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"limitCount":50,"languages":{"author":"作者: 阿翔","link":"链接: ","source":"来源: Xiang's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-11-30 16:05:33'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 5.0.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="alternate" href="/atom.xml" title="Xiang's Blog" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://i.loli.net/2020/09/06/hCrie9pkUMQRzsg.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">18</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">22</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-Compression-Overview-and-Resources"><span class="toc-number">1.</span> <span class="toc-text">Model Compression Overview and Resources</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Videos"><span class="toc-number">1.1.</span> <span class="toc-text">Videos</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Slides"><span class="toc-number">1.2.</span> <span class="toc-text">Slides</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Blogs"><span class="toc-number">1.3.</span> <span class="toc-text">Blogs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Papers-and-Surveys"><span class="toc-number">1.4.</span> <span class="toc-text">Papers and Surveys</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Papers"><span class="toc-number">1.4.1.</span> <span class="toc-text">Papers:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Surveys"><span class="toc-number">1.4.2.</span> <span class="toc-text">Surveys:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Overview"><span class="toc-number">2.</span> <span class="toc-text">Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Network-Pruning"><span class="toc-number">2.1.</span> <span class="toc-text">Network Pruning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-Pruning"><span class="toc-number">2.1.1.</span> <span class="toc-text">Why Pruning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-important-How-to-evaluate-importance"><span class="toc-number">2.1.2.</span> <span class="toc-text">What is important? How to evaluate importance?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluate-Importance"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Evaluate Importance</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Some-theory-about-Network-Pruning"><span class="toc-number">2.1.3.</span> <span class="toc-text">Some theory about Network Pruning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Lottery-Ticket-Hypothesis"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">Lottery Ticket Hypothesis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rethinking-the-Value-of-Network-Pruning"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">Rethinking the Value of Network Pruning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rethinking-VS-Lottery-Ticket"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">Rethinking VS Lottery Ticket</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Knowledge-Distillation"><span class="toc-number">2.2.</span> <span class="toc-text">Knowledge Distillation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Power-of-Soften-Label"><span class="toc-number">2.2.1.</span> <span class="toc-text">The Power of Soften Label</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logits-Distillation"><span class="toc-number">2.2.2.</span> <span class="toc-text">Logits Distillation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Distillation"><span class="toc-number">2.2.3.</span> <span class="toc-text">Feature Distillation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relational-Distillation-Learn-from-Batch"><span class="toc-number">2.2.4.</span> <span class="toc-text">Relational Distillation: Learn from Batch</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Parameter-Quantization"><span class="toc-number">2.3.</span> <span class="toc-text">Parameter Quantization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architecture-Design"><span class="toc-number">2.4.</span> <span class="toc-text">Architecture Design</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dynamic-Computation"><span class="toc-number">2.5.</span> <span class="toc-text">Dynamic Computation</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/11/19/7vzxb8FC2GrXQpN.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Xiang's Blog</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Model Compression overview and resources</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-19T12:47:47.000Z" title="发表于 2020-11-19 20:47:47">2020-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-11-30T08:05:33.688Z" title="更新于 2020-11-30 16:05:33">2020-11-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Works/">Works</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Model-Compression-Overview-and-Resources"><a href="#Model-Compression-Overview-and-Resources" class="headerlink" title="Model Compression Overview and Resources"></a>Model Compression Overview and Resources</h1><p>此NOTE主要记录一些关于model compression 方面的overview和一些不错的入门资源、survey and papers.</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><ul>
<li><strong>Model pruning(模型剪枝)</strong>: removes less important parameters</li>
<li><strong>Weight Quantization</strong>: uses fewer bits to represent the parameters</li>
<li><strong>Parameter sharing</strong></li>
<li><strong>Knowledge distillation(知识蒸馏)</strong>: trains a smaller student model that learns from intermediate outputs from the original model.</li>
<li><strong>Module replacing / Dynamic Computation</strong>: Can network adjust the computation power it need?</li>
</ul>
<h2 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1tE411F7aC?from=search&seid=16988409402694511485">知识蒸馏简述–起源、改进与研究现状【截至2020年3月22日】</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17z411B7Hk?from=search&seid=4245228983927115130">模型压缩Network Compression方法补充</a>    NOTE: 1的延续</li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1SC4y1h7HB?from=search&seid=16988409402694511485">网络压缩和知识蒸馏-李宏毅</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Rt4y1m7Fm?from=search&seid=4245228983927115130">【深度学习的模型压缩与加速】台湾交通大学 張添烜教授</a></li>
</ol>
<h2 id="Slides"><a href="#Slides" class="headerlink" title="Slides"></a>Slides</h2><p>Overview: <a target="_blank" rel="noopener" href="https://slides.com/arvinliu/model-compression">https://slides.com/arvinliu/model-compression</a></p>
<p>Deep Mutual Learning <a target="_blank" rel="noopener" href="https://slides.com/arvinliu/kd_mutual">https://slides.com/arvinliu/kd_mutual</a></p>
<h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><ol>
<li>深度学习模型压缩与加速综述 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67871864">LINK</a></li>
<li>知识蒸馏是什么？一份入门随笔 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/90049906">LINK</a></li>
<li>知识蒸馏（Knowledge Distillation）简述（一）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81467832">LINK</a></li>
<li>Mutual Mean-Teaching：为无监督学习提供更鲁棒的伪标签 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/116074945">LINK</a></li>
</ol>
<h2 id="Papers-and-Surveys"><a href="#Papers-and-Surveys" class="headerlink" title="Papers and Surveys"></a>Papers and Surveys</h2><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h3><ol>
<li>Knowledge Distillation 2015 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.02531.pdf">LINK</a> 知识蒸馏开山之作</li>
<li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.03635">LINK</a></li>
<li>Rethinking the Value of Network Pruning 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05270">LINK</a></li>
<li>BinaryConnect: Training Deep Neural Networks with binary weights during propagations 2015 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.00363">LINK</a></li>
<li>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 2016 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.02830">LINK</a></li>
<li>XNOR-NET 2016 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.05279">LINK</a></li>
<li>MobileNets 2017 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04861">LINK</a></li>
<li>SqueezeNet 2016 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07360">LINK</a></li>
<li>Multi-Scale Dense Networks for Resource Efficient Image Classification 2018 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.09844">LINK</a></li>
<li>Label Refinery: Improving ImageNet Classification through Label Progression 2018 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.02641">LINK</a></li>
<li>Deep Mutual Learning 2018 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf">LINK</a></li>
<li>Born Again Neural Networks 2018 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.04770">LINK</a></li>
<li>Improved Knowledge Distillation via Teacher Assistant 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.03393">LINK</a></li>
<li>FITNETS: HINTS FOR THIN DEEP NETS 2015 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1412.6550.pdf">LINK</a></li>
<li>Relational Knowledge Distillation 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.05068.pdf">LINK</a></li>
<li>Similarity-Preserving Knowledge Distillation 2019 <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf">LINK</a></li>
<li>Pruning Filters for Efficient ConvNets 2017 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08710">LINK</a></li>
<li>Learning Efficient Convolutional Networks Through Network Slimming 2017 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.06519">LINK</a></li>
<li>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.00250.pdf">LINK</a></li>
<li>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures 2016 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.03250">LINK</a></li>
<li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask 2019 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.01067">LINK</a></li>
</ol>
<h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys:"></a>Surveys:</h3><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>model compression最主要包含以下四类大方法，<strong>每一类并不是独立的，是可以交替和交叉使用的。</strong></p>
<p>如下图所示：</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/8bfmXQc5OCFEaj1.png" alt="image-20201126165839580" style="zoom:67%;" />

<h2 id="Network-Pruning"><a href="#Network-Pruning" class="headerlink" title="Network Pruning"></a>Network Pruning</h2><p>主要思想为：将Network<strong>不重要</strong>的weight或neuron删除后，再重新训练一次。</p>
<p>General Reason：尤其在深度学习领域，网路很深，模型参数极多，存在很多冗杂的参数。</p>
<p>应用：所有有神经网络的地方基本都可以使用。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/y97ufNeRSBvkjDY.png" alt="image-20201126160856008" style="zoom:50%;" />

<p>其整体流程大概可以如上图所示。</p>
<p>NOTE：这里如何计算weight或neuron的重要程度有各种不同的方法，e.g. L2, L1 Norm，the number of times it wasn’t zero on a given data set ……</p>
<p>可以看到，这是一个iteration的过程。</p>
<h3 id="Why-Pruning"><a href="#Why-Pruning" class="headerlink" title="Why Pruning"></a>Why Pruning</h3><p>为什么要做network Pruning? 而不是直接在一个小的model上学习?</p>
<p>经验上来讲：<strong>It is widely known that smaller network is more difficult to learn successfully.</strong> 即：大模型的训练往往比小模型更加简单，即更容易跳过一些local minimum。</p>
<p>当然也有一些关于这方面的理论，如：</p>
<ul>
<li>Lottery Ticket Hypothesis <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.03635">LINK</a></li>
<li>Larger network is easier to optimize? [LINK](<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_VuWvQU">https://www.youtube.com/watch?v=_VuWvQU</a><br>MQVk)</li>
<li>Rethinking the Value of Network Pruning <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05270">LINK</a>   NOTE: Lottery Ticket Hypothesis的反例</li>
</ul>
<p>基本上来讲network Pruning可以分为：</p>
<ul>
<li><p>对于weight 做pruning</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/zHNxSCdIkmYylfU.png" alt="image-20201126162158912" style="zoom:67%;" />

<p>这种方法的最大问题是导致<strong>不方便implement！！！</strong>因为现在GPU加速都是使用矩阵运算，这样容易使网络结构变的不规则。导致无法Speed Up甚至会出现pruning后速度反而下降的情况。</p>
<p>在Practice中这类weight pruning常常就简单的将要pruning的weight设置为0，这样显然并没有办法对于模型的体积进行压缩。</p>
</li>
<li><p>对于Neuron 做pruning</p>
</li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/n4LjKY7qrQobh9N.png" alt="image-20201126162424440" style="zoom:67%;" />

<p>可以看到，在pruning后，整个网络仍然是regular的，可以继续使用GPU进行加速。实践中比较常用。</p>
<h3 id="What-is-important-How-to-evaluate-importance"><a href="#What-is-important-How-to-evaluate-importance" class="headerlink" title="What is important? How to evaluate importance?"></a>What is important? How to evaluate importance?</h3><p>如何来衡量一个weight or neuron的importance</p>
<ul>
<li>evaluate by weight（看大小）</li>
<li>evaluate by activation</li>
<li>evaluate by gradient</li>
</ul>
<p>After that?</p>
<ul>
<li>Sort by importance and prune by rank</li>
<li>prune by handcrafted threshold</li>
<li>prune by generated threshold</li>
</ul>
<p>**Threshold or Rank? **</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/4Eci1AqsB67kwVF.png" alt="image-20201126200440010" style="zoom: 67%;" />

<h4 id="Evaluate-Importance"><a href="#Evaluate-Importance" class="headerlink" title="Evaluate Importance"></a>Evaluate Importance</h4><p>这部分主要关注evaluate weight.</p>
<ul>
<li><p>sum of L1 norm（也可以是其他范数）</p>
<p>这种直接使用norm的方法一般如下（对于CNN而言）：</p>
<p>将卷积的filter排列为矩阵，每个filter的channel拼成一行，之后对每一行算norm，根据此norm来选择去掉哪些filter.</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SAIQMnsiZF3DPez.png" alt="image-20201126201736434" style="zoom: 80%;" />

<p>理想的norm分布应该如下图所示，即有：</p>
<ul>
<li>norm非常接近0的部分</li>
<li>整体是一个比价均匀，而且有较大方差的分布</li>
</ul>
<p>我们就想要prune掉norm接近0的部分。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/QYv96ng5r4OtIMH.png" alt="image-20201126201949988" style="zoom:80%;" />

<p>而真实的分布往往并不尽如人意。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/qb2RPiuFAlSgY85.png" alt="image-20201126202145833" style="zoom:80%;" />

<ol>
<li>方差很小，此时很难选取一个合适的threshold。</li>
<li>没有接近0的部分，不接近0，很难从norm的角度说明一个filter他trivial。</li>
</ol>
</li>
<li><p>FPGM(Filiter Pruning via Gemetirc Media 2019): 大的norm一定important？小的norm的一定trivial？用Gemetirc Media来解决这个问题。（解决hazard of pruning by norm）如下图所示。</p>
</li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/hly5eWXK4TaPvq9.png" alt="image-20201126202506572" style="zoom:80%;" />

<ul>
<li><p>Evaluate By BN(Batch Norm) Network Slimming</p>
<ul>
<li>根据BN的γ来判断是否pruning</li>
<li><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/SOvbfiQ73FJ84BM.png" alt="image-20201126203026861"></li>
<li>而往往γ的分布并不好，我们需要做一些penalty。让这个分布更容易被筛选。</li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/KabxdY4LPWyjSn7.png" alt="image-20201126203224808" style="zoom:67%;" />
</li>
<li><p>Eval by 0s after ReLU - APoZ(Average Percentage of Zeros)</p>
</li>
</ul>
<h3 id="Some-theory-about-Network-Pruning"><a href="#Some-theory-about-Network-Pruning" class="headerlink" title="Some theory about Network Pruning"></a>Some theory about Network Pruning</h3><h4 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h4><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/MeY3PogsmfuARxt.png" alt="image-20201126203825313" style="zoom:67%;" />

<h4 id="Rethinking-the-Value-of-Network-Pruning"><a href="#Rethinking-the-Value-of-Network-Pruning" class="headerlink" title="Rethinking the Value of Network Pruning"></a>Rethinking the Value of Network Pruning</h4><h4 id="Rethinking-VS-Lottery-Ticket"><a href="#Rethinking-VS-Lottery-Ticket" class="headerlink" title="Rethinking VS Lottery Ticket"></a>Rethinking VS Lottery Ticket</h4><p>Rethinking: 一种neuron pruning or structural pruning</p>
<p>Lottery Ticket: 一种 weight pruning，且要求learning rate要小。</p>
<h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p>主要思想为：利用一个已经训练好的大model做teacher，来训练小model(student).</p>
<p>最核心的思路为下图所示：</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/RHYLdCQWbqlySxc.png" alt="image-20201126163001439" style="zoom:67%;" />

<p>对于Knowledge Distillation(下文简称为KD)的分类，基本上可以按照Distill What来分类。</p>
<ol>
<li><strong>Logits</strong> 即：网络的输出值，一个label的概率分布<ul>
<li>直接一对一匹配logits</li>
<li>以batch为学习单位来学习其中的logits distillation</li>
<li>……</li>
</ul>
</li>
<li><strong>Feature</strong> 即：网络每层中的中间值<ul>
<li>直接一对一匹配每层的中间值</li>
<li>学习teacher网络中feature是如何转换的</li>
</ul>
</li>
</ol>
<h3 id="The-Power-of-Soften-Label"><a href="#The-Power-of-Soften-Label" class="headerlink" title="The Power of Soften Label"></a>The Power of Soften Label</h3><p>对于分类任务，我们模型的输出并不是想真实的label中一样，是一个one-hot encoding，而是一组在许多label上都用几率的一组概率分布。可以直观的看到，这个模型的输出，相比于真实的label包含了更多的信息，甚至包含了类别间的relationship. 现在有一类研究方向就是在训练时不适用这种one-hot encoding，而是研究如何产生包含更多信息的Soften Label。</p>
<p>例如这篇Label Refinery: Improving ImageNet Classification through Label Progression <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.02641">LINK</a> 之后写这篇文章的总结和介绍。</p>
<h3 id="Logits-Distillation"><a href="#Logits-Distillation" class="headerlink" title="Logits Distillation"></a>Logits Distillation</h3><p>本质就是：通过soft target让小model可以学到class之间的关系。</p>
<p>一些比较有趣的Work:</p>
<ul>
<li>Deep Mutual Learning </li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkrAz1DXclxyK8P.png" alt="image-20201126171644149" style="zoom:67%;" />

<ul>
<li>Born Again Neural Networks</li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ERveOL2rFQAo6H5.png" alt="image-20201126171821469" style="zoom:67%;" />

<p>显然，这其中存在着一些问题，可能因为teacher模型的模型能力过强，而小模型的能力不足，导致无法很好地直接学习。</p>
<p>其中的一种很有趣的解决方法就是，向我们上课一样，引入TA。TA模型的能力介于teacher和student之间。这样可以避免缩小模型间的差距。下图即为：Improved Knowledge Distillation via Teacher Assistant 2020这篇paper的想法。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/Lc9MU1257Gw4jXh.png" alt="image-20201126172800289" style="zoom:67%;" />

<h3 id="Feature-Distillation"><a href="#Feature-Distillation" class="headerlink" title="Feature Distillation"></a>Feature Distillation</h3><p>不再是直接根据logits来学习，而是学习网络中的中间features。</p>
<p>其代表方法有：</p>
<ul>
<li>FitNet: 先让学生学习如何产生teacher的中间feature，以后再使用标准的KD。NOTE：框架越相似，效果越好。</li>
</ul>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/EWcQDowM2CZNlzv.png" alt="image-20201126173316849" style="zoom:67%;" />

<p>该方法存在两大问题：</p>
<ul>
<li>model capacity is different. 显然如果大模型很复杂，可能小模型的中间部分无法学习到大模型的复杂映射。</li>
<li>redundance in teacher feature. 这个是很直观的，对于复杂的模型，这个feature中并不是所有的部分都是起作用的，这些对于小模型来讲是一个学习的负担。</li>
</ul>
<p>解决上述问题的方法可以是对于大模型的每一个feature map做一个知识蒸馏，目的就是在压缩feature的同时也降低了redundance. </p>
<p>另外一种的解决方法就是使用<strong>Attention</strong>，告诉student model中的feature map(主要指CNN)中那些part是最重要的。</p>
<p>下图是一种简单计算attention的方法，就是将filter对应产生的out dim做一个压缩。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/JnTAcqSV2r1UE75.png" alt="image-20201126192625424" style="zoom:67%;" />

<h3 id="Relational-Distillation-Learn-from-Batch"><a href="#Relational-Distillation-Learn-from-Batch" class="headerlink" title="Relational Distillation: Learn from Batch"></a>Relational Distillation: Learn from Batch</h3><p>前面的不论是logit KD还是feature KD，都是对于每一个sample来学习的（即：individual KD），这类Relational Distillation关注的则通过一个batch来distillation batch之间sample的关系。</p>
<p>下图是这individual KD与Relational KD的paradigm的对比。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/OkfRwnCVBd4WLaT.png" alt="image-20201126193016466" style="zoom:67%;" />

<p>而衡量sample之间的relation 可以用以下的两种描述角度：</p>
<ul>
<li>Distance-wise KD: 使用L2 distance来描述。</li>
<li>Angle-wise KD: 使用cosin similarity来描述。</li>
</ul>
<p>该方法是使用logits来做KD的，自然也可以使用features来做KD。这就有了：Similarity-Preserving Knowledge Distillation这篇文章。</p>
<h2 id="Parameter-Quantization"><a href="#Parameter-Quantization" class="headerlink" title="Parameter Quantization"></a>Parameter Quantization</h2><p>将原本神经网络中数据的存储单位float32/64压缩为更小的单位，例如8位。</p>
<p>应用：对于所欲已经train好的模型使用，或者边train边让模型去做quantize。</p>
<p>基本来讲大致分为以下方法：</p>
<ol>
<li>Using less bits to represent a value</li>
<li>Weight clustering</li>
</ol>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/ECuVJeA4ZiOKNsR.png" alt="image-20201126163333485" style="zoom:67%;" />

<ol start="3">
<li>Represent frequent clusters by less bits, represent rare clusters by more bits. e.g. Huffman encoding</li>
</ol>
<p>其中一类是使用Binary Weight。(从某种角度上，也是一种正则化的方法)</p>
<ul>
<li>Binary Connect <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.00363">LINK</a></li>
<li>Binary Network <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.02830">LINK</a></li>
<li>XNOR-Net <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.05279">LINK</a></li>
</ul>
<h2 id="Architecture-Design"><a href="#Architecture-Design" class="headerlink" title="Architecture Design"></a>Architecture Design</h2><p>方法：利用更少的参数来实现原本某些layer效果。</p>
<p>例如对于全连接层：</p>
<p>我们可以将原本的 N到M维的映射 变为 N-&gt;K-&gt;M的映射。从矩阵乘法的角度来看，这可以看作一种Low rank approximate. 这种方法也可以极大的减少全连接层的参数数量。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/U8oKGqR9xNWDlV3.png" alt="image-20201126164212464" style="zoom:67%;" />

<p>对于卷积层：可以使用Depthwise Separable Convolution（这也是大名鼎鼎的mobile net使用的方法）</p>
<p>即将原本一步的卷积运算变为两个卷积运算，如下图所示。</p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/26/mqGlihdKrTAOX9I.png" alt="image-20201126164522446" style="zoom:67%;" />

<p>其他的一些经典的这类design有：</p>
<ul>
<li>MobileNet</li>
<li>SqueezeNet</li>
<li>Xception</li>
<li>ShuffleNet</li>
</ul>
<h2 id="Dynamic-Computation"><a href="#Dynamic-Computation" class="headerlink" title="Dynamic Computation"></a>Dynamic Computation</h2><p>核心思想：Can network adjust the computation power it need?</p>
<p>即：若此时计算资源充分，使用大模型；若不足，使用较小的模型。</p>
<p>可能的解决方法：</p>
<ol>
<li><p>Train multiple classifiers</p>
</li>
<li><p>Classifiers at the intermedia layer 例如 Multi-Scale Dense Networks <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.09844">LINK</a></p>
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/30/zJ5US8EW79Ftkv6.png" alt="image-20201130154939206" style="zoom:80%;" /></li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">阿翔</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/">http://canva4.github.io/2020/11/19/Model-Compression-overview-and-resources/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://canVa4.github.io" target="_blank">Xiang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Model-Compression/">Model Compression</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/11/19/7vzxb8FC2GrXQpN.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/28/Model-Compression-Paper-Reading/"><img class="prev-cover" data-lazy-src="https://i.loli.net/2020/11/19/7vzxb8FC2GrXQpN.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Model Compression Paper Reading</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/10/CS224w-HomeWork-2/"><img class="next-cover" data-lazy-src="https://i.loli.net/2020/10/28/HjLzFA5NRfo64UV.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CS224w HomeWork 2</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/11/28/Model-Compression-Paper-Reading/" title="Model Compression Paper Reading"><img class="relatedPosts_cover" data-lazy-src="https://i.loli.net/2020/11/19/7vzxb8FC2GrXQpN.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fas fa-history fa-fw"></i> 2020-11-29</div><div class="relatedPosts_title">Model Compression Paper Reading</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 阿翔</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my Blog~</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({{ JSON.stringify(config) }});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="{{ src }}">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>